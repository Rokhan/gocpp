// generated by GoCpp from file '$(ImportDir)/runtime/mgcmark.go'
#include <complex>
#include <functional>
#include <iostream>
#include <iomanip>
#include <map>
#include <string>
#include <tuple>
#include <vector>

#include "golang/runtime/mgcmark.h"
#include "gocpp/support.h"

#include "golang/internal/abi/symtab.h"
#include "golang/internal/abi/type.h"
#include "golang/internal/chacha8rand/chacha8.h"
#include "golang/internal/cpu/cpu.h"
#include "golang/internal/goarch/goarch.h"
#include "golang/internal/goexperiment/exp_allocheaders_on.h"
#include "golang/internal/goexperiment/exp_exectracer2_on.h"
#include "golang/runtime/cgocall.h"
#include "golang/runtime/chan.h"
#include "golang/runtime/coro.h"
#include "golang/runtime/debuglog_off.h"
#include "golang/runtime/histogram.h"
#include "golang/runtime/internal/atomic/atomic_amd64.h"
#include "golang/runtime/internal/atomic/types.h"
#include "golang/runtime/internal/sys/intrinsics.h"
#include "golang/runtime/internal/sys/nih.h"
#include "golang/runtime/lfstack.h"
#include "golang/runtime/lock_sema.h"
#include "golang/runtime/lockrank.h"
#include "golang/runtime/lockrank_off.h"
#include "golang/runtime/malloc.h"
#include "golang/runtime/mbitmap.h"
#include "golang/runtime/mbitmap_allocheaders.h"
#include "golang/runtime/mcache.h"
#include "golang/runtime/mcentral.h"
#include "golang/runtime/mcheckmark.h"
#include "golang/runtime/mfinal.h"
#include "golang/runtime/mfixalloc.h"
#include "golang/runtime/mgc.h"
#include "golang/runtime/mgclimit.h"
#include "golang/runtime/mgcpacer.h"
#include "golang/runtime/mgcscavenge.h"
#include "golang/runtime/mgcstack.h"
#include "golang/runtime/mgcwork.h"
#include "golang/runtime/mheap.h"
#include "golang/runtime/mpagealloc.h"
#include "golang/runtime/mpagecache.h"
#include "golang/runtime/mpallocbits.h"
#include "golang/runtime/mprof.h"
#include "golang/runtime/mranges.h"
#include "golang/runtime/mspanset.h"
#include "golang/runtime/mstats.h"
#include "golang/runtime/mwbbuf.h"
#include "golang/runtime/os_windows.h"
#include "golang/runtime/pagetrace_off.h"
#include "golang/runtime/panic.h"
#include "golang/runtime/pinner.h"
#include "golang/runtime/plugin.h"
#include "golang/runtime/preempt.h"
#include "golang/runtime/print.h"
#include "golang/runtime/proc.h"
#include "golang/runtime/runtime1.h"
#include "golang/runtime/runtime2.h"
#include "golang/runtime/signal_windows.h"
#include "golang/runtime/stack.h"
#include "golang/runtime/stkframe.h"
#include "golang/runtime/stubs.h"
#include "golang/runtime/symtab.h"
#include "golang/runtime/time.h"
#include "golang/runtime/time_nofake.h"
#include "golang/runtime/trace2buf.h"
#include "golang/runtime/trace2runtime.h"
#include "golang/runtime/trace2status.h"
#include "golang/runtime/trace2time.h"
#include "golang/runtime/traceback.h"
#include "golang/unsafe/unsafe.h"

namespace golang::runtime
{
    namespace rec
    {
        using namespace mocklib::rec;
        using atomic::rec::Add;
        using atomic::rec::Load;
    }

    // rootBlockBytes is the number of bytes to scan per data or
    // BSS root.
    // maxObletBytes is the maximum bytes of an object to scan at
    // once. Larger objects will be split up into "oblets" of at
    // most this size. Since we can scan 1–2 MB/ms, 128 KB bounds
    // scan preemption at ~100 µs.
    //
    // This must be > _MaxSmallSize so that the object base is the
    // span base.
    // drainCheckThreshold specifies how many units of work to do
    // between self-preemption checks in gcDrain. Assuming a scan
    // rate of 1 MB/ms, this is ~100 µs. Lower values have higher
    // overhead in the scan loop (the scheduler check may perform
    // a syscall, so its overhead is nontrivial). Higher values
    // make the system less responsive to incoming work.
    // pagesPerSpanRoot indicates how many pages to scan from a span root
    // at a time. Used by special root marking.
    //
    // Higher values improve throughput by increasing locality, but
    // increase the minimum latency of a marking operation.
    //
    // Must be a multiple of the pageInUse bitmap element size and
    // must also evenly divide pagesPerArena.
    // gcMarkRootPrepare queues root scanning jobs (stacks, globals, and
    // some miscellany) and initializes scanning-related state.
    //
    // The world must be stopped.
    void gcMarkRootPrepare()
    {
        assertWorldStopped();
        auto nBlocks = [=](uintptr_t bytes) mutable -> int
        {
            return int(divRoundUp(bytes, rootBlockBytes));
        };
        work.nDataRoots = 0;
        work.nBSSRoots = 0;
        for(auto [gocpp_ignored, datap] : activeModules())
        {
            auto nDataRoots = nBlocks(datap->edata - datap->data);
            if(nDataRoots > work.nDataRoots)
            {
                work.nDataRoots = nDataRoots;
            }
        }
        for(auto [gocpp_ignored, datap] : activeModules())
        {
            auto nBSSRoots = nBlocks(datap->ebss - datap->bss);
            if(nBSSRoots > work.nBSSRoots)
            {
                work.nBSSRoots = nBSSRoots;
            }
        }
        mheap_.markArenas = mheap_.allArenas.make_slice(0, len(mheap_.allArenas), len(mheap_.allArenas));
        work.nSpanRoots = len(mheap_.markArenas) * (pagesPerArena / pagesPerSpanRoot);
        work.stackRoots = allGsSnapshot();
        work.nStackRoots = len(work.stackRoots);
        work.markrootNext = 0;
        work.markrootJobs = uint32_t(fixedRootCount + work.nDataRoots + work.nBSSRoots + work.nSpanRoots + work.nStackRoots);
        work.baseData = uint32_t(fixedRootCount);
        work.baseBSS = work.baseData + uint32_t(work.nDataRoots);
        work.baseSpans = work.baseBSS + uint32_t(work.nBSSRoots);
        work.baseStacks = work.baseSpans + uint32_t(work.nSpanRoots);
        work.baseEnd = work.baseStacks + uint32_t(work.nStackRoots);
    }

    // gcMarkRootCheck checks that all roots have been scanned. It is
    // purely for debugging.
    void gcMarkRootCheck()
    {
        if(work.markrootNext < work.markrootJobs)
        {
            print(work.markrootNext, " of "_s, work.markrootJobs, " markroot jobs done\n"_s);
            go_throw("left over markroot jobs"_s);
        }
        auto i = 0;
        forEachGRace([=](struct g* gp) mutable -> void
        {
            if(i >= work.nStackRoots)
            {
                return;
            }
            if(! gp->gcscandone)
            {
                println("gp"_s, gp, "goid"_s, gp->goid, "status"_s, readgstatus(gp), "gcscandone"_s, gp->gcscandone);
                go_throw("scan missed a g"_s);
            }
            i++;
        });
    }

    // ptrmask for an allocation containing a single pointer.
    gocpp::array<uint8_t, 1> oneptrmask = gocpp::array<uint8_t, 1> {1};
    // markroot scans the i'th root.
    //
    // Preemption must be disabled (because this uses a gcWork).
    //
    // Returns the amount of GC work credit produced by the operation.
    // If flushBgCredit is true, then that credit is also flushed
    // to the background credit pool.
    //
    // nowritebarrier is only advisory here.
    //
    //go:nowritebarrier
    int64_t markroot(struct gcWork* gcw, uint32_t i, bool flushBgCredit)
    {
        // Note: if you add a case here, please also update heapdump.go:dumproots.
        int64_t workDone = {};
        atomic::Int64* workCounter = {};
        //Go switch emulation
        {
            int conditionId = -1;
            if(work.baseData <= i && i < work.baseBSS) { conditionId = 0; }
            else if(work.baseBSS <= i && i < work.baseSpans) { conditionId = 1; }
            else if(i == fixedRootFinalizers) { conditionId = 2; }
            else if(i == fixedRootFreeGStacks) { conditionId = 3; }
            else if(work.baseSpans <= i && i < work.baseStacks) { conditionId = 4; }
            switch(conditionId)
            {
                case 0:
                    workCounter = & gcController.globalsScanWork;
                    for(auto [gocpp_ignored, datap] : activeModules())
                    {
                        workDone += markrootBlock(datap->data, datap->edata - datap->data, datap->gcdatamask.bytedata, gcw, int(i - work.baseData));
                    }
                    break;
                case 1:
                    workCounter = & gcController.globalsScanWork;
                    for(auto [gocpp_ignored, datap] : activeModules())
                    {
                        workDone += markrootBlock(datap->bss, datap->ebss - datap->bss, datap->gcbssmask.bytedata, gcw, int(i - work.baseBSS));
                    }
                    break;
                case 2:
                    for(auto fb = allfin; fb != nullptr; fb = fb->alllink)
                    {
                        auto cnt = uintptr_t(atomic::Load(& fb->cnt));
                        scanblock(uintptr_t(gocpp::unsafe_pointer(& fb->fin[0])), cnt * gocpp::Sizeof<finalizer>(), & finptrmask[0], gcw, nullptr);
                    }
                    break;
                case 3:
                    systemstack(markrootFreeGStacks);
                    break;
                case 4:
                    markrootSpans(gcw, int(i - work.baseSpans));
                    break;
                default:
                    workCounter = & gcController.stackScanWork;
                    if(i < work.baseStacks || work.baseEnd <= i)
                    {
                        printlock();
                        print("runtime: markroot index "_s, i, " not in stack roots range ["_s, work.baseStacks, ", "_s, work.baseEnd, ")\n"_s);
                        go_throw("markroot: bad index"_s);
                    }
                    auto gp = work.stackRoots[i - work.baseStacks];
                    auto status = readgstatus(gp);
                    if((status == _Gwaiting || status == _Gsyscall) && gp->waitsince == 0)
                    {
                        gp->waitsince = work.tstart;
                    }
                    systemstack([=]() mutable -> void
                    {
                        auto userG = getg()->m->curg;
                        auto selfScan = gp == userG && readgstatus(userG) == _Grunning;
                        if(selfScan)
                        {
                            casGToWaiting(userG, _Grunning, waitReasonGarbageCollectionScan);
                        }
                        auto stopped = suspendG(gp);
                        if(stopped.dead)
                        {
                            gp->gcscandone = true;
                            return;
                        }
                        if(gp->gcscandone)
                        {
                            go_throw("g already scanned"_s);
                        }
                        workDone += scanstack(gp, gcw);
                        gp->gcscandone = true;
                        resumeG(stopped);
                        if(selfScan)
                        {
                            casgstatus(userG, _Gwaiting, _Grunning);
                        }
                    });
                    break;
            }
        }
        if(workCounter != nullptr && workDone != 0)
        {
            rec::Add(gocpp::recv(workCounter), workDone);
            if(flushBgCredit)
            {
                gcFlushBgCredit(workDone);
            }
        }
        return workDone;
    }

    // markrootBlock scans the shard'th shard of the block of memory [b0,
    // b0+n0), with the given pointer mask.
    //
    // Returns the amount of work done.
    //
    //go:nowritebarrier
    int64_t markrootBlock(uintptr_t b0, uintptr_t n0, uint8_t* ptrmask0, struct gcWork* gcw, int shard)
    {
        if(rootBlockBytes % (8 * goarch::PtrSize) != 0)
        {
            go_throw("rootBlockBytes must be a multiple of 8*ptrSize"_s);
        }
        auto off = uintptr_t(shard) * rootBlockBytes;
        if(off >= n0)
        {
            return 0;
        }
        auto b = b0 + off;
        auto ptrmask = (uint8_t*)(add(gocpp::unsafe_pointer(ptrmask0), uintptr_t(shard) * (rootBlockBytes / (8 * goarch::PtrSize))));
        auto n = uintptr_t(rootBlockBytes);
        if(off + n > n0)
        {
            n = n0 - off;
        }
        scanblock(b, n, ptrmask, gcw, nullptr);
        return int64_t(n);
    }

    // markrootFreeGStacks frees stacks of dead Gs.
    //
    // This does not free stacks of dead Gs cached on Ps, but having a few
    // cached stacks around isn't a problem.
    void markrootFreeGStacks()
    {
        lock(& sched.gFree.lock);
        auto list = sched.gFree.stack;
        sched.gFree.stack = gList {};
        unlock(& sched.gFree.lock);
        if(rec::empty(gocpp::recv(list)))
        {
            return;
        }
        auto q = gQueue {list.head, list.head};
        for(auto gp = rec::ptr(gocpp::recv(list.head)); gp != nullptr; gp = rec::ptr(gocpp::recv(gp->schedlink)))
        {
            stackfree(gp->stack);
            gp->stack.lo = 0;
            gp->stack.hi = 0;
            rec::set(gocpp::recv(q.tail), gp);
        }
        lock(& sched.gFree.lock);
        rec::pushAll(gocpp::recv(sched.gFree.noStack), q);
        unlock(& sched.gFree.lock);
    }

    // markrootSpans marks roots for one shard of markArenas.
    //
    //go:nowritebarrier
    void markrootSpans(struct gcWork* gcw, int shard)
    {
        auto sg = mheap_.sweepgen;
        auto ai = mheap_.markArenas[shard / (pagesPerArena / pagesPerSpanRoot)];
        auto ha = mheap_.arenas[rec::l1(gocpp::recv(ai))][rec::l2(gocpp::recv(ai))];
        auto arenaPage = (unsigned int)(uintptr_t(shard) * pagesPerSpanRoot % pagesPerArena);
        auto specialsbits = ha->pageSpecials.make_slice(arenaPage / 8);
        specialsbits = specialsbits.make_slice(0, pagesPerSpanRoot / 8);
        for(auto [i, gocpp_ignored] : specialsbits)
        {
            auto specials = atomic::Load8(& specialsbits[i]);
            if(specials == 0)
            {
                continue;
            }
            for(auto j = (unsigned int)(0); j < 8; j++)
            {
                if(specials & (1 << j) == 0)
                {
                    continue;
                }
                auto s = ha->spans[arenaPage + (unsigned int)(i) * 8 + j];
                if(auto state = rec::get(gocpp::recv(s->state)); state != mSpanInUse)
                {
                    print("s.state = "_s, state, "\n"_s);
                    go_throw("non in-use span found with specials bit set"_s);
                }
                if(! useCheckmark && ! (s->sweepgen == sg || s->sweepgen == sg + 3))
                {
                    print("sweep "_s, s->sweepgen, " "_s, sg, "\n"_s);
                    go_throw("gc: unswept span"_s);
                }
                lock(& s->speciallock);
                for(auto sp = s->specials; sp != nullptr; sp = sp->next)
                {
                    if(sp->kind != _KindSpecialFinalizer)
                    {
                        continue;
                    }
                    auto spf = (specialfinalizer*)(gocpp::unsafe_pointer(sp));
                    auto p = rec::base(gocpp::recv(s)) + uintptr_t(spf->special.offset) / s->elemsize * s->elemsize;
                    if(! rec::noscan(gocpp::recv(s->spanclass)))
                    {
                        scanobject(p, gcw);
                    }
                    scanblock(uintptr_t(gocpp::unsafe_pointer(& spf->fn)), goarch::PtrSize, & oneptrmask[0], gcw, nullptr);
                }
                unlock(& s->speciallock);
            }
        }
    }

    // gcAssistAlloc performs GC work to make gp's assist debt positive.
    // gp must be the calling user goroutine.
    //
    // This must be called with preemption enabled.
    void gcAssistAlloc(struct g* gp)
    {
        if(getg() == gp->m->g0)
        {
            return;
        }
        if(auto mp = getg()->m; mp->locks > 0 || mp->preemptoff != ""_s)
        {
            return;
        }
        auto enteredMarkAssistForTracing = false;
        retry:
        if(rec::limiting(gocpp::recv(gcCPULimiter)))
        {
            if(enteredMarkAssistForTracing)
            {
                auto trace = traceAcquire();
                if(rec::ok(gocpp::recv(trace)))
                {
                    rec::GCMarkAssistDone(gocpp::recv(trace));
                    gp->inMarkAssist = false;
                    traceRelease(trace);
                }
                else
                {
                    gp->inMarkAssist = false;
                }
            }
            return;
        }
        auto assistWorkPerByte = rec::Load(gocpp::recv(gcController.assistWorkPerByte));
        auto assistBytesPerWork = rec::Load(gocpp::recv(gcController.assistBytesPerWork));
        auto debtBytes = - gp->gcAssistBytes;
        auto scanWork = int64_t(assistWorkPerByte * double(debtBytes));
        if(scanWork < gcOverAssistWork)
        {
            scanWork = gcOverAssistWork;
            debtBytes = int64_t(assistBytesPerWork * double(scanWork));
        }
        auto bgScanCredit = rec::Load(gocpp::recv(gcController.bgScanCredit));
        auto stolen = int64_t(0);
        if(bgScanCredit > 0)
        {
            if(bgScanCredit < scanWork)
            {
                stolen = bgScanCredit;
                gp->gcAssistBytes += 1 + int64_t(assistBytesPerWork * double(stolen));
            }
            else
            {
                stolen = scanWork;
                gp->gcAssistBytes += debtBytes;
            }
            rec::Add(gocpp::recv(gcController.bgScanCredit), - stolen);
            scanWork -= stolen;
            if(scanWork == 0)
            {
                if(enteredMarkAssistForTracing)
                {
                    auto trace = traceAcquire();
                    if(rec::ok(gocpp::recv(trace)))
                    {
                        rec::GCMarkAssistDone(gocpp::recv(trace));
                        gp->inMarkAssist = false;
                        traceRelease(trace);
                    }
                    else
                    {
                        gp->inMarkAssist = false;
                    }
                }
                return;
            }
        }
        if(! enteredMarkAssistForTracing)
        {
            auto trace = traceAcquire();
            if(rec::ok(gocpp::recv(trace)))
            {
                if(! goexperiment::ExecTracer2)
                {
                    enteredMarkAssistForTracing = true;
                }
                rec::GCMarkAssistStart(gocpp::recv(trace));
                gp->inMarkAssist = true;
                traceRelease(trace);
            }
            else
            {
                gp->inMarkAssist = true;
            }
            if(goexperiment::ExecTracer2)
            {
                enteredMarkAssistForTracing = true;
            }
        }
        systemstack([=]() mutable -> void
        {
            gcAssistAlloc1(gp, scanWork);
        });
        auto completed = gp->param != nullptr;
        gp->param = nullptr;
        if(completed)
        {
            gcMarkDone();
        }
        if(gp->gcAssistBytes < 0)
        {
            if(gp->preempt)
            {
                Gosched();
                goto retry;
            }
            if(! gcParkAssist())
            {
                goto retry;
            }
        }
        if(enteredMarkAssistForTracing)
        {
            auto trace = traceAcquire();
            if(rec::ok(gocpp::recv(trace)))
            {
                rec::GCMarkAssistDone(gocpp::recv(trace));
                gp->inMarkAssist = false;
                traceRelease(trace);
            }
            else
            {
                gp->inMarkAssist = false;
            }
        }
    }

    // gcAssistAlloc1 is the part of gcAssistAlloc that runs on the system
    // stack. This is a separate function to make it easier to see that
    // we're not capturing anything from the user stack, since the user
    // stack may move while we're in this function.
    //
    // gcAssistAlloc1 indicates whether this assist completed the mark
    // phase by setting gp.param to non-nil. This can't be communicated on
    // the stack since it may move.
    //
    //go:systemstack
    void gcAssistAlloc1(struct g* gp, int64_t scanWork)
    {
        gp->param = nullptr;
        if(atomic::Load(& gcBlackenEnabled) == 0)
        {
            gp->gcAssistBytes = 0;
            return;
        }
        auto startTime = nanotime();
        auto trackLimiterEvent = rec::start(gocpp::recv(rec::ptr(gocpp::recv(gp->m->p))->limiterEvent), limiterEventMarkAssist, startTime);
        auto decnwait = atomic::Xadd(& work.nwait, - 1);
        if(decnwait == work.nproc)
        {
            println("runtime: work.nwait ="_s, decnwait, "work.nproc="_s, work.nproc);
            go_throw("nwait > work.nprocs"_s);
        }
        casGToWaiting(gp, _Grunning, waitReasonGCAssistMarking);
        auto gcw = & rec::ptr(gocpp::recv(getg()->m->p))->gcw;
        auto workDone = gcDrainN(gcw, scanWork);
        casgstatus(gp, _Gwaiting, _Grunning);
        auto assistBytesPerWork = rec::Load(gocpp::recv(gcController.assistBytesPerWork));
        gp->gcAssistBytes += 1 + int64_t(assistBytesPerWork * double(workDone));
        auto incnwait = atomic::Xadd(& work.nwait, + 1);
        if(incnwait > work.nproc)
        {
            println("runtime: work.nwait="_s, incnwait, "work.nproc="_s, work.nproc);
            go_throw("work.nwait > work.nproc"_s);
        }
        if(incnwait == work.nproc && ! gcMarkWorkAvailable(nullptr))
        {
            gp->param = gocpp::unsafe_pointer(gp);
        }
        auto now = nanotime();
        auto duration = now - startTime;
        auto pp = rec::ptr(gocpp::recv(gp->m->p));
        pp->gcAssistTime += duration;
        if(trackLimiterEvent)
        {
            rec::stop(gocpp::recv(pp->limiterEvent), limiterEventMarkAssist, now);
        }
        if(pp->gcAssistTime > gcAssistTimeSlack)
        {
            rec::Add(gocpp::recv(gcController.assistTime), pp->gcAssistTime);
            rec::update(gocpp::recv(gcCPULimiter), now);
            pp->gcAssistTime = 0;
        }
    }

    // gcWakeAllAssists wakes all currently blocked assists. This is used
    // at the end of a GC cycle. gcBlackenEnabled must be false to prevent
    // new assists from going to sleep after this point.
    void gcWakeAllAssists()
    {
        lock(& work.assistQueue.lock);
        auto list = rec::popList(gocpp::recv(work.assistQueue.q));
        injectglist(& list);
        unlock(& work.assistQueue.lock);
    }

    // gcParkAssist puts the current goroutine on the assist queue and parks.
    //
    // gcParkAssist reports whether the assist is now satisfied. If it
    // returns false, the caller must retry the assist.
    bool gcParkAssist()
    {
        lock(& work.assistQueue.lock);
        if(atomic::Load(& gcBlackenEnabled) == 0)
        {
            unlock(& work.assistQueue.lock);
            return true;
        }
        auto gp = getg();
        auto oldList = work.assistQueue.q;
        rec::pushBack(gocpp::recv(work.assistQueue.q), gp);
        if(rec::Load(gocpp::recv(gcController.bgScanCredit)) > 0)
        {
            work.assistQueue.q = oldList;
            if(oldList.tail != 0)
            {
                rec::set(gocpp::recv(rec::ptr(gocpp::recv(oldList.tail))->schedlink), nullptr);
            }
            unlock(& work.assistQueue.lock);
            return false;
        }
        goparkunlock(& work.assistQueue.lock, waitReasonGCAssistWait, traceBlockGCMarkAssist, 2);
        return true;
    }

    // gcFlushBgCredit flushes scanWork units of background scan work
    // credit. This first satisfies blocked assists on the
    // work.assistQueue and then flushes any remaining credit to
    // gcController.bgScanCredit.
    //
    // Write barriers are disallowed because this is used by gcDrain after
    // it has ensured that all work is drained and this must preserve that
    // condition.
    //
    //go:nowritebarrierrec
    void gcFlushBgCredit(int64_t scanWork)
    {
        if(rec::empty(gocpp::recv(work.assistQueue.q)))
        {
            rec::Add(gocpp::recv(gcController.bgScanCredit), scanWork);
            return;
        }
        auto assistBytesPerWork = rec::Load(gocpp::recv(gcController.assistBytesPerWork));
        auto scanBytes = int64_t(double(scanWork) * assistBytesPerWork);
        lock(& work.assistQueue.lock);
        for(; ! rec::empty(gocpp::recv(work.assistQueue.q)) && scanBytes > 0; )
        {
            auto gp = rec::pop(gocpp::recv(work.assistQueue.q));
            if(scanBytes + gp->gcAssistBytes >= 0)
            {
                scanBytes += gp->gcAssistBytes;
                gp->gcAssistBytes = 0;
                ready(gp, 0, false);
            }
            else
            {
                gp->gcAssistBytes += scanBytes;
                scanBytes = 0;
                rec::pushBack(gocpp::recv(work.assistQueue.q), gp);
                break;
            }
        }
        if(scanBytes > 0)
        {
            auto assistWorkPerByte = rec::Load(gocpp::recv(gcController.assistWorkPerByte));
            scanWork = int64_t(double(scanBytes) * assistWorkPerByte);
            rec::Add(gocpp::recv(gcController.bgScanCredit), scanWork);
        }
        unlock(& work.assistQueue.lock);
    }

    // scanstack scans gp's stack, greying all pointers found on the stack.
    //
    // Returns the amount of scan work performed, but doesn't update
    // gcController.stackScanWork or flush any credit. Any background credit produced
    // by this function should be flushed by its caller. scanstack itself can't
    // safely flush because it may result in trying to wake up a goroutine that
    // was just scanned, resulting in a self-deadlock.
    //
    // scanstack will also shrink the stack if it is safe to do so. If it
    // is not, it schedules a stack shrink for the next synchronous safe
    // point.
    //
    // scanstack is marked go:systemstack because it must not be preempted
    // while using a workbuf.
    //
    //go:nowritebarrier
    //go:systemstack
    int64_t scanstack(struct g* gp, struct gcWork* gcw)
    {
        if(readgstatus(gp) & _Gscan == 0)
        {
            print("runtime:scanstack: gp="_s, gp, ", goid="_s, gp->goid, ", gp->atomicstatus="_s, hex(readgstatus(gp)), "\n"_s);
            go_throw("scanstack - bad status"_s);
        }
        //Go switch emulation
        {
            auto condition = readgstatus(gp) &^ _Gscan;
            int conditionId = -1;
            if(condition == _Gdead) { conditionId = 0; }
            else if(condition == _Grunning) { conditionId = 1; }
            else if(condition == _Grunnable) { conditionId = 2; }
            else if(condition == _Gsyscall) { conditionId = 3; }
            else if(condition == _Gwaiting) { conditionId = 4; }
            switch(conditionId)
            {
                default:
                    print("runtime: gp="_s, gp, ", goid="_s, gp->goid, ", gp->atomicstatus="_s, readgstatus(gp), "\n"_s);
                    go_throw("mark - bad status"_s);
                    break;
                case 0:
                    return 0;
                    break;
                case 1:
                    print("runtime: gp="_s, gp, ", goid="_s, gp->goid, ", gp->atomicstatus="_s, readgstatus(gp), "\n"_s);
                    go_throw("scanstack: goroutine not stopped"_s);
                    break;
                case 2:
                case 3:
                case 4:
                    break;
            }
        }
        if(gp == getg())
        {
            go_throw("can't scan our own stack"_s);
        }
        // scannedSize is the amount of work we'll be reporting.
        //
        // It is less than the allocated size (which is hi-lo).
        uintptr_t sp = {};
        if(gp->syscallsp != 0)
        {
            sp = gp->syscallsp;
        }
        else
        {
            sp = gp->sched.sp;
        }
        auto scannedSize = gp->stack.hi - sp;
        auto p = rec::ptr(gocpp::recv(getg()->m->p));
        p->scannedStackSize += uint64_t(scannedSize);
        p->scannedStacks++;
        if(isShrinkStackSafe(gp))
        {
            shrinkstack(gp);
        }
        else
        {
            gp->preemptShrink = true;
        }
        stackScanState state = {};
        state.stack = gp->stack;
        if(stackTraceDebug)
        {
            println("stack trace goroutine"_s, gp->goid);
        }
        if(debugScanConservative && gp->asyncSafePoint)
        {
            print("scanning async preempted goroutine "_s, gp->goid, " stack ["_s, hex(gp->stack.lo), ","_s, hex(gp->stack.hi), ")\n"_s);
        }
        if(gp->sched.ctxt != nullptr)
        {
            scanblock(uintptr_t(gocpp::unsafe_pointer(& gp->sched.ctxt)), goarch::PtrSize, & oneptrmask[0], gcw, & state);
        }
        // Scan the stack. Accumulate a list of stack objects.
        unwinder u = {};
        for(rec::init(gocpp::recv(u), gp, 0); rec::valid(gocpp::recv(u)); rec::next(gocpp::recv(u)))
        {
            scanframeworker(& u.frame, & state, gcw);
        }
        for(auto d = gp->_defer; d != nullptr; d = d->link)
        {
            if(d->fn != nullptr)
            {
                scanblock(uintptr_t(gocpp::unsafe_pointer(& [&](){ return rec::fn(d); })), goarch::PtrSize, & oneptrmask[0], gcw, & state);
            }
            if(d->link != nullptr)
            {
                scanblock(uintptr_t(gocpp::unsafe_pointer(& d->link)), goarch::PtrSize, & oneptrmask[0], gcw, & state);
            }
            if(d->heap)
            {
                scanblock(uintptr_t(gocpp::unsafe_pointer(& d)), goarch::PtrSize, & oneptrmask[0], gcw, & state);
            }
        }
        if(gp->_panic != nullptr)
        {
            rec::putPtr(gocpp::recv(state), uintptr_t(gocpp::unsafe_pointer(gp->_panic)), false);
        }
        rec::buildIndex(gocpp::recv(state));
        for(; ; )
        {
            auto [p, conservative] = rec::getPtr(gocpp::recv(state));
            if(p == 0)
            {
                break;
            }
            auto obj = rec::findObject(gocpp::recv(state), p);
            if(obj == nullptr)
            {
                continue;
            }
            auto r = obj->r;
            if(r == nullptr)
            {
                continue;
            }
            rec::setRecord(gocpp::recv(obj), nullptr);
            if(stackTraceDebug)
            {
                printlock();
                print("  live stkobj at"_s, hex(state.stack.lo + uintptr_t(obj->off)), "of size"_s, obj->size);
                if(conservative)
                {
                    print(" (conservative)"_s);
                }
                println();
                printunlock();
            }
            auto gcdata = rec::gcdata(gocpp::recv(r));
            mspan* s = {};
            if(rec::useGCProg(gocpp::recv(r)))
            {
                s = materializeGCProg(rec::ptrdata(gocpp::recv(r)), gcdata);
                gcdata = (unsigned char*)(gocpp::unsafe_pointer(s->startAddr));
            }
            auto b = state.stack.lo + uintptr_t(obj->off);
            if(conservative)
            {
                scanConservative(b, rec::ptrdata(gocpp::recv(r)), gcdata, gcw, & state);
            }
            else
            {
                scanblock(b, rec::ptrdata(gocpp::recv(r)), gcdata, gcw, & state);
            }
            if(s != nullptr)
            {
                dematerializeGCProg(s);
            }
        }
        for(; state.head != nullptr; )
        {
            auto x = state.head;
            state.head = x->next;
            if(stackTraceDebug)
            {
                for(auto i = 0; i < x->nobj; i++)
                {
                    auto obj = & x->obj[i];
                    if(obj->r == nullptr)
                    {
                        continue;
                    }
                    println("  dead stkobj at"_s, hex(gp->stack.lo + uintptr_t(obj->off)), "of size"_s, obj->r->size);
                }
            }
            x->nobj = 0;
            putempty((workbuf*)(gocpp::unsafe_pointer(x)));
        }
        if(state.buf != nullptr || state.cbuf != nullptr || state.freeBuf != nullptr)
        {
            go_throw("remaining pointer buffers"_s);
        }
        return int64_t(scannedSize);
    }

    // Scan a stack frame: local variables and function arguments/results.
    //
    //go:nowritebarrier
    void scanframeworker(struct stkframe* frame, struct stackScanState* state, struct gcWork* gcw)
    {
        if(_DebugGC > 1 && frame->continpc != 0)
        {
            print("scanframe "_s, funcname(frame->fn), "\n"_s);
        }
        auto isAsyncPreempt = rec::valid(gocpp::recv(frame->fn)) && frame->fn.funcID == abi::FuncID_asyncPreempt;
        auto isDebugCall = rec::valid(gocpp::recv(frame->fn)) && frame->fn.funcID == abi::FuncID_debugCallV2;
        if(state->conservative || isAsyncPreempt || isDebugCall)
        {
            if(debugScanConservative)
            {
                println("conservatively scanning function"_s, funcname(frame->fn), "at PC"_s, hex(frame->continpc));
            }
            if(frame->varp != 0)
            {
                auto size = frame->varp - frame->sp;
                if(size > 0)
                {
                    scanConservative(frame->sp, size, nullptr, gcw, state);
                }
            }
            if(auto n = rec::argBytes(gocpp::recv(frame)); n != 0)
            {
                scanConservative(frame->argp, n, nullptr, gcw, state);
            }
            if(isAsyncPreempt || isDebugCall)
            {
                state->conservative = true;
            }
            else
            {
                state->conservative = false;
            }
            return;
        }
        auto [locals, args, objs] = rec::getStackMap(gocpp::recv(frame), false);
        if(locals.n > 0)
        {
            auto size = uintptr_t(locals.n) * goarch::PtrSize;
            scanblock(frame->varp - size, size, locals.bytedata, gcw, state);
        }
        if(args.n > 0)
        {
            scanblock(frame->argp, uintptr_t(args.n) * goarch::PtrSize, args.bytedata, gcw, state);
        }
        if(frame->varp != 0)
        {
            for(auto [i, gocpp_ignored] : objs)
            {
                auto obj = & objs[i];
                auto off = obj->off;
                auto base = frame->varp;
                if(off >= 0)
                {
                    base = frame->argp;
                }
                auto ptr = base + uintptr_t(off);
                if(ptr < frame->sp)
                {
                    continue;
                }
                if(stackTraceDebug)
                {
                    println("stkobj at"_s, hex(ptr), "of size"_s, obj->size);
                }
                rec::addObject(gocpp::recv(state), ptr, obj);
            }
        }
    }

    // gcDrainMarkWorkerIdle is a wrapper for gcDrain that exists to better account
    // mark time in profiles.
    void gcDrainMarkWorkerIdle(struct gcWork* gcw)
    {
        gcDrain(gcw, gcDrainIdle | gcDrainUntilPreempt | gcDrainFlushBgCredit);
    }

    // gcDrainMarkWorkerDedicated is a wrapper for gcDrain that exists to better account
    // mark time in profiles.
    void gcDrainMarkWorkerDedicated(struct gcWork* gcw, bool untilPreempt)
    {
        auto flags = gcDrainFlushBgCredit;
        if(untilPreempt)
        {
            flags |= gcDrainUntilPreempt;
        }
        gcDrain(gcw, flags);
    }

    // gcDrainMarkWorkerFractional is a wrapper for gcDrain that exists to better account
    // mark time in profiles.
    void gcDrainMarkWorkerFractional(struct gcWork* gcw)
    {
        gcDrain(gcw, gcDrainFractional | gcDrainUntilPreempt | gcDrainFlushBgCredit);
    }

    // gcDrain scans roots and objects in work buffers, blackening grey
    // objects until it is unable to get more work. It may return before
    // GC is done; it's the caller's responsibility to balance work from
    // other Ps.
    //
    // If flags&gcDrainUntilPreempt != 0, gcDrain returns when g.preempt
    // is set.
    //
    // If flags&gcDrainIdle != 0, gcDrain returns when there is other work
    // to do.
    //
    // If flags&gcDrainFractional != 0, gcDrain self-preempts when
    // pollFractionalWorkerExit() returns true. This implies
    // gcDrainNoBlock.
    //
    // If flags&gcDrainFlushBgCredit != 0, gcDrain flushes scan work
    // credit to gcController.bgScanCredit every gcCreditSlack units of
    // scan work.
    //
    // gcDrain will always return if there is a pending STW or forEachP.
    //
    // Disabling write barriers is necessary to ensure that after we've
    // confirmed that we've drained gcw, that we don't accidentally end
    // up flipping that condition by immediately adding work in the form
    // of a write barrier buffer flush.
    //
    // Don't set nowritebarrierrec because it's safe for some callees to
    // have write barriers enabled.
    //
    //go:nowritebarrier
    void gcDrain(struct gcWork* gcw, golang::runtime::gcDrainFlags flags)
    {
        if(! writeBarrier.enabled)
        {
            go_throw("gcDrain phase incorrect"_s);
        }
        auto gp = getg()->m->curg;
        auto pp = rec::ptr(gocpp::recv(gp->m->p));
        auto preemptible = flags & gcDrainUntilPreempt != 0;
        auto flushBgCredit = flags & gcDrainFlushBgCredit != 0;
        auto idle = flags & gcDrainIdle != 0;
        auto initScanWork = gcw->heapScanWork;
        auto checkWork = int64_t((1 << 63) - 1);
        std::function<bool ()> check = {};
        if(flags & (gcDrainIdle | gcDrainFractional) != 0)
        {
            checkWork = initScanWork + drainCheckThreshold;
            if(idle)
            {
                check = pollWork;
            }
            else
            if(flags & gcDrainFractional != 0)
            {
                check = pollFractionalWorkerExit;
            }
        }
        if(work.markrootNext < work.markrootJobs)
        {
            for(; ! (gp->preempt && (preemptible || rec::Load(gocpp::recv(sched.gcwaiting)) || pp->runSafePointFn != 0)); )
            {
                auto job = atomic::Xadd(& work.markrootNext, + 1) - 1;
                if(job >= work.markrootJobs)
                {
                    break;
                }
                markroot(gcw, job, flushBgCredit);
                if(check != nullptr && check())
                {
                    goto done;
                }
            }
        }
        for(; ! (gp->preempt && (preemptible || rec::Load(gocpp::recv(sched.gcwaiting)) || pp->runSafePointFn != 0)); )
        {
            if(work.full == 0)
            {
                rec::balance(gocpp::recv(gcw));
            }
            auto b = rec::tryGetFast(gocpp::recv(gcw));
            if(b == 0)
            {
                b = rec::tryGet(gocpp::recv(gcw));
                if(b == 0)
                {
                    wbBufFlush();
                    b = rec::tryGet(gocpp::recv(gcw));
                }
            }
            if(b == 0)
            {
                break;
            }
            scanobject(b, gcw);
            if(gcw->heapScanWork >= gcCreditSlack)
            {
                rec::Add(gocpp::recv(gcController.heapScanWork), gcw->heapScanWork);
                if(flushBgCredit)
                {
                    gcFlushBgCredit(gcw->heapScanWork - initScanWork);
                    initScanWork = 0;
                }
                checkWork -= gcw->heapScanWork;
                gcw->heapScanWork = 0;
                if(checkWork <= 0)
                {
                    checkWork += drainCheckThreshold;
                    if(check != nullptr && check())
                    {
                        break;
                    }
                }
            }
        }
        done:
        if(gcw->heapScanWork > 0)
        {
            rec::Add(gocpp::recv(gcController.heapScanWork), gcw->heapScanWork);
            if(flushBgCredit)
            {
                gcFlushBgCredit(gcw->heapScanWork - initScanWork);
            }
            gcw->heapScanWork = 0;
        }
    }

    // gcDrainN blackens grey objects until it has performed roughly
    // scanWork units of scan work or the G is preempted. This is
    // best-effort, so it may perform less work if it fails to get a work
    // buffer. Otherwise, it will perform at least n units of work, but
    // may perform more because scanning is always done in whole object
    // increments. It returns the amount of scan work performed.
    //
    // The caller goroutine must be in a preemptible state (e.g.,
    // _Gwaiting) to prevent deadlocks during stack scanning. As a
    // consequence, this must be called on the system stack.
    //
    //go:nowritebarrier
    //go:systemstack
    int64_t gcDrainN(struct gcWork* gcw, int64_t scanWork)
    {
        if(! writeBarrier.enabled)
        {
            go_throw("gcDrainN phase incorrect"_s);
        }
        auto workFlushed = - gcw->heapScanWork;
        auto gp = getg()->m->curg;
        for(; ! gp->preempt && ! rec::limiting(gocpp::recv(gcCPULimiter)) && workFlushed + gcw->heapScanWork < scanWork; )
        {
            if(work.full == 0)
            {
                rec::balance(gocpp::recv(gcw));
            }
            auto b = rec::tryGetFast(gocpp::recv(gcw));
            if(b == 0)
            {
                b = rec::tryGet(gocpp::recv(gcw));
                if(b == 0)
                {
                    wbBufFlush();
                    b = rec::tryGet(gocpp::recv(gcw));
                }
            }
            if(b == 0)
            {
                if(work.markrootNext < work.markrootJobs)
                {
                    auto job = atomic::Xadd(& work.markrootNext, + 1) - 1;
                    if(job < work.markrootJobs)
                    {
                        workFlushed += markroot(gcw, job, false);
                        continue;
                    }
                }
                break;
            }
            scanobject(b, gcw);
            if(gcw->heapScanWork >= gcCreditSlack)
            {
                rec::Add(gocpp::recv(gcController.heapScanWork), gcw->heapScanWork);
                workFlushed += gcw->heapScanWork;
                gcw->heapScanWork = 0;
            }
        }
        return workFlushed + gcw->heapScanWork;
    }

    // scanblock scans b as scanobject would, but using an explicit
    // pointer bitmap instead of the heap bitmap.
    //
    // This is used to scan non-heap roots, so it does not update
    // gcw.bytesMarked or gcw.heapScanWork.
    //
    // If stk != nil, possible stack pointers are also reported to stk.putPtr.
    //
    //go:nowritebarrier
    void scanblock(uintptr_t b0, uintptr_t n0, uint8_t* ptrmask, struct gcWork* gcw, struct stackScanState* stk)
    {
        auto b = b0;
        auto n = n0;
        for(auto i = uintptr_t(0); i < n; )
        {
            auto bits = uint32_t(*addb(ptrmask, i / (goarch::PtrSize * 8)));
            if(bits == 0)
            {
                i += goarch::PtrSize * 8;
                continue;
            }
            for(auto j = 0; j < 8 && i < n; j++)
            {
                if(bits & 1 != 0)
                {
                    auto p = *(uintptr_t*)(gocpp::unsafe_pointer(b + i));
                    if(p != 0)
                    {
                        if(auto [obj, span, objIndex] = findObject(p, b, i); obj != 0)
                        {
                            greyobject(obj, b, i, span, gcw, objIndex);
                        }
                        else
                        if(stk != nullptr && p >= stk->stack.lo && p < stk->stack.hi)
                        {
                            rec::putPtr(gocpp::recv(stk), p, false);
                        }
                    }
                }
                bits >>= 1;
                i += goarch::PtrSize;
            }
        }
    }

    // scanobject scans the object starting at b, adding pointers to gcw.
    // b must point to the beginning of a heap object or an oblet.
    // scanobject consults the GC bitmap for the pointer mask and the
    // spans for the size of the object.
    //
    //go:nowritebarrier
    void scanobject(uintptr_t b, struct gcWork* gcw)
    {
        sys::Prefetch(b);
        auto s = spanOfUnchecked(b);
        auto n = s->elemsize;
        if(n == 0)
        {
            go_throw("scanobject n == 0"_s);
        }
        if(rec::noscan(gocpp::recv(s->spanclass)))
        {
            go_throw("scanobject of a noscan object"_s);
        }
        typePointers tp = {};
        if(n > maxObletBytes)
        {
            if(b == rec::base(gocpp::recv(s)))
            {
                for(auto oblet = b + maxObletBytes; oblet < rec::base(gocpp::recv(s)) + s->elemsize; oblet += maxObletBytes)
                {
                    if(! rec::putFast(gocpp::recv(gcw), oblet))
                    {
                        rec::put(gocpp::recv(gcw), oblet);
                    }
                }
            }
            n = rec::base(gocpp::recv(s)) + s->elemsize - b;
            n = gocpp::min(n, maxObletBytes);
            if(goexperiment::AllocHeaders)
            {
                tp = rec::typePointersOfUnchecked(gocpp::recv(s), rec::base(gocpp::recv(s)));
                tp = rec::fastForward(gocpp::recv(tp), b - tp.addr, b + n);
            }
        }
        else
        {
            if(goexperiment::AllocHeaders)
            {
                tp = rec::typePointersOfUnchecked(gocpp::recv(s), b);
            }
        }
        heapBits hbits = {};
        if(! goexperiment::AllocHeaders)
        {
            hbits = heapBitsForAddr(b, n);
        }
        uintptr_t scanSize = {};
        for(; ; )
        {
            uintptr_t addr = {};
            if(goexperiment::AllocHeaders)
            {
                if(std::tie(tp, addr) = rec::nextFast(gocpp::recv(tp)); addr == 0)
                {
                    if(std::tie(tp, addr) = rec::next(gocpp::recv(tp), b + n); addr == 0)
                    {
                        break;
                    }
                }
            }
            else
            {
                if(std::tie(hbits, addr) = rec::nextFast(gocpp::recv(hbits)); addr == 0)
                {
                    if(std::tie(hbits, addr) = rec::next(gocpp::recv(hbits)); addr == 0)
                    {
                        break;
                    }
                }
            }
            scanSize = addr - b + goarch::PtrSize;
            auto obj = *(uintptr_t*)(gocpp::unsafe_pointer(addr));
            if(obj != 0 && obj - b >= n)
            {
                {
                    auto [obj_tmp, span, objIndex] = findObject(obj, b, addr - b);
                    if(auto& obj = obj_tmp; obj != 0)
                    {
                        greyobject(obj, b, addr - b, span, gcw, objIndex);
                    }
                }
            }
        }
        gcw->bytesMarked += uint64_t(n);
        gcw->heapScanWork += int64_t(scanSize);
    }

    // scanConservative scans block [b, b+n) conservatively, treating any
    // pointer-like value in the block as a pointer.
    //
    // If ptrmask != nil, only words that are marked in ptrmask are
    // considered as potential pointers.
    //
    // If state != nil, it's assumed that [b, b+n) is a block in the stack
    // and may contain pointers to stack objects.
    void scanConservative(uintptr_t b, uintptr_t n, uint8_t* ptrmask, struct gcWork* gcw, struct stackScanState* state)
    {
        if(debugScanConservative)
        {
            printlock();
            print("conservatively scanning ["_s, hex(b), ","_s, hex(b + n), ")\n"_s);
            hexdumpWords(b, b + n, [=](uintptr_t p) mutable -> unsigned char
            {
                if(ptrmask != nullptr)
                {
                    auto word = (p - b) / goarch::PtrSize;
                    auto bits = *addb(ptrmask, word / 8);
                    if((bits >> (word % 8)) & 1 == 0)
                    {
                        return '$';
                    }
                }
                auto val = *(uintptr_t*)(gocpp::unsafe_pointer(p));
                if(state != nullptr && state->stack.lo <= val && val < state->stack.hi)
                {
                    return '@';
                }
                auto span = spanOfHeap(val);
                if(span == nullptr)
                {
                    return ' ';
                }
                auto idx = rec::objIndex(gocpp::recv(span), val);
                if(rec::isFree(gocpp::recv(span), idx))
                {
                    return ' ';
                }
                return '*';
            });
            printunlock();
        }
        for(auto i = uintptr_t(0); i < n; i += goarch::PtrSize)
        {
            if(ptrmask != nullptr)
            {
                auto word = i / goarch::PtrSize;
                auto bits = *addb(ptrmask, word / 8);
                if(bits == 0)
                {
                    if(i % (goarch::PtrSize * 8) != 0)
                    {
                        go_throw("misaligned mask"_s);
                    }
                    i += goarch::PtrSize * 8 - goarch::PtrSize;
                    continue;
                }
                if((bits >> (word % 8)) & 1 == 0)
                {
                    continue;
                }
            }
            auto val = *(uintptr_t*)(gocpp::unsafe_pointer(b + i));
            if(state != nullptr && state->stack.lo <= val && val < state->stack.hi)
            {
                rec::putPtr(gocpp::recv(state), val, true);
                continue;
            }
            auto span = spanOfHeap(val);
            if(span == nullptr)
            {
                continue;
            }
            auto idx = rec::objIndex(gocpp::recv(span), val);
            if(rec::isFree(gocpp::recv(span), idx))
            {
                continue;
            }
            auto obj = rec::base(gocpp::recv(span)) + idx * span->elemsize;
            greyobject(obj, b, i, span, gcw, idx);
        }
    }

    // Shade the object if it isn't already.
    // The object is not nil and known to be in the heap.
    // Preemption must be disabled.
    //
    //go:nowritebarrier
    void shade(uintptr_t b)
    {
        if(auto [obj, span, objIndex] = findObject(b, 0, 0); obj != 0)
        {
            auto gcw = & rec::ptr(gocpp::recv(getg()->m->p))->gcw;
            greyobject(obj, 0, 0, span, gcw, objIndex);
        }
    }

    // obj is the start of an object with mark mbits.
    // If it isn't already marked, mark it and enqueue into gcw.
    // base and off are for debugging only and could be removed.
    //
    // See also wbBufFlush1, which partially duplicates this logic.
    //
    //go:nowritebarrierrec
    void greyobject(uintptr_t obj, uintptr_t base, uintptr_t off, struct mspan* span, struct gcWork* gcw, uintptr_t objIndex)
    {
        if(obj & (goarch::PtrSize - 1) != 0)
        {
            go_throw("greyobject: obj not pointer-aligned"_s);
        }
        auto mbits = rec::markBitsForIndex(gocpp::recv(span), objIndex);
        if(useCheckmark)
        {
            if(setCheckmark(obj, base, off, mbits))
            {
                return;
            }
        }
        else
        {
            if(debug.gccheckmark > 0 && rec::isFree(gocpp::recv(span), objIndex))
            {
                print("runtime: marking free object "_s, hex(obj), " found at *("_s, hex(base), "+"_s, hex(off), ")\n"_s);
                gcDumpObject("base"_s, base, off);
                gcDumpObject("obj"_s, obj, ~ uintptr_t(0));
                getg()->m->traceback = 2;
                go_throw("marking free object"_s);
            }
            if(rec::isMarked(gocpp::recv(mbits)))
            {
                return;
            }
            rec::setMarked(gocpp::recv(mbits));
            auto [arena, pageIdx, pageMask] = pageIndexOf(rec::base(gocpp::recv(span)));
            if(arena->pageMarks[pageIdx] & pageMask == 0)
            {
                atomic::Or8(& arena->pageMarks[pageIdx], pageMask);
            }
            if(rec::noscan(gocpp::recv(span->spanclass)))
            {
                gcw->bytesMarked += uint64_t(span->elemsize);
                return;
            }
        }
        sys::Prefetch(obj);
        if(! rec::putFast(gocpp::recv(gcw), obj))
        {
            rec::put(gocpp::recv(gcw), obj);
        }
    }

    // gcDumpObject dumps the contents of obj for debugging and marks the
    // field at byte offset off in obj.
    void gcDumpObject(gocpp::string label, uintptr_t obj, uintptr_t off)
    {
        auto s = spanOf(obj);
        print(label, "="_s, hex(obj));
        if(s == nullptr)
        {
            print(" s=nil\n"_s);
            return;
        }
        print(" s.base()="_s, hex(rec::base(gocpp::recv(s))), " s.limit="_s, hex(s->limit), " s.spanclass="_s, s->spanclass, " s.elemsize="_s, s->elemsize, " s.state="_s);
        if(auto state = rec::get(gocpp::recv(s->state)); 0 <= state && int(state) < len(mSpanStateNames))
        {
            print(mSpanStateNames[state], "\n"_s);
        }
        else
        {
            print("unknown("_s, state, ")\n"_s);
        }
        auto skipped = false;
        auto size = s->elemsize;
        if(rec::get(gocpp::recv(s->state)) == mSpanManual && size == 0)
        {
            size = off + goarch::PtrSize;
        }
        for(auto i = uintptr_t(0); i < size; i += goarch::PtrSize)
        {
            if(! (i < 128 * goarch::PtrSize || off - 16 * goarch::PtrSize < i && i < off + 16 * goarch::PtrSize))
            {
                skipped = true;
                continue;
            }
            if(skipped)
            {
                print(" ...\n"_s);
                skipped = false;
            }
            print(" *("_s, label, "+"_s, i, ") = "_s, hex(*(uintptr_t*)(gocpp::unsafe_pointer(obj + i))));
            if(i == off)
            {
                print(" <=="_s);
            }
            print("\n"_s);
        }
        if(skipped)
        {
            print(" ...\n"_s);
        }
    }

    // gcmarknewobject marks a newly allocated object black. obj must
    // not contain any non-nil pointers.
    //
    // This is nosplit so it can manipulate a gcWork without preemption.
    //
    //go:nowritebarrier
    //go:nosplit
    void gcmarknewobject(struct mspan* span, uintptr_t obj)
    {
        if(useCheckmark)
        {
            go_throw("gcmarknewobject called while doing checkmark"_s);
        }
        auto objIndex = rec::objIndex(gocpp::recv(span), obj);
        rec::setMarked(gocpp::recv(rec::markBitsForIndex(gocpp::recv(span), objIndex)));
        auto [arena, pageIdx, pageMask] = pageIndexOf(rec::base(gocpp::recv(span)));
        if(arena->pageMarks[pageIdx] & pageMask == 0)
        {
            atomic::Or8(& arena->pageMarks[pageIdx], pageMask);
        }
        auto gcw = & rec::ptr(gocpp::recv(getg()->m->p))->gcw;
        gcw->bytesMarked += uint64_t(span->elemsize);
    }

    // gcMarkTinyAllocs greys all active tiny alloc blocks.
    //
    // The world must be stopped.
    void gcMarkTinyAllocs()
    {
        assertWorldStopped();
        for(auto [gocpp_ignored, p] : allp)
        {
            auto c = p->mcache;
            if(c == nullptr || c->tiny == 0)
            {
                continue;
            }
            auto [gocpp_id_0, span, objIndex] = findObject(c->tiny, 0, 0);
            auto gcw = & p->gcw;
            greyobject(c->tiny, 0, 0, span, gcw, objIndex);
        }
    }

}

