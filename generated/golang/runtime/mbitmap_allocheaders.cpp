// generated by GoCpp from file '$(ImportDir)/runtime/mbitmap_allocheaders.go'
#include <complex>
#include <functional>
#include <iostream>
#include <iomanip>
#include <map>
#include <string>
#include <tuple>
#include <vector>

#include "golang/runtime/mbitmap_allocheaders.h"
#include "gocpp/support.h"

#include "golang/internal/abi/symtab.h"
#include "golang/internal/abi/type.h"
#include "golang/internal/chacha8rand/chacha8.h"
#include "golang/internal/cpu/cpu.h"
#include "golang/internal/goarch/goarch.h"
#include "golang/runtime/cgocall.h"
#include "golang/runtime/chan.h"
#include "golang/runtime/coro.h"
#include "golang/runtime/debuglog_off.h"
#include "golang/runtime/internal/atomic/types.h"
#include "golang/runtime/internal/sys/intrinsics.h"
#include "golang/runtime/internal/sys/nih.h"
#include "golang/runtime/lockrank.h"
#include "golang/runtime/lockrank_off.h"
#include "golang/runtime/malloc.h"
#include "golang/runtime/mbitmap.h"
#include "golang/runtime/mcache.h"
#include "golang/runtime/mcentral.h"
#include "golang/runtime/mcheckmark.h"
#include "golang/runtime/mfinal.h"
#include "golang/runtime/mfixalloc.h"
#include "golang/runtime/mgc.h"
#include "golang/runtime/mgclimit.h"
#include "golang/runtime/mgcscavenge.h"
#include "golang/runtime/mgcwork.h"
#include "golang/runtime/mheap.h"
#include "golang/runtime/mpagealloc.h"
#include "golang/runtime/mpagecache.h"
#include "golang/runtime/mpallocbits.h"
#include "golang/runtime/mprof.h"
#include "golang/runtime/mranges.h"
#include "golang/runtime/mspanset.h"
#include "golang/runtime/mstats.h"
#include "golang/runtime/mwbbuf.h"
#include "golang/runtime/os_windows.h"
#include "golang/runtime/pagetrace_off.h"
#include "golang/runtime/panic.h"
#include "golang/runtime/pinner.h"
#include "golang/runtime/plugin.h"
#include "golang/runtime/print.h"
#include "golang/runtime/proc.h"
#include "golang/runtime/rand.h"
#include "golang/runtime/runtime2.h"
#include "golang/runtime/signal_windows.h"
#include "golang/runtime/slice.h"
#include "golang/runtime/stack.h"
#include "golang/runtime/stkframe.h"
#include "golang/runtime/stubs.h"
#include "golang/runtime/symtab.h"
#include "golang/runtime/time.h"
#include "golang/runtime/trace2buf.h"
#include "golang/runtime/trace2runtime.h"
#include "golang/runtime/trace2status.h"
#include "golang/runtime/trace2time.h"
#include "golang/runtime/traceback.h"
#include "golang/runtime/type.h"
#include "golang/runtime/typekind.h"
#include "golang/unsafe/unsafe.h"

namespace golang::runtime
{
    namespace rec
    {
        using namespace mocklib::rec;
    }

    // A malloc header is functionally a single type pointer, but
    // we need to use 8 here to ensure 8-byte alignment of allocations
    // on 32-bit platforms. It's wasteful, but a lot of code relies on
    // 8-byte alignment for 8-byte atomics.
    // The minimum object size that has a malloc header, exclusive.
    //
    // The size of this value controls overheads from the malloc header.
    // The minimum size is bound by writeHeapBitsSmall, which assumes that the
    // pointer bitmap for objects of a size smaller than this doesn't cross
    // more than one pointer-word boundary. This sets an upper-bound on this
    // value at the number of bits in a uintptr, multiplied by the pointer
    // size in bytes.
    //
    // We choose a value here that has a natural cutover point in terms of memory
    // overheads. This value just happens to be the maximum possible value this
    // can be.
    //
    // A span with heap bits in it will have 128 bytes of heap bits on 64-bit
    // platforms, and 256 bytes of heap bits on 32-bit platforms. The first size
    // class where malloc headers match this overhead for 64-bit platforms is
    // 512 bytes (8 KiB / 512 bytes * 8 bytes-per-header = 128 bytes of overhead).
    // On 32-bit platforms, this same point is the 256 byte size class
    // (8 KiB / 256 bytes * 8 bytes-per-header = 256 bytes of overhead).
    //
    // Guaranteed to be exactly at a size class boundary. The reason this value is
    // an exclusive minimum is subtle. Suppose we're allocating a 504-byte object
    // and its rounded up to 512 bytes for the size class. If minSizeForMallocHeader
    // is 512 and an inclusive minimum, then a comparison against minSizeForMallocHeader
    // by the two values would produce different results. In other words, the comparison
    // would not be invariant to size-class rounding. Eschewing this property means a
    // more complex check or possibly storing additional state to determine whether a
    // span has malloc headers.
    // heapBitsInSpan returns true if the size of an object implies its ptr/scalar
    // data is stored at the end of the span, and is accessible via span.heapBits.
    //
    // Note: this works for both rounded-up sizes (span.elemsize) and unrounded
    // type sizes because minSizeForMallocHeader is guaranteed to be at a size
    // class boundary.
    //
    //go:nosplit
    bool heapBitsInSpan(uintptr_t userSize)
    {
        return userSize <= minSizeForMallocHeader;
    }

    // heapArenaPtrScalar contains the per-heapArena pointer/scalar metadata for the GC.
    
    template<typename T> requires gocpp::GoStruct<T>
    heapArenaPtrScalar::operator T()
    {
        T result;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool heapArenaPtrScalar::operator==(const T& ref) const
    {
        return true;
    }

    std::ostream& heapArenaPtrScalar::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct heapArenaPtrScalar& value)
    {
        return value.PrintTo(os);
    }

    // typePointers is an iterator over the pointers in a heap object.
    //
    // Iteration through this type implements the tiling algorithm described at the
    // top of this file.
    
    template<typename T> requires gocpp::GoStruct<T>
    typePointers::operator T()
    {
        T result;
        result.elem = this->elem;
        result.addr = this->addr;
        result.mask = this->mask;
        result.typ = this->typ;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool typePointers::operator==(const T& ref) const
    {
        if (elem != ref.elem) return false;
        if (addr != ref.addr) return false;
        if (mask != ref.mask) return false;
        if (typ != ref.typ) return false;
        return true;
    }

    std::ostream& typePointers::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << elem;
        os << " " << addr;
        os << " " << mask;
        os << " " << typ;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct typePointers& value)
    {
        return value.PrintTo(os);
    }

    // typePointersOf returns an iterator over all heap pointers in the range [addr, addr+size).
    //
    // addr and addr+size must be in the range [span.base(), span.limit).
    //
    // Note: addr+size must be passed as the limit argument to the iterator's next method on
    // each iteration. This slightly awkward API is to allow typePointers to be destructured
    // by the compiler.
    //
    // nosplit because it is used during write barriers and must not be preempted.
    //
    //go:nosplit
    struct typePointers rec::typePointersOf(golang::runtime::mspan* span, uintptr_t addr, uintptr_t size)
    {
        auto base = rec::objBase(gocpp::recv(span), addr);
        auto tp = rec::typePointersOfUnchecked(gocpp::recv(span), base);
        if(base == addr && size == span->elemsize)
        {
            return tp;
        }
        return rec::fastForward(gocpp::recv(tp), addr - tp.addr, addr + size);
    }

    // typePointersOfUnchecked is like typePointersOf, but assumes addr is the base
    // of an allocation slot in a span (the start of the object if no header, the
    // header otherwise). It returns an iterator that generates all pointers
    // in the range [addr, addr+span.elemsize).
    //
    // nosplit because it is used during write barriers and must not be preempted.
    //
    //go:nosplit
    struct typePointers rec::typePointersOfUnchecked(golang::runtime::mspan* span, uintptr_t addr)
    {
        auto doubleCheck = false;
        if(doubleCheck && rec::objBase(gocpp::recv(span), addr) != addr)
        {
            print("runtime: addr="_s, addr, " base="_s, rec::objBase(gocpp::recv(span), addr), "\n"_s);
            go_throw("typePointersOfUnchecked consisting of non-base-address for object"_s);
        }
        auto spc = span->spanclass;
        if(rec::noscan(gocpp::recv(spc)))
        {
            return typePointers {};
        }
        if(heapBitsInSpan(span->elemsize))
        {
            return gocpp::Init<typePointers>([=](auto& x) {
                x.elem = addr;
                x.addr = addr;
                x.mask = rec::heapBitsSmallForAddr(gocpp::recv(span), addr);
            });
        }
        // All of these objects have a header.
        runtime::_type* typ = {};
        if(rec::sizeclass(gocpp::recv(spc)) != 0)
        {
            typ = *(runtime::_type**)(unsafe::Pointer(addr));
            addr += mallocHeaderSize;
        }
        else
        {
            typ = span->largeType;
        }
        auto gcdata = typ->GCData;
        return gocpp::Init<typePointers>([=](auto& x) {
            x.elem = addr;
            x.addr = addr;
            x.mask = readUintptr(gcdata);
            x.typ = typ;
        });
    }

    // typePointersOfType is like typePointersOf, but assumes addr points to one or more
    // contiguous instances of the provided type. The provided type must not be nil and
    // it must not have its type metadata encoded as a gcprog.
    //
    // It returns an iterator that tiles typ.GCData starting from addr. It's the caller's
    // responsibility to limit iteration.
    //
    // nosplit because its callers are nosplit and require all their callees to be nosplit.
    //
    //go:nosplit
    struct typePointers rec::typePointersOfType(golang::runtime::mspan* span, abi::Type* typ, uintptr_t addr)
    {
        auto doubleCheck = false;
        if(doubleCheck && (typ == nullptr || typ->Kind_ & kindGCProg != 0))
        {
            go_throw("bad type passed to typePointersOfType"_s);
        }
        if(rec::noscan(gocpp::recv(span->spanclass)))
        {
            return typePointers {};
        }
        auto gcdata = typ->GCData;
        return gocpp::Init<typePointers>([=](auto& x) {
            x.elem = addr;
            x.addr = addr;
            x.mask = readUintptr(gcdata);
            x.typ = typ;
        });
    }

    // nextFast is the fast path of next. nextFast is written to be inlineable and,
    // as the name implies, fast.
    //
    // Callers that are performance-critical should iterate using the following
    // pattern:
    //
    //	for {
    //		var addr uintptr
    //		if tp, addr = tp.nextFast(); addr == 0 {
    //			if tp, addr = tp.next(limit); addr == 0 {
    //				break
    //			}
    //		}
    //		// Use addr.
    //		...
    //	}
    //
    // nosplit because it is used during write barriers and must not be preempted.
    //
    //go:nosplit
    std::tuple<struct typePointers, uintptr_t> rec::nextFast(golang::runtime::typePointers tp)
    {
        if(tp.mask == 0)
        {
            return {tp, 0};
        }
        // BSFQ
        int i = {};
        if(goarch::PtrSize == 8)
        {
            i = sys::TrailingZeros64(uint64_t(tp.mask));
        }
        else
        {
            i = sys::TrailingZeros32(uint32_t(tp.mask));
        }
        tp.mask ^= uintptr_t(1) << (i & (ptrBits - 1));
        return {tp, tp.addr + uintptr_t(i) * goarch::PtrSize};
    }

    // next advances the pointers iterator, returning the updated iterator and
    // the address of the next pointer.
    //
    // limit must be the same each time it is passed to next.
    //
    // nosplit because it is used during write barriers and must not be preempted.
    //
    //go:nosplit
    std::tuple<struct typePointers, uintptr_t> rec::next(golang::runtime::typePointers tp, uintptr_t limit)
    {
        for(; ; )
        {
            if(tp.mask != 0)
            {
                return rec::nextFast(gocpp::recv(tp));
            }
            if(tp.typ == nullptr)
            {
                return {typePointers {}, 0};
            }
            if(tp.addr + goarch::PtrSize * ptrBits >= tp.elem + tp.typ->PtrBytes)
            {
                tp.elem += tp.typ->Size_;
                tp.addr = tp.elem;
            }
            else
            {
                tp.addr += ptrBits * goarch::PtrSize;
            }
            if(tp.addr >= limit)
            {
                return {typePointers {}, 0};
            }
            tp.mask = readUintptr(addb(tp.typ->GCData, (tp.addr - tp.elem) / goarch::PtrSize / 8));
            if(tp.addr + goarch::PtrSize * ptrBits > limit)
            {
                auto bits = (tp.addr + goarch::PtrSize * ptrBits - limit) / goarch::PtrSize;
                tp.mask &^= ((1 << (bits)) - 1) << (ptrBits - bits);
            }
        }
    }

    // fastForward moves the iterator forward by n bytes. n must be a multiple
    // of goarch.PtrSize. limit must be the same limit passed to next for this
    // iterator.
    //
    // nosplit because it is used during write barriers and must not be preempted.
    //
    //go:nosplit
    struct typePointers rec::fastForward(golang::runtime::typePointers tp, uintptr_t n, uintptr_t limit)
    {
        auto target = tp.addr + n;
        if(target >= limit)
        {
            return typePointers {};
        }
        if(tp.typ == nullptr)
        {
            tp.mask &^= (1 << ((target - tp.addr) / goarch::PtrSize)) - 1;
            if(tp.addr + goarch::PtrSize * ptrBits > limit)
            {
                auto bits = (tp.addr + goarch::PtrSize * ptrBits - limit) / goarch::PtrSize;
                tp.mask &^= ((1 << (bits)) - 1) << (ptrBits - bits);
            }
            return tp;
        }
        if(n >= tp.typ->Size_)
        {
            auto oldelem = tp.elem;
            tp.elem += (tp.addr - tp.elem + n) / tp.typ->Size_ * tp.typ->Size_;
            tp.addr = tp.elem + alignDown(n - (tp.elem - oldelem), ptrBits * goarch::PtrSize);
        }
        else
        {
            tp.addr += alignDown(n, ptrBits * goarch::PtrSize);
        }
        if(tp.addr - tp.elem >= tp.typ->PtrBytes)
        {
            tp.elem += tp.typ->Size_;
            tp.addr = tp.elem;
            tp.mask = readUintptr(tp.typ->GCData);
            if(tp.addr >= limit)
            {
                return typePointers {};
            }
        }
        else
        {
            tp.mask = readUintptr(addb(tp.typ->GCData, (tp.addr - tp.elem) / goarch::PtrSize / 8));
            tp.mask &^= (1 << ((target - tp.addr) / goarch::PtrSize)) - 1;
        }
        if(tp.addr + goarch::PtrSize * ptrBits > limit)
        {
            auto bits = (tp.addr + goarch::PtrSize * ptrBits - limit) / goarch::PtrSize;
            tp.mask &^= ((1 << (bits)) - 1) << (ptrBits - bits);
        }
        return tp;
    }

    // objBase returns the base pointer for the object containing addr in span.
    //
    // Assumes that addr points into a valid part of span (span.base() <= addr < span.limit).
    //
    //go:nosplit
    uintptr_t rec::objBase(golang::runtime::mspan* span, uintptr_t addr)
    {
        return rec::base(gocpp::recv(span)) + rec::objIndex(gocpp::recv(span), addr) * span->elemsize;
    }

    // bulkBarrierPreWrite executes a write barrier
    // for every pointer slot in the memory range [src, src+size),
    // using pointer/scalar information from [dst, dst+size).
    // This executes the write barriers necessary before a memmove.
    // src, dst, and size must be pointer-aligned.
    // The range [dst, dst+size) must lie within a single object.
    // It does not perform the actual writes.
    //
    // As a special case, src == 0 indicates that this is being used for a
    // memclr. bulkBarrierPreWrite will pass 0 for the src of each write
    // barrier.
    //
    // Callers should call bulkBarrierPreWrite immediately before
    // calling memmove(dst, src, size). This function is marked nosplit
    // to avoid being preempted; the GC must not stop the goroutine
    // between the memmove and the execution of the barriers.
    // The caller is also responsible for cgo pointer checks if this
    // may be writing Go pointers into non-Go memory.
    //
    // Pointer data is not maintained for allocations containing
    // no pointers at all; any caller of bulkBarrierPreWrite must first
    // make sure the underlying allocation contains pointers, usually
    // by checking typ.PtrBytes.
    //
    // The typ argument is the type of the space at src and dst (and the
    // element type if src and dst refer to arrays) and it is optional.
    // If typ is nil, the barrier will still behave as expected and typ
    // is used purely as an optimization. However, it must be used with
    // care.
    //
    // If typ is not nil, then src and dst must point to one or more values
    // of type typ. The caller must ensure that the ranges [src, src+size)
    // and [dst, dst+size) refer to one or more whole values of type src and
    // dst (leaving off the pointerless tail of the space is OK). If this
    // precondition is not followed, this function will fail to scan the
    // right pointers.
    //
    // When in doubt, pass nil for typ. That is safe and will always work.
    //
    // Callers must perform cgo checks if goexperiment.CgoCheck2.
    //
    //go:nosplit
    void bulkBarrierPreWrite(uintptr_t dst, uintptr_t src, uintptr_t size, abi::Type* typ)
    {
        if((dst | src | size) & (goarch::PtrSize - 1) != 0)
        {
            go_throw("bulkBarrierPreWrite: unaligned arguments"_s);
        }
        if(! writeBarrier.enabled)
        {
            return;
        }
        auto s = spanOf(dst);
        if(s == nullptr)
        {
            for(auto [gocpp_ignored, datap] : activeModules())
            {
                if(datap->data <= dst && dst < datap->edata)
                {
                    bulkBarrierBitmap(dst, src, size, dst - datap->data, datap->gcdatamask.bytedata);
                    return;
                }
            }
            for(auto [gocpp_ignored, datap] : activeModules())
            {
                if(datap->bss <= dst && dst < datap->ebss)
                {
                    bulkBarrierBitmap(dst, src, size, dst - datap->bss, datap->gcbssmask.bytedata);
                    return;
                }
            }
            return;
        }
        else
        if(rec::get(gocpp::recv(s->state)) != mSpanInUse || dst < rec::base(gocpp::recv(s)) || s->limit <= dst)
        {
            return;
        }
        auto buf = & rec::ptr(gocpp::recv(getg()->m->p))->wbBuf;
        // Double-check that the bitmaps generated in the two possible paths match.
        auto doubleCheck = false;
        if(doubleCheck)
        {
            doubleCheckTypePointersOfType(s, typ, dst, size);
        }
        typePointers tp = {};
        if(typ != nullptr && typ->Kind_ & kindGCProg == 0)
        {
            tp = rec::typePointersOfType(gocpp::recv(s), typ, dst);
        }
        else
        {
            tp = rec::typePointersOf(gocpp::recv(s), dst, size);
        }
        if(src == 0)
        {
            for(; ; )
            {
                uintptr_t addr = {};
                if(std::tie(tp, addr) = rec::next(gocpp::recv(tp), dst + size); addr == 0)
                {
                    break;
                }
                auto dstx = (uintptr_t*)(unsafe::Pointer(addr));
                auto p = rec::get1(gocpp::recv(buf));
                p[0] = *dstx;
            }
        }
        else
        {
            for(; ; )
            {
                uintptr_t addr = {};
                if(std::tie(tp, addr) = rec::next(gocpp::recv(tp), dst + size); addr == 0)
                {
                    break;
                }
                auto dstx = (uintptr_t*)(unsafe::Pointer(addr));
                auto srcx = (uintptr_t*)(unsafe::Pointer(src + (addr - dst)));
                auto p = rec::get2(gocpp::recv(buf));
                p[0] = *dstx;
                p[1] = *srcx;
            }
        }
    }

    // bulkBarrierPreWriteSrcOnly is like bulkBarrierPreWrite but
    // does not execute write barriers for [dst, dst+size).
    //
    // In addition to the requirements of bulkBarrierPreWrite
    // callers need to ensure [dst, dst+size) is zeroed.
    //
    // This is used for special cases where e.g. dst was just
    // created and zeroed with malloc.
    //
    // The type of the space can be provided purely as an optimization.
    // See bulkBarrierPreWrite's comment for more details -- use this
    // optimization with great care.
    //
    //go:nosplit
    void bulkBarrierPreWriteSrcOnly(uintptr_t dst, uintptr_t src, uintptr_t size, abi::Type* typ)
    {
        if((dst | src | size) & (goarch::PtrSize - 1) != 0)
        {
            go_throw("bulkBarrierPreWrite: unaligned arguments"_s);
        }
        if(! writeBarrier.enabled)
        {
            return;
        }
        auto buf = & rec::ptr(gocpp::recv(getg()->m->p))->wbBuf;
        auto s = spanOf(dst);
        // Double-check that the bitmaps generated in the two possible paths match.
        auto doubleCheck = false;
        if(doubleCheck)
        {
            doubleCheckTypePointersOfType(s, typ, dst, size);
        }
        typePointers tp = {};
        if(typ != nullptr && typ->Kind_ & kindGCProg == 0)
        {
            tp = rec::typePointersOfType(gocpp::recv(s), typ, dst);
        }
        else
        {
            tp = rec::typePointersOf(gocpp::recv(s), dst, size);
        }
        for(; ; )
        {
            uintptr_t addr = {};
            if(std::tie(tp, addr) = rec::next(gocpp::recv(tp), dst + size); addr == 0)
            {
                break;
            }
            auto srcx = (uintptr_t*)(unsafe::Pointer(addr - dst + src));
            auto p = rec::get1(gocpp::recv(buf));
            p[0] = *srcx;
        }
    }

    // initHeapBits initializes the heap bitmap for a span.
    //
    // TODO(mknyszek): This should set the heap bits for single pointer
    // allocations eagerly to avoid calling heapSetType at allocation time,
    // just to write one bit.
    void rec::initHeapBits(golang::runtime::mspan* s, bool forceClear)
    {
        if((! rec::noscan(gocpp::recv(s->spanclass)) && heapBitsInSpan(s->elemsize)) || s->isUserArenaChunk)
        {
            auto b = rec::heapBits(gocpp::recv(s));
            for(auto [i, gocpp_ignored] : b)
            {
                b[i] = 0;
            }
        }
    }

    // bswapIfBigEndian swaps the byte order of the uintptr on goarch.BigEndian platforms,
    // and leaves it alone elsewhere.
    uintptr_t bswapIfBigEndian(uintptr_t x)
    {
        if(goarch::BigEndian)
        {
            if(goarch::PtrSize == 8)
            {
                return uintptr_t(sys::Bswap64(uint64_t(x)));
            }
            return uintptr_t(sys::Bswap32(uint32_t(x)));
        }
        return x;
    }

    
    template<typename T> requires gocpp::GoStruct<T>
    writeUserArenaHeapBits::operator T()
    {
        T result;
        result.offset = this->offset;
        result.mask = this->mask;
        result.valid = this->valid;
        result.low = this->low;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool writeUserArenaHeapBits::operator==(const T& ref) const
    {
        if (offset != ref.offset) return false;
        if (mask != ref.mask) return false;
        if (valid != ref.valid) return false;
        if (low != ref.low) return false;
        return true;
    }

    std::ostream& writeUserArenaHeapBits::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << offset;
        os << " " << mask;
        os << " " << valid;
        os << " " << low;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct writeUserArenaHeapBits& value)
    {
        return value.PrintTo(os);
    }

    struct writeUserArenaHeapBits rec::writeUserArenaHeapBits(golang::runtime::mspan* s, uintptr_t addr)
    {
        struct writeUserArenaHeapBits h;
        auto offset = addr - rec::base(gocpp::recv(s));
        h.low = offset / goarch::PtrSize % ptrBits;
        h.offset = offset - h.low * goarch::PtrSize;
        h.mask = 0;
        h.valid = h.low;
        return h;
    }

    // write appends the pointerness of the next valid pointer slots
    // using the low valid bits of bits. 1=pointer, 0=scalar.
    struct writeUserArenaHeapBits rec::write(golang::runtime::writeUserArenaHeapBits h, struct mspan* s, uintptr_t bits, uintptr_t valid)
    {
        if(h.valid + valid <= ptrBits)
        {
            h.mask |= bits << h.valid;
            h.valid += valid;
            return h;
        }
        auto data = h.mask | (bits << h.valid);
        h.mask = bits >> (ptrBits - h.valid);
        h.valid += valid - ptrBits;
        auto idx = h.offset / (ptrBits * goarch::PtrSize);
        auto m = (uintptr_t(1) << h.low) - 1;
        auto bitmap = rec::heapBits(gocpp::recv(s));
        bitmap[idx] = bswapIfBigEndian(bswapIfBigEndian(bitmap[idx]) & m | data);
        h.offset += ptrBits * goarch::PtrSize;
        h.low = 0;
        return h;
    }

    // Add padding of size bytes.
    struct writeUserArenaHeapBits rec::pad(golang::runtime::writeUserArenaHeapBits h, struct mspan* s, uintptr_t size)
    {
        if(size == 0)
        {
            return h;
        }
        auto words = size / goarch::PtrSize;
        for(; words > ptrBits; )
        {
            h = rec::write(gocpp::recv(h), s, 0, ptrBits);
            words -= ptrBits;
        }
        return rec::write(gocpp::recv(h), s, 0, words);
    }

    // Flush the bits that have been written, and add zeros as needed
    // to cover the full object [addr, addr+size).
    void rec::flush(golang::runtime::writeUserArenaHeapBits h, struct mspan* s, uintptr_t addr, uintptr_t size)
    {
        auto offset = addr - rec::base(gocpp::recv(s));
        auto zeros = (offset + size - h.offset) / goarch::PtrSize - h.valid;
        if(zeros > 0)
        {
            auto z = ptrBits - h.valid;
            if(z > zeros)
            {
                z = zeros;
            }
            h.valid += z;
            zeros -= z;
        }
        auto bitmap = rec::heapBits(gocpp::recv(s));
        auto idx = h.offset / (ptrBits * goarch::PtrSize);
        if(h.valid != h.low)
        {
            auto m = (uintptr_t(1) << h.low) - 1;
            m |= ~ ((uintptr_t(1) << h.valid) - 1);
            bitmap[idx] = bswapIfBigEndian(bswapIfBigEndian(bitmap[idx]) & m | h.mask);
        }
        if(zeros == 0)
        {
            return;
        }
        h.offset += ptrBits * goarch::PtrSize;
        for(; ; )
        {
            auto idx = h.offset / (ptrBits * goarch::PtrSize);
            if(zeros < ptrBits)
            {
                bitmap[idx] = bswapIfBigEndian(bswapIfBigEndian(bitmap[idx]) &^ ((uintptr_t(1) << zeros) - 1));
                break;
            }
            else
            if(zeros == ptrBits)
            {
                bitmap[idx] = 0;
                break;
            }
            else
            {
                bitmap[idx] = 0;
                zeros -= ptrBits;
            }
            h.offset += ptrBits * goarch::PtrSize;
        }
    }

    // heapBits returns the heap ptr/scalar bits stored at the end of the span for
    // small object spans and heap arena spans.
    //
    // Note that the uintptr of each element means something different for small object
    // spans and for heap arena spans. Small object spans are easy: they're never interpreted
    // as anything but uintptr, so they're immune to differences in endianness. However, the
    // heapBits for user arena spans is exposed through a dummy type descriptor, so the byte
    // ordering needs to match the same byte ordering the compiler would emit. The compiler always
    // emits the bitmap data in little endian byte ordering, so on big endian platforms these
    // uintptrs will have their byte orders swapped from what they normally would be.
    //
    // heapBitsInSpan(span.elemsize) or span.isUserArenaChunk must be true.
    //
    //go:nosplit
    gocpp::slice<uintptr_t> rec::heapBits(golang::runtime::mspan* span)
    {
        auto doubleCheck = false;
        if(doubleCheck && ! span->isUserArenaChunk)
        {
            if(rec::noscan(gocpp::recv(span->spanclass)))
            {
                go_throw("heapBits called for noscan"_s);
            }
            if(span->elemsize > minSizeForMallocHeader)
            {
                go_throw("heapBits called for span class that should have a malloc header"_s);
            }
        }
        if(span->npages == 1)
        {
            return heapBitsSlice(rec::base(gocpp::recv(span)), pageSize);
        }
        return heapBitsSlice(rec::base(gocpp::recv(span)), span->npages * pageSize);
    }

    // Helper for constructing a slice for the span's heap bits.
    //
    //go:nosplit
    gocpp::slice<uintptr_t> heapBitsSlice(uintptr_t spanBase, uintptr_t spanSize)
    {
        auto bitmapSize = spanSize / goarch::PtrSize / 8;
        auto elems = int(bitmapSize / goarch::PtrSize);
        notInHeapSlice sl = {};
        sl = notInHeapSlice {(notInHeap*)(unsafe::Pointer(spanBase + spanSize - bitmapSize)), elems, elems};
        return *(gocpp::slice<uintptr_t>*)(unsafe::Pointer(& sl));
    }

    // heapBitsSmallForAddr loads the heap bits for the object stored at addr from span.heapBits.
    //
    // addr must be the base pointer of an object in the span. heapBitsInSpan(span.elemsize)
    // must be true.
    //
    //go:nosplit
    uintptr_t rec::heapBitsSmallForAddr(golang::runtime::mspan* span, uintptr_t addr)
    {
        auto spanSize = span->npages * pageSize;
        auto bitmapSize = spanSize / goarch::PtrSize / 8;
        auto hbits = (unsigned char*)(unsafe::Pointer(rec::base(gocpp::recv(span)) + spanSize - bitmapSize));
        auto i = (addr - rec::base(gocpp::recv(span))) / goarch::PtrSize / ptrBits;
        auto j = (addr - rec::base(gocpp::recv(span))) / goarch::PtrSize % ptrBits;
        auto bits = span->elemsize / goarch::PtrSize;
        auto word0 = (uintptr_t*)(unsafe::Pointer(addb(hbits, goarch::PtrSize * (i + 0))));
        auto word1 = (uintptr_t*)(unsafe::Pointer(addb(hbits, goarch::PtrSize * (i + 1))));
        uintptr_t read = {};
        if(j + bits > ptrBits)
        {
            auto bits0 = ptrBits - j;
            auto bits1 = bits - bits0;
            read = *word0 >> j;
            read |= (*word1 & ((1 << bits1) - 1)) << bits0;
        }
        else
        {
            read = (*word0 >> j) & ((1 << bits) - 1);
        }
        return read;
    }

    // writeHeapBitsSmall writes the heap bits for small objects whose ptr/scalar data is
    // stored as a bitmap at the end of the span.
    //
    // Assumes dataSize is <= ptrBits*goarch.PtrSize. x must be a pointer into the span.
    // heapBitsInSpan(dataSize) must be true. dataSize must be >= typ.Size_.
    //
    //go:nosplit
    uintptr_t rec::writeHeapBitsSmall(golang::runtime::mspan* span, uintptr_t x, uintptr_t dataSize, golang::runtime::_type* typ)
    {
        uintptr_t scanSize;
        auto src0 = readUintptr(typ->GCData);
        auto bits = span->elemsize / goarch::PtrSize;
        scanSize = typ->PtrBytes;
        auto src = src0;
        //Go switch emulation
        {
            auto condition = typ->Size_;
            int conditionId = -1;
            if(condition == goarch::PtrSize) { conditionId = 0; }
            switch(conditionId)
            {
                case 0:
                    src = (1 << (dataSize / goarch::PtrSize)) - 1;
                    break;
                default:
                    for(auto i = typ->Size_; i < dataSize; i += typ->Size_)
                    {
                        src |= src0 << (i / goarch::PtrSize);
                        scanSize += typ->Size_;
                    }
                    break;
            }
        }
        auto dst = rec::heapBits(gocpp::recv(span));
        auto o = (x - rec::base(gocpp::recv(span))) / goarch::PtrSize;
        auto i = o / ptrBits;
        auto j = o % ptrBits;
        if(j + bits > ptrBits)
        {
            auto bits0 = ptrBits - j;
            auto bits1 = bits - bits0;
            dst[i + 0] = dst[i + 0] & (~ uintptr_t(0) >> bits0) | (src << j);
            dst[i + 1] = dst[i + 1] &^ ((1 << bits1) - 1) | (src >> bits0);
        }
        else
        {
            dst[i] = (dst[i] &^ (((1 << bits) - 1) << j)) | (src << j);
        }
        auto doubleCheck = false;
        if(doubleCheck)
        {
            auto srcRead = rec::heapBitsSmallForAddr(gocpp::recv(span), x);
            if(srcRead != src)
            {
                print("runtime: x="_s, hex(x), " i="_s, i, " j="_s, j, " bits="_s, bits, "\n"_s);
                print("runtime: dataSize="_s, dataSize, " typ.Size_="_s, typ->Size_, " typ.PtrBytes="_s, typ->PtrBytes, "\n"_s);
                print("runtime: src0="_s, hex(src0), " src="_s, hex(src), " srcRead="_s, hex(srcRead), "\n"_s);
                go_throw("bad pointer bits written for small object"_s);
            }
        }
        return scanSize;
    }

    // For !goexperiment.AllocHeaders.
    void heapBitsSetType(uintptr_t x, uintptr_t size, uintptr_t dataSize, golang::runtime::_type* typ)
    {
    }

    // heapSetType records that the new allocation [x, x+size)
    // holds in [x, x+dataSize) one or more values of type typ.
    // (The number of values is given by dataSize / typ.Size.)
    // If dataSize < size, the fragment [x+dataSize, x+size) is
    // recorded as non-pointer data.
    // It is known that the type has pointers somewhere;
    // malloc does not call heapSetType when there are no pointers.
    //
    // There can be read-write races between heapSetType and things
    // that read the heap metadata like scanobject. However, since
    // heapSetType is only used for objects that have not yet been
    // made reachable, readers will ignore bits being modified by this
    // function. This does mean this function cannot transiently modify
    // shared memory that belongs to neighboring objects. Also, on weakly-ordered
    // machines, callers must execute a store/store (publication) barrier
    // between calling this function and making the object reachable.
    uintptr_t heapSetType(uintptr_t x, uintptr_t dataSize, golang::runtime::_type* typ, golang::runtime::_type** header, struct mspan* span)
    {
        uintptr_t scanSize;
        auto doubleCheck = false;
        auto gctyp = typ;
        if(header == nullptr)
        {
            if(doubleCheck && (! heapBitsInSpan(dataSize) || ! heapBitsInSpan(span->elemsize)))
            {
                go_throw("tried to write heap bits, but no heap bits in span"_s);
            }
            scanSize = rec::writeHeapBitsSmall(gocpp::recv(span), x, dataSize, typ);
        }
        else
        {
            if(typ->Kind_ & kindGCProg != 0)
            {
                if(rec::sizeclass(gocpp::recv(span->spanclass)) != 0)
                {
                    go_throw("GCProg for type that isn't large"_s);
                }
                auto spaceNeeded = alignUp(gocpp::Sizeof<runtime::_type>(), goarch::PtrSize);
                auto heapBitsOff = spaceNeeded;
                spaceNeeded += alignUp(typ->PtrBytes / goarch::PtrSize / 8, goarch::PtrSize);
                auto npages = alignUp(spaceNeeded, pageSize) / pageSize;
                mspan* progSpan = {};
                systemstack([=]() mutable -> void
                {
                    progSpan = rec::allocManual(gocpp::recv(mheap_), npages, spanAllocPtrScalarBits);
                    memclrNoHeapPointers(unsafe::Pointer(rec::base(gocpp::recv(progSpan))), progSpan->npages * pageSize);
                });
                gctyp = (runtime::_type*)(unsafe::Pointer(rec::base(gocpp::recv(progSpan))));
                gctyp->Size_ = typ->Size_;
                gctyp->PtrBytes = typ->PtrBytes;
                gctyp->GCData = (unsigned char*)(add(unsafe::Pointer(rec::base(gocpp::recv(progSpan))), heapBitsOff));
                gctyp->TFlag = abi::TFlagUnrolledBitmap;
                runGCProg(addb(typ->GCData, 4), gctyp->GCData);
            }
            *header = gctyp;
            scanSize = span->elemsize;
        }
        if(doubleCheck)
        {
            doubleCheckHeapPointers(x, dataSize, gctyp, header, span);
            auto maxIterBytes = span->elemsize;
            if(header == nullptr)
            {
                maxIterBytes = dataSize;
            }
            auto off = alignUp(uintptr_t(cheaprand()) % dataSize, goarch::PtrSize);
            auto size = dataSize - off;
            if(size == 0)
            {
                off -= goarch::PtrSize;
                size += goarch::PtrSize;
            }
            auto interior = x + off;
            size -= alignDown(uintptr_t(cheaprand()) % size, goarch::PtrSize);
            if(size == 0)
            {
                size = goarch::PtrSize;
            }
            size = (size + gctyp->Size_ - 1) / gctyp->Size_ * gctyp->Size_;
            if(interior + size > x + maxIterBytes)
            {
                size = x + maxIterBytes - interior;
            }
            doubleCheckHeapPointersInterior(x, interior, size, dataSize, gctyp, header, span);
        }
        return scanSize;
    }

    void doubleCheckHeapPointers(uintptr_t x, uintptr_t dataSize, golang::runtime::_type* typ, golang::runtime::_type** header, struct mspan* span)
    {
        auto tp = rec::typePointersOfUnchecked(gocpp::recv(span), rec::objBase(gocpp::recv(span), x));
        auto maxIterBytes = span->elemsize;
        if(header == nullptr)
        {
            maxIterBytes = dataSize;
        }
        auto bad = false;
        for(auto i = uintptr_t(0); i < maxIterBytes; i += goarch::PtrSize)
        {
            auto want = false;
            if(i < span->elemsize)
            {
                auto off = i % typ->Size_;
                if(off < typ->PtrBytes)
                {
                    auto j = off / goarch::PtrSize;
                    want = (*addb(typ->GCData, j / 8) >> (j % 8)) & 1 != 0;
                }
            }
            if(want)
            {
                uintptr_t addr = {};
                std::tie(tp, addr) = rec::next(gocpp::recv(tp), x + span->elemsize);
                if(addr == 0)
                {
                    println("runtime: found bad iterator"_s);
                }
                if(addr != x + i)
                {
                    print("runtime: addr="_s, hex(addr), " x+i="_s, hex(x + i), "\n"_s);
                    bad = true;
                }
            }
        }
        if(! bad)
        {
            uintptr_t addr = {};
            std::tie(tp, addr) = rec::next(gocpp::recv(tp), x + span->elemsize);
            if(addr == 0)
            {
                return;
            }
            println("runtime: extra pointer:"_s, hex(addr));
        }
        print("runtime: hasHeader="_s, header != nullptr, " typ.Size_="_s, typ->Size_, " hasGCProg="_s, typ->Kind_ & kindGCProg != 0, "\n"_s);
        print("runtime: x="_s, hex(x), " dataSize="_s, dataSize, " elemsize="_s, span->elemsize, "\n"_s);
        print("runtime: typ="_s, unsafe::Pointer(typ), " typ.PtrBytes="_s, typ->PtrBytes, "\n"_s);
        print("runtime: limit="_s, hex(x + span->elemsize), "\n"_s);
        tp = rec::typePointersOfUnchecked(gocpp::recv(span), x);
        dumpTypePointers(tp);
        for(; ; )
        {
            uintptr_t addr = {};
            if(std::tie(tp, addr) = rec::next(gocpp::recv(tp), x + span->elemsize); addr == 0)
            {
                println("runtime: would've stopped here"_s);
                dumpTypePointers(tp);
                break;
            }
            print("runtime: addr="_s, hex(addr), "\n"_s);
            dumpTypePointers(tp);
        }
        go_throw("heapSetType: pointer entry not correct"_s);
    }

    void doubleCheckHeapPointersInterior(uintptr_t x, uintptr_t interior, uintptr_t size, uintptr_t dataSize, golang::runtime::_type* typ, golang::runtime::_type** header, struct mspan* span)
    {
        auto bad = false;
        if(interior < x)
        {
            print("runtime: interior="_s, hex(interior), " x="_s, hex(x), "\n"_s);
            go_throw("found bad interior pointer"_s);
        }
        auto off = interior - x;
        auto tp = rec::typePointersOf(gocpp::recv(span), interior, size);
        for(auto i = off; i < off + size; i += goarch::PtrSize)
        {
            auto want = false;
            if(i < span->elemsize)
            {
                auto off = i % typ->Size_;
                if(off < typ->PtrBytes)
                {
                    auto j = off / goarch::PtrSize;
                    want = (*addb(typ->GCData, j / 8) >> (j % 8)) & 1 != 0;
                }
            }
            if(want)
            {
                uintptr_t addr = {};
                std::tie(tp, addr) = rec::next(gocpp::recv(tp), interior + size);
                if(addr == 0)
                {
                    println("runtime: found bad iterator"_s);
                    bad = true;
                }
                if(addr != x + i)
                {
                    print("runtime: addr="_s, hex(addr), " x+i="_s, hex(x + i), "\n"_s);
                    bad = true;
                }
            }
        }
        if(! bad)
        {
            uintptr_t addr = {};
            std::tie(tp, addr) = rec::next(gocpp::recv(tp), interior + size);
            if(addr == 0)
            {
                return;
            }
            println("runtime: extra pointer:"_s, hex(addr));
        }
        print("runtime: hasHeader="_s, header != nullptr, " typ.Size_="_s, typ->Size_, "\n"_s);
        print("runtime: x="_s, hex(x), " dataSize="_s, dataSize, " elemsize="_s, span->elemsize, " interior="_s, hex(interior), " size="_s, size, "\n"_s);
        print("runtime: limit="_s, hex(interior + size), "\n"_s);
        tp = rec::typePointersOf(gocpp::recv(span), interior, size);
        dumpTypePointers(tp);
        for(; ; )
        {
            uintptr_t addr = {};
            if(std::tie(tp, addr) = rec::next(gocpp::recv(tp), interior + size); addr == 0)
            {
                println("runtime: would've stopped here"_s);
                dumpTypePointers(tp);
                break;
            }
            print("runtime: addr="_s, hex(addr), "\n"_s);
            dumpTypePointers(tp);
        }
        print("runtime: want: "_s);
        for(auto i = off; i < off + size; i += goarch::PtrSize)
        {
            auto want = false;
            if(i < dataSize)
            {
                auto off = i % typ->Size_;
                if(off < typ->PtrBytes)
                {
                    auto j = off / goarch::PtrSize;
                    want = (*addb(typ->GCData, j / 8) >> (j % 8)) & 1 != 0;
                }
            }
            if(want)
            {
                print("1"_s);
            }
            else
            {
                print("0"_s);
            }
        }
        println();
        go_throw("heapSetType: pointer entry not correct"_s);
    }

    //go:nosplit
    void doubleCheckTypePointersOfType(struct mspan* s, golang::runtime::_type* typ, uintptr_t addr, uintptr_t size)
    {
        if(typ == nullptr || typ->Kind_ & kindGCProg != 0)
        {
            return;
        }
        if(typ->Kind_ & kindMask == kindInterface)
        {
            return;
        }
        auto tp0 = rec::typePointersOfType(gocpp::recv(s), typ, addr);
        auto tp1 = rec::typePointersOf(gocpp::recv(s), addr, size);
        auto failed = false;
        for(; ; )
        {
            uintptr_t addr0 = {};
            uintptr_t addr1 = {};
            std::tie(tp0, addr0) = rec::next(gocpp::recv(tp0), addr + size);
            std::tie(tp1, addr1) = rec::next(gocpp::recv(tp1), addr + size);
            if(addr0 != addr1)
            {
                failed = true;
                break;
            }
            if(addr0 == 0)
            {
                break;
            }
        }
        if(failed)
        {
            auto tp0 = rec::typePointersOfType(gocpp::recv(s), typ, addr);
            auto tp1 = rec::typePointersOf(gocpp::recv(s), addr, size);
            print("runtime: addr="_s, hex(addr), " size="_s, size, "\n"_s);
            print("runtime: type="_s, rec::string(gocpp::recv(toRType(typ))), "\n"_s);
            dumpTypePointers(tp0);
            dumpTypePointers(tp1);
            for(; ; )
            {
                uintptr_t addr0 = {};
                uintptr_t addr1 = {};
                std::tie(tp0, addr0) = rec::next(gocpp::recv(tp0), addr + size);
                std::tie(tp1, addr1) = rec::next(gocpp::recv(tp1), addr + size);
                print("runtime: "_s, hex(addr0), " "_s, hex(addr1), "\n"_s);
                if(addr0 == 0 && addr1 == 0)
                {
                    break;
                }
            }
            go_throw("mismatch between typePointersOfType and typePointersOf"_s);
        }
    }

    void dumpTypePointers(struct typePointers tp)
    {
        print("runtime: tp.elem="_s, hex(tp.elem), " tp.typ="_s, unsafe::Pointer(tp.typ), "\n"_s);
        print("runtime: tp.addr="_s, hex(tp.addr), " tp.mask="_s);
        for(auto i = uintptr_t(0); i < ptrBits; i++)
        {
            if(tp.mask & (uintptr_t(1) << i) != 0)
            {
                print("1"_s);
            }
            else
            {
                print("0"_s);
            }
        }
        println();
    }

    // Returns GC type info for the pointer stored in ep for testing.
    // If ep points to the stack, only static live information will be returned
    // (i.e. not for objects which are only dynamically live stack objects).
    gocpp::slice<unsigned char> getgcmask(go_any ep)
    {
        gocpp::slice<unsigned char> mask;
        auto e = *efaceOf(& ep);
        auto p = e.data;
        auto t = e._type;
        runtime::_type* et = {};
        if(t->Kind_ & kindMask != kindPtr)
        {
            go_throw("bad argument to getgcmask: expected type to be a pointer to the value type whose mask is being queried"_s);
        }
        et = (runtime::ptrtype*)(unsafe::Pointer(t))->Elem;
        for(auto [gocpp_ignored, datap] : activeModules())
        {
            if(datap->data <= uintptr_t(p) && uintptr_t(p) < datap->edata)
            {
                auto bitmap = datap->gcdatamask.bytedata;
                auto n = et->Size_;
                mask = gocpp::make(gocpp::Tag<gocpp::slice<unsigned char>>(), n / goarch::PtrSize);
                for(auto i = uintptr_t(0); i < n; i += goarch::PtrSize)
                {
                    auto off = (uintptr_t(p) + i - datap->data) / goarch::PtrSize;
                    mask[i / goarch::PtrSize] = (*addb(bitmap, off / 8) >> (off % 8)) & 1;
                }
                return mask;
            }
            if(datap->bss <= uintptr_t(p) && uintptr_t(p) < datap->ebss)
            {
                auto bitmap = datap->gcbssmask.bytedata;
                auto n = et->Size_;
                mask = gocpp::make(gocpp::Tag<gocpp::slice<unsigned char>>(), n / goarch::PtrSize);
                for(auto i = uintptr_t(0); i < n; i += goarch::PtrSize)
                {
                    auto off = (uintptr_t(p) + i - datap->bss) / goarch::PtrSize;
                    mask[i / goarch::PtrSize] = (*addb(bitmap, off / 8) >> (off % 8)) & 1;
                }
                return mask;
            }
        }
        if(auto [base, s, gocpp_id_0] = findObject(uintptr_t(p), 0, 0); base != 0)
        {
            if(rec::noscan(gocpp::recv(s->spanclass)))
            {
                return nullptr;
            }
            auto limit = base + s->elemsize;
            auto tp = rec::typePointersOfUnchecked(gocpp::recv(s), base);
            base = tp.addr;
            auto maskFromHeap = gocpp::make(gocpp::Tag<gocpp::slice<unsigned char>>(), (limit - base) / goarch::PtrSize);
            for(; ; )
            {
                uintptr_t addr = {};
                if(std::tie(tp, addr) = rec::next(gocpp::recv(tp), limit); addr == 0)
                {
                    break;
                }
                maskFromHeap[(addr - base) / goarch::PtrSize] = 1;
            }
            for(auto i = limit; i < s->elemsize; i++)
            {
                if(*(unsigned char*)(unsafe::Pointer(i)) != 0)
                {
                    go_throw("found non-zeroed tail of allocation"_s);
                }
            }
            for(; len(maskFromHeap) > 0 && maskFromHeap[len(maskFromHeap) - 1] == 0; )
            {
                maskFromHeap = maskFromHeap.make_slice(0, len(maskFromHeap) - 1);
            }
            if(et->Kind_ & kindGCProg == 0)
            {
                auto maskFromType = gocpp::make(gocpp::Tag<gocpp::slice<unsigned char>>(), (limit - base) / goarch::PtrSize);
                tp = rec::typePointersOfType(gocpp::recv(s), et, base);
                for(; ; )
                {
                    uintptr_t addr = {};
                    if(std::tie(tp, addr) = rec::next(gocpp::recv(tp), limit); addr == 0)
                    {
                        break;
                    }
                    maskFromType[(addr - base) / goarch::PtrSize] = 1;
                }
                auto differs = false;
                for(auto [i, gocpp_ignored] : maskFromHeap)
                {
                    if(maskFromHeap[i] != maskFromType[i])
                    {
                        differs = true;
                        break;
                    }
                }
                if(differs)
                {
                    print("runtime: heap mask="_s);
                    for(auto [gocpp_ignored, b] : maskFromHeap)
                    {
                        print(b);
                    }
                    println();
                    print("runtime: type mask="_s);
                    for(auto [gocpp_ignored, b] : maskFromType)
                    {
                        print(b);
                    }
                    println();
                    print("runtime: type="_s, rec::string(gocpp::recv(toRType(et))), "\n"_s);
                    go_throw("found two different masks from two different methods"_s);
                }
            }
            mask = maskFromHeap;
            KeepAlive(ep);
            return mask;
        }
        if(auto gp = getg(); gp->m->curg->stack.lo <= uintptr_t(p) && uintptr_t(p) < gp->m->curg->stack.hi)
        {
            auto found = false;
            unwinder u = {};
            for(rec::initAt(gocpp::recv(u), gp->m->curg->sched.pc, gp->m->curg->sched.sp, 0, gp->m->curg, 0); rec::valid(gocpp::recv(u)); rec::next(gocpp::recv(u)))
            {
                if(u.frame.sp <= uintptr_t(p) && uintptr_t(p) < u.frame.varp)
                {
                    found = true;
                    break;
                }
            }
            if(found)
            {
                auto [locals, gocpp_id_1, gocpp_id_2] = rec::getStackMap(gocpp::recv(u.frame), false);
                if(locals.n == 0)
                {
                    return mask;
                }
                auto size = uintptr_t(locals.n) * goarch::PtrSize;
                auto n = (runtime::ptrtype*)(unsafe::Pointer(t))->Elem->Size_;
                mask = gocpp::make(gocpp::Tag<gocpp::slice<unsigned char>>(), n / goarch::PtrSize);
                for(auto i = uintptr_t(0); i < n; i += goarch::PtrSize)
                {
                    auto off = (uintptr_t(p) + i - u.frame.varp + size) / goarch::PtrSize;
                    mask[i / goarch::PtrSize] = rec::ptrbit(gocpp::recv(locals), off);
                }
            }
            return mask;
        }
        return mask;
    }

    // userArenaHeapBitsSetType is the equivalent of heapSetType but for
    // non-slice-backing-store Go values allocated in a user arena chunk. It
    // sets up the type metadata for the value with type typ allocated at address ptr.
    // base is the base address of the arena chunk.
    void userArenaHeapBitsSetType(golang::runtime::_type* typ, unsafe::Pointer ptr, struct mspan* s)
    {
        auto base = rec::base(gocpp::recv(s));
        auto h = rec::writeUserArenaHeapBits(gocpp::recv(s), uintptr_t(ptr));
        auto p = typ->GCData;
        uintptr_t gcProgBits = {};
        if(typ->Kind_ & kindGCProg != 0)
        {
            gcProgBits = runGCProg(addb(p, 4), (unsigned char*)(ptr));
            p = (unsigned char*)(ptr);
        }
        auto nb = typ->PtrBytes / goarch::PtrSize;
        for(auto i = uintptr_t(0); i < nb; i += ptrBits)
        {
            auto k = nb - i;
            if(k > ptrBits)
            {
                k = ptrBits;
            }
            h = rec::write(gocpp::recv(h), s, readUintptr(addb(p, i / 8)), k);
        }
        h = rec::pad(gocpp::recv(h), s, typ->Size_ - typ->PtrBytes);
        rec::flush(gocpp::recv(h), s, uintptr_t(ptr), typ->Size_);
        if(typ->Kind_ & kindGCProg != 0)
        {
            memclrNoHeapPointers(ptr, (gcProgBits + 7) / 8);
        }
        s->largeType->PtrBytes = uintptr_t(ptr) - base + typ->PtrBytes;
        // Double-check that the bitmap was written out correctly.
        auto doubleCheck = false;
        if(doubleCheck)
        {
            doubleCheckHeapPointersInterior(uintptr_t(ptr), uintptr_t(ptr), typ->Size_, typ->Size_, typ, & s->largeType, s);
        }
    }

    // For !goexperiment.AllocHeaders, to pass TestIntendedInlining.
    void writeHeapBitsForAddr()
    {
        gocpp::panic("not implemented"_s);
    }

    // For !goexperiment.AllocHeaders.
    
    template<typename T> requires gocpp::GoStruct<T>
    heapBits::operator T()
    {
        T result;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool heapBits::operator==(const T& ref) const
    {
        return true;
    }

    std::ostream& heapBits::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct heapBits& value)
    {
        return value.PrintTo(os);
    }

    // For !goexperiment.AllocHeaders.
    //
    //go:nosplit
    struct heapBits heapBitsForAddr(uintptr_t addr, uintptr_t size)
    {
        gocpp::panic("not implemented"_s);
    }

    // For !goexperiment.AllocHeaders.
    //
    //go:nosplit
    std::tuple<struct heapBits, uintptr_t> rec::next(golang::runtime::heapBits h)
    {
        gocpp::panic("not implemented"_s);
    }

    // For !goexperiment.AllocHeaders.
    //
    //go:nosplit
    std::tuple<struct heapBits, uintptr_t> rec::nextFast(golang::runtime::heapBits h)
    {
        gocpp::panic("not implemented"_s);
    }

}

