// generated by GoCpp from file '$(ImportDir)/runtime/malloc.go'
#include <complex>
#include <functional>
#include <iostream>
#include <iomanip>
#include <map>
#include <string>
#include <tuple>
#include <vector>

#include "golang/runtime/malloc.h"
#include "gocpp/support.h"

#include "golang/internal/abi/type.h"
#include "golang/internal/chacha8rand/chacha8.h"
#include "golang/internal/cpu/cpu.h"
#include "golang/internal/goarch/goarch.h"
#include "golang/internal/goarch/zgoarch_amd64.h"
#include "golang/internal/goexperiment/exp_allocheaders_on.h"
#include "golang/internal/goos/zgoos_windows.h"
#include "golang/runtime/asan0.h"
#include "golang/runtime/cgocall.h"
#include "golang/runtime/chan.h"
#include "golang/runtime/coro.h"
#include "golang/runtime/debuglog_off.h"
#include "golang/runtime/error.h"
#include "golang/runtime/extern.h"
#include "golang/runtime/fastlog2.h"
#include "golang/runtime/internal/atomic/atomic_amd64.h"
#include "golang/runtime/internal/atomic/stubs.h"
#include "golang/runtime/internal/atomic/types.h"
#include "golang/runtime/internal/math/math.h"
#include "golang/runtime/internal/sys/intrinsics.h"
#include "golang/runtime/internal/sys/nih.h"
#include "golang/runtime/lock_sema.h"
#include "golang/runtime/lockrank.h"
#include "golang/runtime/lockrank_off.h"
#include "golang/runtime/mbitmap.h"
#include "golang/runtime/mbitmap_allocheaders.h"
#include "golang/runtime/mcache.h"
#include "golang/runtime/mcentral.h"
#include "golang/runtime/mcheckmark.h"
#include "golang/runtime/mem.h"
#include "golang/runtime/mem_windows.h"
#include "golang/runtime/mfinal.h"
#include "golang/runtime/mfixalloc.h"
#include "golang/runtime/mgc.h"
#include "golang/runtime/mgclimit.h"
#include "golang/runtime/mgcmark.h"
#include "golang/runtime/mgcpacer.h"
#include "golang/runtime/mgcscavenge.h"
#include "golang/runtime/mgcwork.h"
#include "golang/runtime/mheap.h"
#include "golang/runtime/mpagealloc.h"
#include "golang/runtime/mpagecache.h"
#include "golang/runtime/mpallocbits.h"
#include "golang/runtime/mprof.h"
#include "golang/runtime/mranges.h"
#include "golang/runtime/msan0.h"
#include "golang/runtime/mspanset.h"
#include "golang/runtime/mstats.h"
#include "golang/runtime/mwbbuf.h"
#include "golang/runtime/os_windows.h"
#include "golang/runtime/pagetrace_off.h"
#include "golang/runtime/panic.h"
#include "golang/runtime/pinner.h"
#include "golang/runtime/plugin.h"
#include "golang/runtime/print.h"
#include "golang/runtime/proc.h"
#include "golang/runtime/race0.h"
#include "golang/runtime/rand.h"
#include "golang/runtime/runtime1.h"
#include "golang/runtime/runtime2.h"
#include "golang/runtime/signal_windows.h"
#include "golang/runtime/sizeclasses.h"
#include "golang/runtime/slice.h"
#include "golang/runtime/stack.h"
#include "golang/runtime/string.h"
#include "golang/runtime/stubs.h"
#include "golang/runtime/stubs_nonlinux.h"
#include "golang/runtime/symtab.h"
#include "golang/runtime/tagptr.h"
#include "golang/runtime/tagptr_64bit.h"
#include "golang/runtime/time.h"
#include "golang/runtime/trace2buf.h"
#include "golang/runtime/trace2runtime.h"
#include "golang/runtime/trace2status.h"
#include "golang/runtime/trace2time.h"
#include "golang/runtime/type.h"
#include "golang/unsafe/unsafe.h"

namespace golang::runtime
{
    namespace rec
    {
        using namespace mocklib::rec;
        using atomic::rec::Store;
    }

    // _64bit = 1 on 64-bit systems, 0 on 32-bit systems
    // Tiny allocator parameters, see "Tiny allocator" comment in malloc.go.
    // Per-P, per order stack segment cache size.
    // Number of orders that get caching. Order 0 is FixedStack
    // and each successive order is twice as large.
    // We want to cache 2KB, 4KB, 8KB, and 16KB stacks. Larger stacks
    // will be allocated directly.
    // Since FixedStack is different on different systems, we
    // must vary NumStackOrders to keep the same maximum cached size.
    //   OS               | FixedStack | NumStackOrders
    //   -----------------+------------+---------------
    //   linux/darwin/bsd | 2KB        | 4
    //   windows/32       | 4KB        | 3
    //   windows/64       | 8KB        | 2
    //   plan9            | 4KB        | 3
    // heapAddrBits is the number of bits in a heap address. On
    // amd64, addresses are sign-extended beyond heapAddrBits. On
    // other arches, they are zero-extended.
    //
    // On most 64-bit platforms, we limit this to 48 bits based on a
    // combination of hardware and OS limitations.
    //
    // amd64 hardware limits addresses to 48 bits, sign-extended
    // to 64 bits. Addresses where the top 16 bits are not either
    // all 0 or all 1 are "non-canonical" and invalid. Because of
    // these "negative" addresses, we offset addresses by 1<<47
    // (arenaBaseOffset) on amd64 before computing indexes into
    // the heap arenas index. In 2017, amd64 hardware added
    // support for 57 bit addresses; however, currently only Linux
    // supports this extension and the kernel will never choose an
    // address above 1<<47 unless mmap is called with a hint
    // address above 1<<47 (which we never do).
    //
    // arm64 hardware (as of ARMv8) limits user addresses to 48
    // bits, in the range [0, 1<<48).
    //
    // ppc64, mips64, and s390x support arbitrary 64 bit addresses
    // in hardware. On Linux, Go leans on stricter OS limits. Based
    // on Linux's processor.h, the user address space is limited as
    // follows on 64-bit architectures:
    //
    // Architecture  Name              Maximum Value (exclusive)
    // ---------------------------------------------------------------------
    // amd64         TASK_SIZE_MAX     0x007ffffffff000 (47 bit addresses)
    // arm64         TASK_SIZE_64      0x01000000000000 (48 bit addresses)
    // ppc64{,le}    TASK_SIZE_USER64  0x00400000000000 (46 bit addresses)
    // mips64{,le}   TASK_SIZE64       0x00010000000000 (40 bit addresses)
    // s390x         TASK_SIZE         1<<64 (64 bit addresses)
    //
    // These limits may increase over time, but are currently at
    // most 48 bits except on s390x. On all architectures, Linux
    // starts placing mmap'd regions at addresses that are
    // significantly below 48 bits, so even if it's possible to
    // exceed Go's 48 bit limit, it's extremely unlikely in
    // practice.
    //
    // On 32-bit platforms, we accept the full 32-bit address
    // space because doing so is cheap.
    // mips32 only has access to the low 2GB of virtual memory, so
    // we further limit it to 31 bits.
    //
    // On ios/arm64, although 64-bit pointers are presumably
    // available, pointers are truncated to 33 bits in iOS <14.
    // Furthermore, only the top 4 GiB of the address space are
    // actually available to the application. In iOS >=14, more
    // of the address space is available, and the OS can now
    // provide addresses outside of those 33 bits. Pick 40 bits
    // as a reasonable balance between address space usage by the
    // page allocator, and flexibility for what mmap'd regions
    // we'll accept for the heap. We can't just move to the full
    // 48 bits because this uses too much address space for older
    // iOS versions.
    // TODO(mknyszek): Once iOS <14 is deprecated, promote ios/arm64
    // to a 48-bit address space like every other arm64 platform.
    //
    // WebAssembly currently has a limit of 4GB linear memory.
    // maxAlloc is the maximum size of an allocation. On 64-bit,
    // it's theoretically possible to allocate 1<<heapAddrBits bytes. On
    // 32-bit, however, this is one less than 1<<32 because the
    // number of bytes in the address space doesn't actually fit
    // in a uintptr.
    // heapArenaBytes is the size of a heap arena. The heap
    // consists of mappings of size heapArenaBytes, aligned to
    // heapArenaBytes. The initial heap mapping is one arena.
    //
    // This is currently 64MB on 64-bit non-Windows and 4MB on
    // 32-bit and on Windows. We use smaller arenas on Windows
    // because all committed memory is charged to the process,
    // even if it's not touched. Hence, for processes with small
    // heaps, the mapped arena space needs to be commensurate.
    // This is particularly important with the race detector,
    // since it significantly amplifies the cost of committed
    // memory.
    // logHeapArenaBytes is log_2 of heapArenaBytes. For clarity,
    // prefer using heapArenaBytes where possible (we need the
    // constant to compute some other constants).
    // heapArenaBitmapWords is the size of each heap arena's bitmap in uintptrs.
    // arenaL1Bits is the number of bits of the arena number
    // covered by the first level arena map.
    //
    // This number should be small, since the first level arena
    // map requires PtrSize*(1<<arenaL1Bits) of space in the
    // binary's BSS. It can be zero, in which case the first level
    // index is effectively unused. There is a performance benefit
    // to this, since the generated code can be more efficient,
    // but comes at the cost of having a large L2 mapping.
    //
    // We use the L1 map on 64-bit Windows because the arena size
    // is small, but the address space is still 48 bits, and
    // there's a high cost to having a large L2.
    // arenaL2Bits is the number of bits of the arena number
    // covered by the second level arena index.
    //
    // The size of each arena map allocation is proportional to
    // 1<<arenaL2Bits, so it's important that this not be too
    // large. 48 bits leads to 32MB arena index allocations, which
    // is about the practical threshold.
    // arenaL1Shift is the number of bits to shift an arena frame
    // number by to compute an index into the first level arena map.
    // arenaBits is the total bits in a combined arena map index.
    // This is split between the index into the L1 arena map and
    // the L2 arena map.
    // arenaBaseOffset is the pointer value that corresponds to
    // index 0 in the heap arena map.
    //
    // On amd64, the address space is 48 bits, sign extended to 64
    // bits. This offset lets us handle "negative" addresses (or
    // high addresses if viewed as unsigned).
    //
    // On aix/ppc64, this offset allows to keep the heapAddrBits to
    // 48. Otherwise, it would be 60 in order to handle mmap addresses
    // (in range 0x0a00000000000000 - 0x0afffffffffffff). But in this
    // case, the memory reserved in (s *pageAlloc).init for chunks
    // is causing important slowdowns.
    //
    // On other platforms, the user address space is contiguous
    // and starts at 0, so no offset is necessary.
    // A typed version of this constant that will make it into DWARF (for viewcore).
    // Max number of threads to run garbage collection.
    // 2, 3, and 4 are all plausible maximums depending
    // on the hardware details of the machine. The garbage
    // collector scales well to 32 cpus.
    // minLegalPointer is the smallest possible legal pointer.
    // This is the smallest possible architectural page size,
    // since we assume that the first page is never mapped.
    //
    // This should agree with minZeroPage in the compiler.
    // minHeapForMetadataHugePages sets a threshold on when certain kinds of
    // heap metadata, currently the arenas map L2 entries and page alloc bitmap
    // mappings, are allowed to be backed by huge pages. If the heap goal ever
    // exceeds this threshold, then huge pages are enabled.
    //
    // These numbers are chosen with the assumption that huge pages are on the
    // order of a few MiB in size.
    //
    // The kind of metadata this applies to has a very low overhead when compared
    // to address space used, but their constant overheads for small heaps would
    // be very high if they were to be backed by huge pages (e.g. a few MiB makes
    // a huge difference for an 8 MiB heap, but barely any difference for a 1 GiB
    // heap). The benefit of huge pages is also not worth it for small heaps,
    // because only a very, very small part of the metadata is used for small heaps.
    //
    // N.B. If the heap goal exceeds the threshold then shrinks to a very small size
    // again, then huge pages will still be enabled for this mapping. The reason is that
    // there's no point unless we're also returning the physical memory for these
    // metadata mappings back to the OS. That would be quite complex to do in general
    // as the heap is likely fragmented after a reduction in heap size.
    // physPageSize is the size in bytes of the OS's physical pages.
    // Mapping and unmapping operations must be done at multiples of
    // physPageSize.
    //
    // This must be set by the OS init code (typically in osinit) before
    // mallocinit.
    uintptr_t physPageSize;
    // physHugePageSize is the size in bytes of the OS's default physical huge
    // page size whose allocation is opaque to the application. It is assumed
    // and verified to be a power of two.
    //
    // If set, this must be set by the OS init code (typically in osinit) before
    // mallocinit. However, setting it at all is optional, and leaving the default
    // value is always safe (though potentially less efficient).
    //
    // Since physHugePageSize is always assumed to be a power of two,
    // physHugePageShift is defined as physHugePageSize == 1 << physHugePageShift.
    // The purpose of physHugePageShift is to avoid doing divisions in
    // performance critical functions.
    uintptr_t physHugePageSize;
    unsigned int physHugePageShift;
    void mallocinit()
    {
        if(class_to_size[_TinySizeClass] != _TinySize)
        {
            go_throw("bad TinySizeClass"_s);
        }
        if(heapArenaBitmapWords & (heapArenaBitmapWords - 1) != 0)
        {
            go_throw("heapArenaBitmapWords not a power of 2"_s);
        }
        if(physPageSize == 0)
        {
            go_throw("failed to get system page size"_s);
        }
        if(physPageSize > maxPhysPageSize)
        {
            print("system page size ("_s, physPageSize, ") is larger than maximum page size ("_s, maxPhysPageSize, ")\n"_s);
            go_throw("bad system page size"_s);
        }
        if(physPageSize < minPhysPageSize)
        {
            print("system page size ("_s, physPageSize, ") is smaller than minimum page size ("_s, minPhysPageSize, ")\n"_s);
            go_throw("bad system page size"_s);
        }
        if(physPageSize & (physPageSize - 1) != 0)
        {
            print("system page size ("_s, physPageSize, ") must be a power of 2\n"_s);
            go_throw("bad system page size"_s);
        }
        if(physHugePageSize & (physHugePageSize - 1) != 0)
        {
            print("system huge page size ("_s, physHugePageSize, ") must be a power of 2\n"_s);
            go_throw("bad system huge page size"_s);
        }
        if(physHugePageSize > maxPhysHugePageSize)
        {
            physHugePageSize = 0;
        }
        if(physHugePageSize != 0)
        {
            for(; (1 << physHugePageShift) != physHugePageSize; )
            {
                physHugePageShift++;
            }
        }
        if(pagesPerArena % pagesPerSpanRoot != 0)
        {
            print("pagesPerArena ("_s, pagesPerArena, ") is not divisible by pagesPerSpanRoot ("_s, pagesPerSpanRoot, ")\n"_s);
            go_throw("bad pagesPerSpanRoot"_s);
        }
        if(pagesPerArena % pagesPerReclaimerChunk != 0)
        {
            print("pagesPerArena ("_s, pagesPerArena, ") is not divisible by pagesPerReclaimerChunk ("_s, pagesPerReclaimerChunk, ")\n"_s);
            go_throw("bad pagesPerReclaimerChunk"_s);
        }
        if(goexperiment::AllocHeaders)
        {
            auto minSizeForMallocHeaderIsSizeClass = false;
            for(auto i = 0; i < len(class_to_size); i++)
            {
                if(minSizeForMallocHeader == uintptr_t(class_to_size[i]))
                {
                    minSizeForMallocHeaderIsSizeClass = true;
                    break;
                }
            }
            if(! minSizeForMallocHeaderIsSizeClass)
            {
                go_throw("min size of malloc header is not a size class boundary"_s);
            }
            if(minSizeForMallocHeader / goarch::PtrSize > 8 * goarch::PtrSize)
            {
                go_throw("max pointer/scan bitmap size for headerless objects is too large"_s);
            }
        }
        if(minTagBits > taggedPointerBits)
        {
            go_throw("taggedPointerbits too small"_s);
        }
        rec::init(gocpp::recv(mheap_));
        mcache0 = allocmcache();
        lockInit(& gcBitsArenas.lock, lockRankGcBitsArenas);
        lockInit(& profInsertLock, lockRankProfInsert);
        lockInit(& profBlockLock, lockRankProfBlock);
        lockInit(& profMemActiveLock, lockRankProfMemActive);
        for(auto [i, gocpp_ignored] : profMemFutureLock)
        {
            lockInit(& profMemFutureLock[i], lockRankProfMemFuture);
        }
        lockInit(& globalAlloc.mutex, lockRankGlobalAlloc);
        if(goarch::PtrSize == 8)
        {
            for(auto i = 0x7f; i >= 0; i--)
            {
                uintptr_t p = {};
                //Go switch emulation
                {
                    int conditionId = -1;
                    if(raceenabled) { conditionId = 0; }
                    else if(GOARCH == "arm64"_s && GOOS == "ios"_s) { conditionId = 1; }
                    else if(GOARCH == "arm64"_s) { conditionId = 2; }
                    else if(GOOS == "aix"_s) { conditionId = 3; }
                    switch(conditionId)
                    {
                        case 0:
                            p = (uintptr_t(i) << 32) | uintptrMask & (0x00c0 << 32);
                            if(p >= uintptrMask & 0x00e000000000)
                            {
                                continue;
                            }
                            break;
                        case 1:
                            p = (uintptr_t(i) << 40) | uintptrMask & (0x0013 << 28);
                            break;
                        case 2:
                            p = (uintptr_t(i) << 40) | uintptrMask & (0x0040 << 32);
                            break;
                        case 3:
                            if(i == 0)
                            {
                                continue;
                            }
                            p = (uintptr_t(i) << 40) | uintptrMask & (0xa0 << 52);
                            break;
                        default:
                            p = (uintptr_t(i) << 40) | uintptrMask & (0x00c0 << 32);
                            break;
                    }
                }
                auto hintList = & mheap_.arenaHints;
                if((! raceenabled && i > 0x3f) || (raceenabled && i > 0x5f))
                {
                    hintList = & mheap_.userArena.arenaHints;
                }
                auto hint = (arenaHint*)(rec::alloc(gocpp::recv(mheap_.arenaHintAlloc)));
                hint->addr = p;
                std::tie(hint->next, *hintList) = std::tuple{*hintList, hint};
            }
        }
        else
        {
            auto arenaMetaSize = (1 << arenaBits) * gocpp::Sizeof<heapArena>();
            auto meta = uintptr_t(sysReserve(nullptr, arenaMetaSize));
            if(meta != 0)
            {
                rec::init(gocpp::recv(mheap_.heapArenaAlloc), meta, arenaMetaSize, true);
            }
            auto procBrk = sbrk0();
            auto p = firstmoduledata.end;
            if(p < procBrk)
            {
                p = procBrk;
            }
            if(mheap_.heapArenaAlloc.next <= p && p < mheap_.heapArenaAlloc.end)
            {
                p = mheap_.heapArenaAlloc.end;
            }
            p = alignUp(p + (256 << 10), heapArenaBytes);
            auto arenaSizes = gocpp::slice<uintptr_t> {512 << 20, 256 << 20, 128 << 20};
            for(auto [gocpp_ignored, arenaSize] : arenaSizes)
            {
                auto [a, size] = sysReserveAligned(unsafe::Pointer(p), arenaSize, heapArenaBytes);
                if(a != nullptr)
                {
                    rec::init(gocpp::recv(mheap_.arena), uintptr_t(a), size, false);
                    p = mheap_.arena.end;
                    break;
                }
            }
            auto hint = (arenaHint*)(rec::alloc(gocpp::recv(mheap_.arenaHintAlloc)));
            hint->addr = p;
            std::tie(hint->next, mheap_.arenaHints) = std::tuple{mheap_.arenaHints, hint};
            auto userArenaHint = (arenaHint*)(rec::alloc(gocpp::recv(mheap_.arenaHintAlloc)));
            userArenaHint->addr = p;
            std::tie(userArenaHint->next, mheap_.userArena.arenaHints) = std::tuple{mheap_.userArena.arenaHints, userArenaHint};
        }
        rec::Store(gocpp::recv(gcController.memoryLimit), maxInt64);
    }

    // sysAlloc allocates heap arena space for at least n bytes. The
    // returned pointer is always heapArenaBytes-aligned and backed by
    // h.arenas metadata. The returned size is always a multiple of
    // heapArenaBytes. sysAlloc returns nil on failure.
    // There is no corresponding free function.
    //
    // hintList is a list of hint addresses for where to allocate new
    // heap arenas. It must be non-nil.
    //
    // register indicates whether the heap arena should be registered
    // in allArenas.
    //
    // sysAlloc returns a memory region in the Reserved state. This region must
    // be transitioned to Prepared and then Ready before use.
    //
    // h must be locked.
    std::tuple<unsafe::Pointer, uintptr_t> rec::sysAlloc(struct mheap* h, uintptr_t n, struct arenaHint** hintList, bool go_register)
    {
        unsafe::Pointer v;
        uintptr_t size;
        assertLockHeld(& h->lock);
        n = alignUp(n, heapArenaBytes);
        if(hintList == & h->arenaHints)
        {
            v = rec::alloc(gocpp::recv(h->arena), n, heapArenaBytes, & gcController.heapReleased);
            if(v != nullptr)
            {
                size = n;
                goto mapped;
            }
        }
        for(; *hintList != nullptr; )
        {
            auto hint = *hintList;
            auto p = hint->addr;
            if(hint->down)
            {
                p -= n;
            }
            if(p + n < p)
            {
                v = nullptr;
            }
            else
            if(arenaIndex(p + n - 1) >= (1 << arenaBits))
            {
                v = nullptr;
            }
            else
            {
                v = sysReserve(unsafe::Pointer(p), n);
            }
            if(p == uintptr_t(v))
            {
                if(! hint->down)
                {
                    p += n;
                }
                hint->addr = p;
                size = n;
                break;
            }
            if(v != nullptr)
            {
                sysFreeOS(v, n);
            }
            *hintList = hint->next;
            rec::free(gocpp::recv(h->arenaHintAlloc), unsafe::Pointer(hint));
        }
        if(size == 0)
        {
            if(raceenabled)
            {
                go_throw("too many address space collisions for -race mode"_s);
            }
            std::tie(v, size) = sysReserveAligned(nullptr, n, heapArenaBytes);
            if(v == nullptr)
            {
                return {nullptr, 0};
            }
            auto hint = (arenaHint*)(rec::alloc(gocpp::recv(h->arenaHintAlloc)));
            std::tie(hint->addr, hint->down) = std::tuple{uintptr_t(v), true};
            std::tie(hint->next, mheap_.arenaHints) = std::tuple{mheap_.arenaHints, hint};
            hint = (arenaHint*)(rec::alloc(gocpp::recv(h->arenaHintAlloc)));
            hint->addr = uintptr_t(v) + size;
            std::tie(hint->next, mheap_.arenaHints) = std::tuple{mheap_.arenaHints, hint};
        }
        {
            gocpp::string bad = {};
            auto p = uintptr_t(v);
            if(p + size < p)
            {
                bad = "region exceeds uintptr range"_s;
            }
            else
            if(arenaIndex(p) >= (1 << arenaBits))
            {
                bad = "base outside usable address space"_s;
            }
            else
            if(arenaIndex(p + size - 1) >= (1 << arenaBits))
            {
                bad = "end outside usable address space"_s;
            }
            if(bad != ""_s)
            {
                print("runtime: memory allocated by OS ["_s, hex(p), ", "_s, hex(p + size), ") not in usable address space: "_s, bad, "\n"_s);
                go_throw("memory reservation exceeds address space limit"_s);
            }
        }
        if(uintptr_t(v) & (heapArenaBytes - 1) != 0)
        {
            go_throw("misrounded allocation in sysAlloc"_s);
        }
        mapped:
        for(auto ri = arenaIndex(uintptr_t(v)); ri <= arenaIndex(uintptr_t(v) + size - 1); ri++)
        {
            auto l2 = h->arenas[rec::l1(gocpp::recv(ri))];
            if(l2 == nullptr)
            {
                l2 = (gocpp::array<heapArena*, 1 << arenaL2Bits>*)(sysAllocOS(gocpp::Sizeof<gocpp::array<*runtime::heapArena, 1048576>>()));
                if(l2 == nullptr)
                {
                    go_throw("out of memory allocating heap arena map"_s);
                }
                if(h->arenasHugePages)
                {
                    sysHugePage(unsafe::Pointer(l2), gocpp::Sizeof<gocpp::array<*runtime::heapArena, 1048576>>());
                }
                else
                {
                    sysNoHugePage(unsafe::Pointer(l2), gocpp::Sizeof<gocpp::array<*runtime::heapArena, 1048576>>());
                }
                atomic::StorepNoWB(unsafe::Pointer(& h->arenas[rec::l1(gocpp::recv(ri))]), unsafe::Pointer(l2));
            }
            if(l2[rec::l2(gocpp::recv(ri))] != nullptr)
            {
                go_throw("arena already initialized"_s);
            }
            heapArena* r = {};
            r = (heapArena*)(rec::alloc(gocpp::recv(h->heapArenaAlloc), gocpp::Sizeof<heapArena>(), goarch::PtrSize, & memstats.gcMiscSys));
            if(r == nullptr)
            {
                r = (heapArena*)(persistentalloc(gocpp::Sizeof<heapArena>(), goarch::PtrSize, & memstats.gcMiscSys));
                if(r == nullptr)
                {
                    go_throw("out of memory allocating heap arena metadata"_s);
                }
            }
            if(go_register)
            {
                if(len(h->allArenas) == cap(h->allArenas))
                {
                    auto size = 2 * uintptr_t(cap(h->allArenas)) * goarch::PtrSize;
                    if(size == 0)
                    {
                        size = physPageSize;
                    }
                    auto newArray = (notInHeap*)(persistentalloc(size, goarch::PtrSize, & memstats.gcMiscSys));
                    if(newArray == nullptr)
                    {
                        go_throw("out of memory allocating allArenas"_s);
                    }
                    auto oldSlice = h->allArenas;
                    *(notInHeapSlice*)(unsafe::Pointer(& h->allArenas)) = notInHeapSlice {newArray, len(h->allArenas), int(size / goarch::PtrSize)};
                    copy(h->allArenas, oldSlice);
                }
                h->allArenas = h->allArenas.make_slice(0, len(h->allArenas) + 1);
                h->allArenas[len(h->allArenas) - 1] = ri;
            }
            atomic::StorepNoWB(unsafe::Pointer(& l2[rec::l2(gocpp::recv(ri))]), unsafe::Pointer(r));
            if(false) {
            mapped_continue:
                continue;
            mapped_break:
                break;
            }
        }
        if(raceenabled)
        {
            racemapshadow(v, size);
        }
        return {v, size};
    }

    // sysReserveAligned is like sysReserve, but the returned pointer is
    // aligned to align bytes. It may reserve either n or n+align bytes,
    // so it returns the size that was reserved.
    std::tuple<unsafe::Pointer, uintptr_t> sysReserveAligned(unsafe::Pointer v, uintptr_t size, uintptr_t align)
    {
        auto retries = 0;
        retry:
        auto p = uintptr_t(sysReserve(v, size + align));
        //Go switch emulation
        {
            int conditionId = -1;
            if(p == 0) { conditionId = 0; }
            else if(p & (align - 1) == 0) { conditionId = 1; }
            else if(GOOS == "windows"_s) { conditionId = 2; }
            switch(conditionId)
            {
                case 0:
                    return {nullptr, 0};
                    break;
                case 1:
                    return {unsafe::Pointer(p), size + align};
                    break;
                case 2:
                    sysFreeOS(unsafe::Pointer(p), size + align);
                    p = alignUp(p, align);
                    auto p2 = sysReserve(unsafe::Pointer(p), size);
                    if(p != uintptr_t(p2))
                    {
                        sysFreeOS(p2, size);
                        if(retries++; retries == 100)
                        {
                            go_throw("failed to allocate aligned heap memory; too many retries"_s);
                        }
                        goto retry;
                    }
                    return {p2, size};
                    break;
                default:
                    auto pAligned = alignUp(p, align);
                    sysFreeOS(unsafe::Pointer(p), pAligned - p);
                    auto end = pAligned + size;
                    auto endLen = (p + size + align) - end;
                    if(endLen > 0)
                    {
                        sysFreeOS(unsafe::Pointer(end), endLen);
                    }
                    return {unsafe::Pointer(pAligned), size};
                    break;
            }
        }
    }

    // enableMetadataHugePages enables huge pages for various sources of heap metadata.
    //
    // A note on latency: for sufficiently small heaps (<10s of GiB) this function will take constant
    // time, but may take time proportional to the size of the mapped heap beyond that.
    //
    // This function is idempotent.
    //
    // The heap lock must not be held over this operation, since it will briefly acquire
    // the heap lock.
    //
    // Must be called on the system stack because it acquires the heap lock.
    //
    //go:systemstack
    void rec::enableMetadataHugePages(struct mheap* h)
    {
        rec::enableChunkHugePages(gocpp::recv(h->pages));
        lock(& h->lock);
        if(h->arenasHugePages)
        {
            unlock(& h->lock);
            return;
        }
        h->arenasHugePages = true;
        unlock(& h->lock);
        for(auto [i, gocpp_ignored] : h->arenas)
        {
            auto l2 = (gocpp::array<heapArena*, 1 << arenaL2Bits>*)(atomic::Loadp(unsafe::Pointer(& h->arenas[i])));
            if(l2 == nullptr)
            {
                continue;
            }
            sysHugePage(unsafe::Pointer(l2), gocpp::Sizeof<gocpp::array<*runtime::heapArena, 1048576>>());
        }
    }

    // base address for all 0-byte allocations
    uintptr_t zerobase;
    // nextFreeFast returns the next free object if one is quickly available.
    // Otherwise it returns 0.
    runtime::gclinkptr nextFreeFast(struct mspan* s)
    {
        auto theBit = sys::TrailingZeros64(s->allocCache);
        if(theBit < 64)
        {
            auto result = s->freeindex + uint16_t(theBit);
            if(result < s->nelems)
            {
                auto freeidx = result + 1;
                if(freeidx % 64 == 0 && freeidx != s->nelems)
                {
                    return 0;
                }
                s->allocCache >>= (unsigned int)(theBit + 1);
                s->freeindex = freeidx;
                s->allocCount++;
                return gclinkptr(uintptr_t(result) * s->elemsize + rec::base(gocpp::recv(s)));
            }
        }
        return 0;
    }

    // nextFree returns the next free object from the cached span if one is available.
    // Otherwise it refills the cache with a span with an available object and
    // returns that object along with a flag indicating that this was a heavy
    // weight allocation. If it is a heavy weight allocation the caller must
    // determine whether a new GC cycle needs to be started or if the GC is active
    // whether this goroutine needs to assist the GC.
    //
    // Must run in a non-preemptible context since otherwise the owner of
    // c could change.
    std::tuple<runtime::gclinkptr, struct mspan*, bool> rec::nextFree(struct mcache* c, golang::runtime::spanClass spc)
    {
        runtime::gclinkptr v;
        struct mspan* s;
        bool shouldhelpgc;
        s = c->alloc[spc];
        shouldhelpgc = false;
        auto freeIndex = rec::nextFreeIndex(gocpp::recv(s));
        if(freeIndex == s->nelems)
        {
            if(s->allocCount != s->nelems)
            {
                println("runtime: s.allocCount="_s, s->allocCount, "s.nelems="_s, s->nelems);
                go_throw("s.allocCount != s.nelems && freeIndex == s.nelems"_s);
            }
            rec::refill(gocpp::recv(c), spc);
            shouldhelpgc = true;
            s = c->alloc[spc];
            freeIndex = rec::nextFreeIndex(gocpp::recv(s));
        }
        if(freeIndex >= s->nelems)
        {
            go_throw("freeIndex is not valid"_s);
        }
        v = gclinkptr(uintptr_t(freeIndex) * s->elemsize + rec::base(gocpp::recv(s)));
        s->allocCount++;
        if(s->allocCount > s->nelems)
        {
            println("s.allocCount="_s, s->allocCount, "s.nelems="_s, s->nelems);
            go_throw("s.allocCount > s.nelems"_s);
        }
        return {v, s, shouldhelpgc};
    }

    // Allocate an object of size bytes.
    // Small objects are allocated from the per-P cache's free lists.
    // Large objects (> 32 kB) are allocated straight from the heap.
    unsafe::Pointer mallocgc(uintptr_t size, golang::runtime::_type* typ, bool needzero)
    {
        if(gcphase == _GCmarktermination)
        {
            go_throw("mallocgc called with gcphase == _GCmarktermination"_s);
        }
        if(size == 0)
        {
            return unsafe::Pointer(& zerobase);
        }
        lockRankMayQueueFinalizer();
        auto userSize = size;
        if(asanenabled)
        {
            size += computeRZlog(size);
        }
        if(debug.malloc)
        {
            if(debug.sbrk != 0)
            {
                auto align = uintptr_t(16);
                if(typ != nullptr)
                {
                    if(size & 7 == 0)
                    {
                        align = 8;
                    }
                    else
                    if(size & 3 == 0)
                    {
                        align = 4;
                    }
                    else
                    if(size & 1 == 0)
                    {
                        align = 2;
                    }
                    else
                    {
                        align = 1;
                    }
                }
                return persistentalloc(size, align, & memstats.other_sys);
            }
            if(inittrace.active && inittrace.id == getg()->goid)
            {
                inittrace.allocs += 1;
            }
        }
        auto assistG = deductAssistCredit(size);
        auto mp = acquirem();
        if(mp->mallocing != 0)
        {
            go_throw("malloc deadlock"_s);
        }
        if(mp->gsignal == getg())
        {
            go_throw("malloc during signal"_s);
        }
        mp->mallocing = 1;
        auto shouldhelpgc = false;
        auto dataSize = userSize;
        auto c = getMCache(mp);
        if(c == nullptr)
        {
            go_throw("mallocgc called without a P or outside bootstrapping"_s);
        }
        mspan* span = {};
        runtime::_type** header = {};
        unsafe::Pointer x = {};
        auto noscan = typ == nullptr || typ->PtrBytes == 0;
        auto delayedZeroing = false;
        if(size <= maxSmallSize - mallocHeaderSize)
        {
            if(noscan && size < maxTinySize)
            {
                auto off = c->tinyoffset;
                if(size & 7 == 0)
                {
                    off = alignUp(off, 8);
                }
                else
                if(goarch::PtrSize == 4 && size == 12)
                {
                    off = alignUp(off, 8);
                }
                else
                if(size & 3 == 0)
                {
                    off = alignUp(off, 4);
                }
                else
                if(size & 1 == 0)
                {
                    off = alignUp(off, 2);
                }
                if(off + size <= maxTinySize && c->tiny != 0)
                {
                    x = unsafe::Pointer(c->tiny + off);
                    c->tinyoffset = off + size;
                    c->tinyAllocs++;
                    mp->mallocing = 0;
                    releasem(mp);
                    return x;
                }
                span = c->alloc[tinySpanClass];
                auto v = nextFreeFast(span);
                if(v == 0)
                {
                    std::tie(v, span, shouldhelpgc) = rec::nextFree(gocpp::recv(c), tinySpanClass);
                }
                x = unsafe::Pointer(v);
                (gocpp::array<uint64_t, 2>*)(x)[0] = 0;
                (gocpp::array<uint64_t, 2>*)(x)[1] = 0;
                if(! raceenabled && (size < c->tinyoffset || c->tiny == 0))
                {
                    c->tiny = uintptr_t(x);
                    c->tinyoffset = size;
                }
                size = maxTinySize;
            }
            else
            {
                auto hasHeader = ! noscan && ! heapBitsInSpan(size);
                if(goexperiment::AllocHeaders && hasHeader)
                {
                    size += mallocHeaderSize;
                }
                uint8_t sizeclass = {};
                if(size <= smallSizeMax - 8)
                {
                    sizeclass = size_to_class8[divRoundUp(size, smallSizeDiv)];
                }
                else
                {
                    sizeclass = size_to_class128[divRoundUp(size - smallSizeMax, largeSizeDiv)];
                }
                size = uintptr_t(class_to_size[sizeclass]);
                auto spc = makeSpanClass(sizeclass, noscan);
                span = c->alloc[spc];
                auto v = nextFreeFast(span);
                if(v == 0)
                {
                    std::tie(v, span, shouldhelpgc) = rec::nextFree(gocpp::recv(c), spc);
                }
                x = unsafe::Pointer(v);
                if(needzero && span->needzero != 0)
                {
                    memclrNoHeapPointers(x, size);
                }
                if(goexperiment::AllocHeaders && hasHeader)
                {
                    header = (runtime::_type**)(x);
                    x = add(x, mallocHeaderSize);
                    size -= mallocHeaderSize;
                }
            }
        }
        else
        {
            shouldhelpgc = true;
            span = rec::allocLarge(gocpp::recv(c), size, noscan);
            span->freeindex = 1;
            span->allocCount = 1;
            size = span->elemsize;
            x = unsafe::Pointer(rec::base(gocpp::recv(span)));
            if(needzero && span->needzero != 0)
            {
                if(noscan)
                {
                    delayedZeroing = true;
                }
                else
                {
                    memclrNoHeapPointers(x, size);
                }
            }
            if(goexperiment::AllocHeaders && ! noscan)
            {
                header = & span->largeType;
            }
        }
        if(! noscan)
        {
            if(goexperiment::AllocHeaders)
            {
                c->scanAlloc += heapSetType(uintptr_t(x), dataSize, typ, header, span);
            }
            else
            {
                uintptr_t scanSize = {};
                heapBitsSetType(uintptr_t(x), size, dataSize, typ);
                if(dataSize > typ->Size_)
                {
                    if(typ->PtrBytes != 0)
                    {
                        scanSize = dataSize - typ->Size_ + typ->PtrBytes;
                    }
                }
                else
                {
                    scanSize = typ->PtrBytes;
                }
                c->scanAlloc += scanSize;
            }
        }
        publicationBarrier();
        span->freeIndexForScan = span->freeindex;
        if(gcphase != _GCoff)
        {
            gcmarknewobject(span, uintptr_t(x));
        }
        if(raceenabled)
        {
            racemalloc(x, size);
        }
        if(msanenabled)
        {
            msanmalloc(x, size);
        }
        if(asanenabled)
        {
            auto rzBeg = unsafe::Add(x, userSize);
            asanpoison(rzBeg, size - userSize);
            asanunpoison(x, userSize);
        }
        auto fullSize = size;
        if(goexperiment::AllocHeaders)
        {
            fullSize = span->elemsize;
        }
        if(auto rate = MemProfileRate; rate > 0)
        {
            if(rate != 1 && fullSize < c->nextSample)
            {
                c->nextSample -= fullSize;
            }
            else
            {
                profilealloc(mp, x, fullSize);
            }
        }
        mp->mallocing = 0;
        releasem(mp);
        if(delayedZeroing)
        {
            if(! noscan)
            {
                go_throw("delayed zeroing on data that may contain pointers"_s);
            }
            if(goexperiment::AllocHeaders && header != nullptr)
            {
                go_throw("unexpected malloc header in delayed zeroing of large object"_s);
            }
            memclrNoHeapPointersChunked(size, x);
        }
        if(debug.malloc)
        {
            if(debug.allocfreetrace != 0)
            {
                tracealloc(x, size, typ);
            }
            if(inittrace.active && inittrace.id == getg()->goid)
            {
                inittrace.bytes += uint64_t(fullSize);
            }
        }
        if(assistG != nullptr)
        {
            assistG->gcAssistBytes -= int64_t(fullSize - dataSize);
        }
        if(shouldhelpgc)
        {
            if(auto t = (gocpp::Init<gcTrigger>([=](auto& x) {
                x.kind = gcTriggerHeap;
            })); rec::test(gocpp::recv(t)))
            {
                gcStart(t);
            }
        }
        if(raceenabled && noscan && dataSize < maxTinySize)
        {
            x = add(x, size - dataSize);
        }
        return x;
    }

    // deductAssistCredit reduces the current G's assist credit
    // by size bytes, and assists the GC if necessary.
    //
    // Caller must be preemptible.
    //
    // Returns the G for which the assist credit was accounted.
    struct g* deductAssistCredit(uintptr_t size)
    {
        g* assistG = {};
        if(gcBlackenEnabled != 0)
        {
            assistG = getg();
            if(assistG->m->curg != nullptr)
            {
                assistG = assistG->m->curg;
            }
            assistG->gcAssistBytes -= int64_t(size);
            if(assistG->gcAssistBytes < 0)
            {
                gcAssistAlloc(assistG);
            }
        }
        return assistG;
    }

    // memclrNoHeapPointersChunked repeatedly calls memclrNoHeapPointers
    // on chunks of the buffer to be zeroed, with opportunities for preemption
    // along the way.  memclrNoHeapPointers contains no safepoints and also
    // cannot be preemptively scheduled, so this provides a still-efficient
    // block copy that can also be preempted on a reasonable granularity.
    //
    // Use this with care; if the data being cleared is tagged to contain
    // pointers, this allows the GC to run before it is all cleared.
    void memclrNoHeapPointersChunked(uintptr_t size, unsafe::Pointer x)
    {
        auto v = uintptr_t(x);
        // got this from benchmarking. 128k is too small, 512k is too large.
        auto chunkBytes = 256 * 1024;
        auto vsize = v + size;
        for(auto voff = v; voff < vsize; voff = voff + chunkBytes)
        {
            if(getg()->preempt)
            {
                goschedguarded();
            }
            auto n = vsize - voff;
            if(n > chunkBytes)
            {
                n = chunkBytes;
            }
            memclrNoHeapPointers(unsafe::Pointer(voff), n);
        }
    }

    // implementation of new builtin
    // compiler (both frontend and SSA backend) knows the signature
    // of this function.
    unsafe::Pointer newobject(golang::runtime::_type* typ)
    {
        return mallocgc(typ->Size_, typ, true);
    }

    //go:linkname reflect_unsafe_New reflect.unsafe_New
    unsafe::Pointer reflect_unsafe_New(golang::runtime::_type* typ)
    {
        return mallocgc(typ->Size_, typ, true);
    }

    //go:linkname reflectlite_unsafe_New internal/reflectlite.unsafe_New
    unsafe::Pointer reflectlite_unsafe_New(golang::runtime::_type* typ)
    {
        return mallocgc(typ->Size_, typ, true);
    }

    // newarray allocates an array of n elements of type typ.
    unsafe::Pointer newarray(golang::runtime::_type* typ, int n)
    {
        if(n == 1)
        {
            return mallocgc(typ->Size_, typ, true);
        }
        auto [mem, overflow] = math::MulUintptr(typ->Size_, uintptr_t(n));
        if(overflow || mem > maxAlloc || n < 0)
        {
            gocpp::panic(plainError("runtime: allocation size out of range"_s));
        }
        return mallocgc(mem, typ, true);
    }

    //go:linkname reflect_unsafe_NewArray reflect.unsafe_NewArray
    unsafe::Pointer reflect_unsafe_NewArray(golang::runtime::_type* typ, int n)
    {
        return newarray(typ, n);
    }

    void profilealloc(struct m* mp, unsafe::Pointer x, uintptr_t size)
    {
        auto c = getMCache(mp);
        if(c == nullptr)
        {
            go_throw("profilealloc called without a P or outside bootstrapping"_s);
        }
        c->nextSample = nextSample();
        mProf_Malloc(x, size);
    }

    // nextSample returns the next sampling point for heap profiling. The goal is
    // to sample allocations on average every MemProfileRate bytes, but with a
    // completely random distribution over the allocation timeline; this
    // corresponds to a Poisson process with parameter MemProfileRate. In Poisson
    // processes, the distance between two samples follows the exponential
    // distribution (exp(MemProfileRate)), so the best return value is a random
    // number taken from an exponential distribution whose mean is MemProfileRate.
    uintptr_t nextSample()
    {
        if(MemProfileRate == 1)
        {
            return 0;
        }
        if(GOOS == "plan9"_s)
        {
            if(auto gp = getg(); gp == gp->m->gsignal)
            {
                return nextSampleNoFP();
            }
        }
        return uintptr_t(fastexprand(MemProfileRate));
    }

    // fastexprand returns a random number from an exponential distribution with
    // the specified mean.
    int32_t fastexprand(int mean)
    {
        //Go switch emulation
        {
            int conditionId = -1;
            if(mean > 0x7000000) { conditionId = 0; }
            else if(mean == 0) { conditionId = 1; }
            switch(conditionId)
            {
                case 0:
                    mean = 0x7000000;
                    break;
                case 1:
                    return 0;
                    break;
            }
        }
        // Take a random sample of the exponential distribution exp(-mean*x).
        // The probability distribution function is mean*exp(-mean*x), so the CDF is
        // p = 1 - exp(-mean*x), so
        // q = 1 - p == exp(-mean*x)
        // log_e(q) = -mean*x
        // -log_e(q)/mean = x
        // x = -log_e(q) * mean
        // x = log_2(q) * (-log_e(2)) * mean    ; Using log_2 for efficiency
        auto randomBitCount = 26;
        auto q = cheaprandn(1 << randomBitCount) + 1;
        auto qlog = fastlog2(double(q)) - randomBitCount;
        if(qlog > 0)
        {
            qlog = 0;
        }
        auto minusLog2 = - 0.6931471805599453;
        return int32_t(qlog * (minusLog2 * double(mean))) + 1;
    }

    // nextSampleNoFP is similar to nextSample, but uses older,
    // simpler code to avoid floating point.
    uintptr_t nextSampleNoFP()
    {
        auto rate = MemProfileRate;
        if(rate > 0x3fffffff)
        {
            rate = 0x3fffffff;
        }
        if(rate != 0)
        {
            return uintptr_t(cheaprandn(uint32_t(2 * rate)));
        }
        return 0;
    }

    
    template<typename T> requires gocpp::GoStruct<T>
    persistentAlloc::operator T()
    {
        T result;
        result.base = this->base;
        result.off = this->off;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool persistentAlloc::operator==(const T& ref) const
    {
        if (base != ref.base) return false;
        if (off != ref.off) return false;
        return true;
    }

    std::ostream& persistentAlloc::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << base;
        os << " " << off;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct persistentAlloc& value)
    {
        return value.PrintTo(os);
    }

    struct gocpp_id_0
    {
        mutex mutex;
        persistentAlloc persistentAlloc;

        using isGoStruct = void;

        template<typename T> requires gocpp::GoStruct<T>
        operator T()
        {
            T result;
            result.mutex = this->mutex;
            result.persistentAlloc = this->persistentAlloc;
            return result;
        }

        template<typename T> requires gocpp::GoStruct<T>
        bool operator==(const T& ref) const
        {
            if (mutex != ref.mutex) return false;
            if (persistentAlloc != ref.persistentAlloc) return false;
            return true;
        }

        std::ostream& PrintTo(std::ostream& os) const
        {
            os << '{';
            os << "" << mutex;
            os << " " << persistentAlloc;
            os << '}';
            return os;
        }
    };

    std::ostream& operator<<(std::ostream& os, const struct gocpp_id_0& value)
    {
        return value.PrintTo(os);
    }


    gocpp_id_0 globalAlloc;
    // persistentChunkSize is the number of bytes we allocate when we grow
    // a persistentAlloc.
    // persistentChunks is a list of all the persistent chunks we have
    // allocated. The list is maintained through the first word in the
    // persistent chunk. This is updated atomically.
    notInHeap* persistentChunks;
    // Wrapper around sysAlloc that can allocate small chunks.
    // There is no associated free operation.
    // Intended for things like function/type/debug-related persistent data.
    // If align is 0, uses default align (currently 8).
    // The returned memory will be zeroed.
    // sysStat must be non-nil.
    //
    // Consider marking persistentalloc'd types not in heap by embedding
    // runtime/internal/sys.NotInHeap.
    unsafe::Pointer persistentalloc(uintptr_t size, uintptr_t align, golang::runtime::sysMemStat* sysStat)
    {
        notInHeap* p = {};
        systemstack([=]() mutable -> void
        {
            p = persistentalloc1(size, align, sysStat);
        });
        return unsafe::Pointer(p);
    }

    // Must run on system stack because stack growth can (re)invoke it.
    // See issue 9174.
    //
    //go:systemstack
    struct notInHeap* persistentalloc1(uintptr_t size, uintptr_t align, golang::runtime::sysMemStat* sysStat)
    {
        auto maxBlock = 64 << 10;
        if(size == 0)
        {
            go_throw("persistentalloc: size == 0"_s);
        }
        if(align != 0)
        {
            if(align & (align - 1) != 0)
            {
                go_throw("persistentalloc: align is not a power of 2"_s);
            }
            if(align > _PageSize)
            {
                go_throw("persistentalloc: align is too large"_s);
            }
        }
        else
        {
            align = 8;
        }
        if(size >= maxBlock)
        {
            return (notInHeap*)(sysAlloc(size, sysStat));
        }
        auto mp = acquirem();
        persistentAlloc* persistent = {};
        if(mp != nullptr && mp->p != 0)
        {
            persistent = & rec::ptr(gocpp::recv(mp->p))->palloc;
        }
        else
        {
            lock(& globalAlloc.mutex);
            persistent = & globalAlloc.persistentAlloc;
        }
        persistent->off = alignUp(persistent->off, align);
        if(persistent->off + size > persistentChunkSize || persistent->base == nullptr)
        {
            persistent->base = (notInHeap*)(sysAlloc(persistentChunkSize, & memstats.other_sys));
            if(persistent->base == nullptr)
            {
                if(persistent == & globalAlloc.persistentAlloc)
                {
                    unlock(& globalAlloc.mutex);
                }
                go_throw("runtime: cannot allocate memory"_s);
            }
            for(; ; )
            {
                auto chunks = uintptr_t(unsafe::Pointer(persistentChunks));
                *(uintptr_t*)(unsafe::Pointer(persistent->base)) = chunks;
                if(atomic::Casuintptr((uintptr_t*)(unsafe::Pointer(& persistentChunks)), chunks, uintptr_t(unsafe::Pointer(persistent->base))))
                {
                    break;
                }
            }
            persistent->off = alignUp(goarch::PtrSize, align);
        }
        auto p = rec::add(gocpp::recv(persistent->base), persistent->off);
        persistent->off += size;
        releasem(mp);
        if(persistent == & globalAlloc.persistentAlloc)
        {
            unlock(& globalAlloc.mutex);
        }
        if(sysStat != & memstats.other_sys)
        {
            rec::add(gocpp::recv(sysStat), int64_t(size));
            rec::add(gocpp::recv(memstats.other_sys), - int64_t(size));
        }
        return p;
    }

    // inPersistentAlloc reports whether p points to memory allocated by
    // persistentalloc. This must be nosplit because it is called by the
    // cgo checker code, which is called by the write barrier code.
    //
    //go:nosplit
    bool inPersistentAlloc(uintptr_t p)
    {
        auto chunk = atomic::Loaduintptr((uintptr_t*)(unsafe::Pointer(& persistentChunks)));
        for(; chunk != 0; )
        {
            if(p >= chunk && p < chunk + persistentChunkSize)
            {
                return true;
            }
            chunk = *(uintptr_t*)(unsafe::Pointer(chunk));
        }
        return false;
    }

    // linearAlloc is a simple linear allocator that pre-reserves a region
    // of memory and then optionally maps that region into the Ready state
    // as needed.
    //
    // The caller is responsible for locking.
    
    template<typename T> requires gocpp::GoStruct<T>
    linearAlloc::operator T()
    {
        T result;
        result.next = this->next;
        result.mapped = this->mapped;
        result.end = this->end;
        result.mapMemory = this->mapMemory;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool linearAlloc::operator==(const T& ref) const
    {
        if (next != ref.next) return false;
        if (mapped != ref.mapped) return false;
        if (end != ref.end) return false;
        if (mapMemory != ref.mapMemory) return false;
        return true;
    }

    std::ostream& linearAlloc::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << next;
        os << " " << mapped;
        os << " " << end;
        os << " " << mapMemory;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct linearAlloc& value)
    {
        return value.PrintTo(os);
    }

    void rec::init(struct linearAlloc* l, uintptr_t base, uintptr_t size, bool mapMemory)
    {
        if(base + size < base)
        {
            size -= 1;
        }
        std::tie(l->next, l->mapped) = std::tuple{base, base};
        l->end = base + size;
        l->mapMemory = mapMemory;
    }

    unsafe::Pointer rec::alloc(struct linearAlloc* l, uintptr_t size, uintptr_t align, golang::runtime::sysMemStat* sysStat)
    {
        auto p = alignUp(l->next, align);
        if(p + size > l->end)
        {
            return nullptr;
        }
        l->next = p + size;
        if(auto pEnd = alignUp(l->next - 1, physPageSize); pEnd > l->mapped)
        {
            if(l->mapMemory)
            {
                auto n = pEnd - l->mapped;
                sysMap(unsafe::Pointer(l->mapped), n, sysStat);
                sysUsed(unsafe::Pointer(l->mapped), n, n);
            }
            l->mapped = pEnd;
        }
        return unsafe::Pointer(p);
    }

    // notInHeap is off-heap memory allocated by a lower-level allocator
    // like sysAlloc or persistentAlloc.
    //
    // In general, it's better to use real types which embed
    // runtime/internal/sys.NotInHeap, but this serves as a generic type
    // for situations where that isn't possible (like in the allocators).
    //
    // TODO: Use this as the return type of sysAlloc, persistentAlloc, etc?
    
    template<typename T> requires gocpp::GoStruct<T>
    notInHeap::operator T()
    {
        T result;
        result._1 = this->_1;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool notInHeap::operator==(const T& ref) const
    {
        if (_1 != ref._1) return false;
        return true;
    }

    std::ostream& notInHeap::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << _1;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct notInHeap& value)
    {
        return value.PrintTo(os);
    }

    struct notInHeap* rec::add(struct notInHeap* p, uintptr_t bytes)
    {
        return (notInHeap*)(unsafe::Pointer(uintptr_t(unsafe::Pointer(p)) + bytes));
    }

    // computeRZlog computes the size of the redzone.
    // Refer to the implementation of the compiler-rt.
    uintptr_t computeRZlog(uintptr_t userSize)
    {
        //Go switch emulation
        {
            int conditionId = -1;
            if(userSize <= (64 - 16)) { conditionId = 0; }
            else if(userSize <= (128 - 32)) { conditionId = 1; }
            else if(userSize <= (512 - 64)) { conditionId = 2; }
            else if(userSize <= (4096 - 128)) { conditionId = 3; }
            else if(userSize <= (1 << 14) - 256) { conditionId = 4; }
            else if(userSize <= (1 << 15) - 512) { conditionId = 5; }
            else if(userSize <= (1 << 16) - 1024) { conditionId = 6; }
            switch(conditionId)
            {
                case 0:
                    return 16 << 0;
                    break;
                case 1:
                    return 16 << 1;
                    break;
                case 2:
                    return 16 << 2;
                    break;
                case 3:
                    return 16 << 3;
                    break;
                case 4:
                    return 16 << 4;
                    break;
                case 5:
                    return 16 << 5;
                    break;
                case 6:
                    return 16 << 6;
                    break;
                default:
                    return 16 << 7;
                    break;
            }
        }
    }

}

