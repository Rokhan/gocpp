// generated by GoCpp from file '$(ImportDir)/runtime/proc.go'
#include <complex>
#include <functional>
#include <iostream>
#include <iomanip>
#include <map>
#include <string>
#include <tuple>
#include <vector>

#include "golang/runtime/proc.h"
#include "gocpp/support.h"

#include "golang/internal/abi/funcpc.h"
#include "golang/internal/abi/symtab.h"
#include "golang/internal/abi/type.h"
#include "golang/internal/chacha8rand/chacha8.h"
#include "golang/internal/cpu/cpu.h"
#include "golang/internal/goarch/goarch.h"
#include "golang/internal/goexperiment/exp_exectracer2_on.h"
#include "golang/internal/goos/zgoos_windows.h"
#include "golang/runtime/alg.h"
#include "golang/runtime/asan0.h"
#include "golang/runtime/atomic_pointer.h"
#include "golang/runtime/cgocall.h"
#include "golang/runtime/chan.h"
#include "golang/runtime/coro.h"
#include "golang/runtime/cpuprof.h"
#include "golang/runtime/debuglog_off.h"
#include "golang/runtime/env_posix.h"
#include "golang/runtime/error.h"
#include "golang/runtime/exithook.h"
#include "golang/runtime/extern.h"
#include "golang/runtime/fds_nonunix.h"
#include "golang/runtime/histogram.h"
#include "golang/runtime/iface.h"
#include "golang/runtime/internal/atomic/atomic_amd64.h"
#include "golang/runtime/internal/atomic/stubs.h"
#include "golang/runtime/internal/atomic/types.h"
#include "golang/runtime/internal/sys/consts.h"
#include "golang/runtime/internal/sys/nih.h"
#include "golang/runtime/lfstack.h"
#include "golang/runtime/lock_sema.h"
#include "golang/runtime/lockrank.h"
#include "golang/runtime/lockrank_off.h"
#include "golang/runtime/malloc.h"
#include "golang/runtime/mbitmap_allocheaders.h"
#include "golang/runtime/mcache.h"
#include "golang/runtime/mcentral.h"
#include "golang/runtime/mcheckmark.h"
#include "golang/runtime/mfinal.h"
#include "golang/runtime/mfixalloc.h"
#include "golang/runtime/mgc.h"
#include "golang/runtime/mgclimit.h"
#include "golang/runtime/mgcpacer.h"
#include "golang/runtime/mgcscavenge.h"
#include "golang/runtime/mgcwork.h"
#include "golang/runtime/mheap.h"
#include "golang/runtime/mpagealloc.h"
#include "golang/runtime/mpagecache.h"
#include "golang/runtime/mpallocbits.h"
#include "golang/runtime/mprof.h"
#include "golang/runtime/mranges.h"
#include "golang/runtime/msan0.h"
#include "golang/runtime/mspanset.h"
#include "golang/runtime/mstats.h"
#include "golang/runtime/mwbbuf.h"
#include "golang/runtime/netpoll.h"
#include "golang/runtime/netpoll_windows.h"
#include "golang/runtime/os_windows.h"
#include "golang/runtime/pagetrace_off.h"
#include "golang/runtime/panic.h"
#include "golang/runtime/pinner.h"
#include "golang/runtime/plugin.h"
#include "golang/runtime/preempt.h"
#include "golang/runtime/print.h"
#include "golang/runtime/profbuf.h"
#include "golang/runtime/race0.h"
#include "golang/runtime/rand.h"
#include "golang/runtime/runtime.h"
#include "golang/runtime/runtime1.h"
#include "golang/runtime/runtime2.h"
#include "golang/runtime/rwmutex.h"
#include "golang/runtime/security_nonunix.h"
#include "golang/runtime/sema.h"
#include "golang/runtime/signal_windows.h"
#include "golang/runtime/stack.h"
#include "golang/runtime/stkframe.h"
#include "golang/runtime/string.h"
#include "golang/runtime/stubs.h"
#include "golang/runtime/symtab.h"
#include "golang/runtime/sys_nonppc64x.h"
#include "golang/runtime/time.h"
#include "golang/runtime/time_nofake.h"
#include "golang/runtime/tls_windows_amd64.h"
#include "golang/runtime/trace2.h"
#include "golang/runtime/trace2buf.h"
#include "golang/runtime/trace2cpu.h"
#include "golang/runtime/trace2runtime.h"
#include "golang/runtime/trace2status.h"
#include "golang/runtime/trace2time.h"
#include "golang/runtime/traceback.h"
#include "golang/runtime/type.h"
#include "golang/runtime/vdso_in_none.h"
#include "golang/unsafe/unsafe.h"

namespace golang::runtime
{
    namespace rec
    {
        using namespace mocklib::rec;
        using atomic::rec::Add;
        using atomic::rec::CompareAndSwap;
        using atomic::rec::CompareAndSwapNoWB;
        using atomic::rec::Load;
        using atomic::rec::Store;
        using atomic::rec::Swap;
    }

    // set using cmd/go/internal/modload.ModInfoProg
    std::string modinfo;
    m m0;
    g g0;
    mcache* mcache0;
    uintptr_t raceprocctx0;
    mutex raceFiniLock;
    // This slice records the initializing tasks that need to be
    // done to start up the runtime. It is built by the linker.
    gocpp::slice<initTask*> runtime_inittasks;
    // main_init_done is a signal used by cgocallbackg that initialization
    // has been completed. It is made before _cgo_notify_runtime_init_done,
    // so all cgo calls can rely on it existing. When main_init is complete,
    // it is closed, meaning cgocallbackg can reliably receive from it.
    gocpp::channel<bool> main_init_done;
    //go:linkname main_main main.main
    void main_main()
    /* convertBlockStmt, nil block */;

    // mainStarted indicates that the main M has started.
    bool mainStarted;
    // runtimeInitTime is the nanotime() at which the runtime started.
    int64_t runtimeInitTime;
    // Value to use for signal mask for newly created M's.
    sigset initSigmask;
    // The main goroutine.
    void main()
    {
        gocpp::Defer defer;
        try
        {
            auto mp = getg()->m;
            mp->g0->racectx = 0;
            if(goarch::PtrSize == 8)
            {
                maxstacksize = 1000000000;
            }
            else
            {
                maxstacksize = 250000000;
            }
            maxstackceiling = 2 * maxstacksize;
            mainStarted = true;
            if(GOARCH != "wasm"s)
            {
                systemstack([=]() mutable -> void
                {
                    newm(sysmon, nullptr, - 1);
                });
            }
            lockOSThread();
            if(mp != & m0)
            {
                go_throw("runtime.main not on m0"s);
            }
            runtimeInitTime = nanotime();
            if(runtimeInitTime == 0)
            {
                go_throw("nanotime returning zero"s);
            }
            if(debug.inittrace != 0)
            {
                inittrace.id = getg()->goid;
                inittrace.active = true;
            }
            doInit(runtime_inittasks);
            auto needUnlock = true;
            defer.push_back([=]{ [=]() mutable -> void
            {
                if(needUnlock)
                {
                    unlockOSThread();
                }
            }(); });
            gcenable();
            main_init_done = gocpp::make(gocpp::Tag<gocpp::channel<bool>>());
            if(iscgo)
            {
                if(_cgo_pthread_key_created == nullptr)
                {
                    go_throw("_cgo_pthread_key_created missing"s);
                }
                if(_cgo_thread_start == nullptr)
                {
                    go_throw("_cgo_thread_start missing"s);
                }
                if(GOOS != "windows"s)
                {
                    if(_cgo_setenv == nullptr)
                    {
                        go_throw("_cgo_setenv missing"s);
                    }
                    if(_cgo_unsetenv == nullptr)
                    {
                        go_throw("_cgo_unsetenv missing"s);
                    }
                }
                if(_cgo_notify_runtime_init_done == nullptr)
                {
                    go_throw("_cgo_notify_runtime_init_done missing"s);
                }
                if(set_crosscall2 == nullptr)
                {
                    go_throw("set_crosscall2 missing"s);
                }
                set_crosscall2();
                startTemplateThread();
                cgocall(_cgo_notify_runtime_init_done, nullptr);
            }
            for(auto m = & firstmoduledata; m != nullptr; m = m->next)
            {
                doInit(m->inittasks);
            }
            inittrace.active = false;
            close(main_init_done);
            needUnlock = false;
            unlockOSThread();
            if(isarchive || islibrary)
            {
                return;
            }
            auto fn = main_main;
            fn();
            if(raceenabled)
            {
                runExitHooks(0);
                racefini();
            }
            if(rec::Load(gocpp::recv(runningPanicDefers)) != 0)
            {
                for(auto c = 0; c < 1000; c++)
                {
                    if(rec::Load(gocpp::recv(runningPanicDefers)) == 0)
                    {
                        break;
                    }
                    Gosched();
                }
            }
            if(rec::Load(gocpp::recv(panicking)) != 0)
            {
                gopark(nullptr, nullptr, waitReasonPanicWait, traceBlockForever, 1);
            }
            runExitHooks(0);
            exit(0);
            for(; ; )
            {
                int32_t* x = {};
                *x = 0;
            }
        }
        catch(gocpp::GoPanic& gp)
        {
            defer.handlePanic(gp);
        }
    }

    // os_beforeExit is called from os.Exit(0).
    //
    //go:linkname os_beforeExit os.runtime_beforeExit
    void os_beforeExit(int exitCode)
    {
        runExitHooks(exitCode);
        if(exitCode == 0 && raceenabled)
        {
            racefini();
        }
    }

    // start forcegc helper goroutine
    void init()
    {
        gocpp::go([&]{ forcegchelper(); });
    }

    void forcegchelper()
    {
        forcegc.g = getg();
        lockInit(& forcegc.lock, lockRankForcegc);
        for(; ; )
        {
            lock(& forcegc.lock);
            if(rec::Load(gocpp::recv(forcegc.idle)))
            {
                go_throw("forcegc: phase error"s);
            }
            rec::Store(gocpp::recv(forcegc.idle), true);
            goparkunlock(& forcegc.lock, waitReasonForceGCIdle, traceBlockSystemGoroutine, 1);
            if(debug.gctrace > 0)
            {
                println("GC forced"s);
            }
            gcStart(gocpp::Init<gcTrigger>([=](auto& x) {
                x.kind = gcTriggerTime;
                x.now = nanotime();
            }));
        }
    }

    // Gosched yields the processor, allowing other goroutines to run. It does not
    // suspend the current goroutine, so execution resumes automatically.
    //
    //go:nosplit
    void Gosched()
    {
        checkTimeouts();
        mcall(gosched_m);
    }

    // goschedguarded yields the processor like gosched, but also checks
    // for forbidden states and opts out of the yield in those cases.
    //
    //go:nosplit
    void goschedguarded()
    {
        mcall(goschedguarded_m);
    }

    // goschedIfBusy yields the processor like gosched, but only does so if
    // there are no idle Ps or if we're on the only P and there's nothing in
    // the run queue. In both cases, there is freely available idle time.
    //
    //go:nosplit
    void goschedIfBusy()
    {
        auto gp = getg();
        if(! gp->preempt && rec::Load(gocpp::recv(sched.npidle)) > 0)
        {
            return;
        }
        mcall(gosched_m);
    }

    // Puts the current goroutine into a waiting state and calls unlockf on the
    // system stack.
    //
    // If unlockf returns false, the goroutine is resumed.
    //
    // unlockf must not access this G's stack, as it may be moved between
    // the call to gopark and the call to unlockf.
    //
    // Note that because unlockf is called after putting the G into a waiting
    // state, the G may have already been readied by the time unlockf is called
    // unless there is external synchronization preventing the G from being
    // readied. If unlockf returns false, it must guarantee that the G cannot be
    // externally readied.
    //
    // Reason explains why the goroutine has been parked. It is displayed in stack
    // traces and heap dumps. Reasons should be unique and descriptive. Do not
    // re-use reasons, add new ones.
    void gopark(std::function<bool (g*, unsafe::Pointer)> unlockf, unsafe::Pointer lock, golang::runtime::waitReason reason, golang::runtime::traceBlockReason traceReason, int traceskip)
    {
        if(reason != waitReasonSleep)
        {
            checkTimeouts();
        }
        auto mp = acquirem();
        auto gp = mp->curg;
        auto status = readgstatus(gp);
        if(status != _Grunning && status != _Gscanrunning)
        {
            go_throw("gopark: bad g status"s);
        }
        mp->waitlock = lock;
        mp->waitunlockf = unlockf;
        gp->waitreason = reason;
        mp->waitTraceBlockReason = traceReason;
        mp->waitTraceSkip = traceskip;
        releasem(mp);
        mcall(park_m);
    }

    // Puts the current goroutine into a waiting state and unlocks the lock.
    // The goroutine can be made runnable again by calling goready(gp).
    void goparkunlock(struct mutex* lock, golang::runtime::waitReason reason, golang::runtime::traceBlockReason traceReason, int traceskip)
    {
        gopark(parkunlock_c, unsafe::Pointer(lock), reason, traceReason, traceskip);
    }

    void goready(struct g* gp, int traceskip)
    {
        systemstack([=]() mutable -> void
        {
            ready(gp, traceskip, true);
        });
    }

    //go:nosplit
    struct sudog* acquireSudog()
    {
        auto mp = acquirem();
        auto pp = rec::ptr(gocpp::recv(mp->p));
        if(len(pp->sudogcache) == 0)
        {
            lock(& sched.sudoglock);
            for(; len(pp->sudogcache) < cap(pp->sudogcache) / 2 && sched.sudogcache != nullptr; )
            {
                auto s = sched.sudogcache;
                sched.sudogcache = s->next;
                s->next = nullptr;
                pp->sudogcache = append(pp->sudogcache, s);
            }
            unlock(& sched.sudoglock);
            if(len(pp->sudogcache) == 0)
            {
                pp->sudogcache = append(pp->sudogcache, new(sudog));
            }
        }
        auto n = len(pp->sudogcache);
        auto s = pp->sudogcache[n - 1];
        pp->sudogcache[n - 1] = nullptr;
        pp->sudogcache = pp->sudogcache.make_slice(0, n - 1);
        if(s->elem != nullptr)
        {
            go_throw("acquireSudog: found s.elem != nil in cache"s);
        }
        releasem(mp);
        return s;
    }

    //go:nosplit
    void releaseSudog(struct sudog* s)
    {
        if(s->elem != nullptr)
        {
            go_throw("runtime: sudog with non-nil elem"s);
        }
        if(s->isSelect)
        {
            go_throw("runtime: sudog with non-false isSelect"s);
        }
        if(s->next != nullptr)
        {
            go_throw("runtime: sudog with non-nil next"s);
        }
        if(s->prev != nullptr)
        {
            go_throw("runtime: sudog with non-nil prev"s);
        }
        if(s->waitlink != nullptr)
        {
            go_throw("runtime: sudog with non-nil waitlink"s);
        }
        if(s->c != nullptr)
        {
            go_throw("runtime: sudog with non-nil c"s);
        }
        auto gp = getg();
        if(gp->param != nullptr)
        {
            go_throw("runtime: releaseSudog with non-nil gp.param"s);
        }
        auto mp = acquirem();
        auto pp = rec::ptr(gocpp::recv(mp->p));
        if(len(pp->sudogcache) == cap(pp->sudogcache))
        {
            // Transfer half of local cache to the central cache.
            sudog* first = {};
            sudog* last = {};
            for(; len(pp->sudogcache) > cap(pp->sudogcache) / 2; )
            {
                auto n = len(pp->sudogcache);
                auto p = pp->sudogcache[n - 1];
                pp->sudogcache[n - 1] = nullptr;
                pp->sudogcache = pp->sudogcache.make_slice(0, n - 1);
                if(first == nullptr)
                {
                    first = p;
                }
                else
                {
                    last->next = p;
                }
                last = p;
            }
            lock(& sched.sudoglock);
            last->next = sched.sudogcache;
            sched.sudogcache = first;
            unlock(& sched.sudoglock);
        }
        pp->sudogcache = append(pp->sudogcache, s);
        releasem(mp);
    }

    // called from assembly.
    void badmcall(std::function<void (g*)> fn)
    {
        go_throw("runtime: mcall called on m->g0 stack"s);
    }

    void badmcall2(std::function<void (g*)> fn)
    {
        go_throw("runtime: mcall function returned"s);
    }

    void badreflectcall()
    {
        gocpp::panic(plainError("arg size to reflect.call more than 1GB"s));
    }

    //go:nosplit
    //go:nowritebarrierrec
    void badmorestackg0()
    {
        if(! crashStackImplemented)
        {
            writeErrStr("fatal: morestack on g0\n"s);
            return;
        }
        auto g = getg();
        switchToCrashStack([=]() mutable -> void
        {
            print("runtime: morestack on g0, stack ["s, hex(g->stack.lo), " "s, hex(g->stack.hi), "], sp="s, hex(g->sched.sp), ", called from\n"s);
            g->m->traceback = 2;
            traceback1(g->sched.pc, g->sched.sp, g->sched.lr, g, 0);
            print("\n"s);
            go_throw("morestack on g0"s);
        });
    }

    //go:nosplit
    //go:nowritebarrierrec
    void badmorestackgsignal()
    {
        writeErrStr("fatal: morestack on gsignal\n"s);
    }

    //go:nosplit
    void badctxt()
    {
        go_throw("ctxt != 0"s);
    }

    // gcrash is a fake g that can be used when crashing due to bad
    // stack conditions.
    g gcrash;
    atomic::Pointer<g> crashingG;
    // Switch to crashstack and call fn, with special handling of
    // concurrent and recursive cases.
    //
    // Nosplit as it is called in a bad stack condition (we know
    // morestack would fail).
    //
    //go:nosplit
    //go:nowritebarrierrec
    void switchToCrashStack(std::function<void ()> fn)
    {
        auto me = getg();
        if(rec::CompareAndSwapNoWB(gocpp::recv(crashingG), nullptr, me))
        {
            switchToCrashStack0(fn);
            abort();
        }
        if(rec::Load(gocpp::recv(crashingG)) == me)
        {
            writeErrStr("fatal: recursive switchToCrashStack\n"s);
            abort();
        }
        usleep_no_g(100);
        writeErrStr("fatal: concurrent switchToCrashStack\n"s);
        abort();
    }

    // Disable crash stack on Windows for now. Apparently, throwing an exception
    // on a non-system-allocated crash stack causes EXCEPTION_STACK_OVERFLOW and
    // hangs the process (see issue 63938).
    bool crashStackImplemented = (GOARCH == "amd64"s || GOARCH == "arm64"s || GOARCH == "mips64"s || GOARCH == "mips64le"s || GOARCH == "ppc64"s || GOARCH == "ppc64le"s || GOARCH == "riscv64"s || GOARCH == "wasm"s) && GOOS != "windows"s;
    //go:noescape
    void switchToCrashStack0(std::function<void ()> fn)
    /* convertBlockStmt, nil block */;

    bool lockedOSThread()
    {
        auto gp = getg();
        return gp->lockedm != 0 && gp->m->lockedg != 0;
    }

    // allgs contains all Gs ever created (including dead Gs), and thus
    // never shrinks.
    //
    // Access via the slice is protected by allglock or stop-the-world.
    // Readers that cannot take the lock may (carefully!) use the atomic
    // variables below.
    // allglen and allgptr are atomic variables that contain len(allgs) and
    // &allgs[0] respectively. Proper ordering depends on totally-ordered
    // loads and stores. Writes are protected by allglock.
    //
    // allgptr is updated before allglen. Readers should read allglen
    // before allgptr to ensure that allglen is always <= len(allgptr). New
    // Gs appended during the race can be missed. For a consistent view of
    // all Gs, allglock must be held.
    //
    // allgptr copies should always be stored as a concrete type or
    // unsafe.Pointer, not uintptr, to ensure that GC can still reach it
    // even if it points to a stale array.
    mutex allglock;
    gocpp::slice<g*> allgs;
    uintptr_t allglen;
    g** allgptr;
    void allgadd(struct g* gp)
    {
        if(readgstatus(gp) == _Gidle)
        {
            go_throw("allgadd: bad status Gidle"s);
        }
        lock(& allglock);
        allgs = append(allgs, gp);
        if(& allgs[0] != allgptr)
        {
            atomicstorep(unsafe::Pointer(& allgptr), unsafe::Pointer(& allgs[0]));
        }
        atomic::Storeuintptr(& allglen, uintptr_t(len(allgs)));
        unlock(& allglock);
    }

    // allGsSnapshot returns a snapshot of the slice of all Gs.
    //
    // The world must be stopped or allglock must be held.
    gocpp::slice<g*> allGsSnapshot()
    {
        assertWorldStoppedOrLockHeld(& allglock);
        return allgs.make_slice(, len(allgs), len(allgs));
    }

    // atomicAllG returns &allgs[0] and len(allgs) for use with atomicAllGIndex.
    std::tuple<struct g**, uintptr_t> atomicAllG()
    {
        auto length = atomic::Loaduintptr(& allglen);
        auto ptr = (g**)(atomic::Loadp(unsafe::Pointer(& allgptr)));
        return {ptr, length};
    }

    // atomicAllGIndex returns ptr[i] with the allgptr returned from atomicAllG.
    struct g* atomicAllGIndex(struct g** ptr, uintptr_t i)
    {
        return *(g**)(add(unsafe::Pointer(ptr), i * goarch::PtrSize));
    }

    // forEachG calls fn on every G from allgs.
    //
    // forEachG takes a lock to exclude concurrent addition of new Gs.
    void forEachG(std::function<void (struct g* gp)> fn)
    {
        lock(& allglock);
        for(auto [gocpp_ignored, gp] : allgs)
        {
            fn(gp);
        }
        unlock(& allglock);
    }

    // forEachGRace calls fn on every G from allgs.
    //
    // forEachGRace avoids locking, but does not exclude addition of new Gs during
    // execution, which may be missed.
    void forEachGRace(std::function<void (struct g* gp)> fn)
    {
        auto [ptr, length] = atomicAllG();
        for(auto i = uintptr_t(0); i < length; i++)
        {
            auto gp = atomicAllGIndex(ptr, i);
            fn(gp);
        }
        return;
    }

    // Number of goroutine ids to grab from sched.goidgen to local per-P cache at once.
    // 16 seems to provide enough amortization, but other than that it's mostly arbitrary number.
    // cpuinit sets up CPU feature flags and calls internal/cpu.Initialize. env should be the complete
    // value of the GODEBUG environment variable.
    void cpuinit(std::string env)
    {
        //Go switch emulation
        {
            auto condition = GOOS;
            int conditionId = -1;
            if(condition == "aix"s) { conditionId = 0; }
            else if(condition == "darwin"s) { conditionId = 1; }
            else if(condition == "ios"s) { conditionId = 2; }
            else if(condition == "dragonfly"s) { conditionId = 3; }
            else if(condition == "freebsd"s) { conditionId = 4; }
            else if(condition == "netbsd"s) { conditionId = 5; }
            else if(condition == "openbsd"s) { conditionId = 6; }
            else if(condition == "illumos"s) { conditionId = 7; }
            else if(condition == "solaris"s) { conditionId = 8; }
            else if(condition == "linux"s) { conditionId = 9; }
            switch(conditionId)
            {
                case 0:
                case 1:
                case 2:
                case 3:
                case 4:
                case 5:
                case 6:
                case 7:
                case 8:
                case 9:
                    cpu::DebugOptions = true;
                    break;
            }
        }
        cpu::Initialize(env);
        //Go switch emulation
        {
            auto condition = GOARCH;
            int conditionId = -1;
            if(condition == "386"s) { conditionId = 0; }
            else if(condition == "amd64"s) { conditionId = 1; }
            else if(condition == "arm"s) { conditionId = 2; }
            else if(condition == "arm64"s) { conditionId = 3; }
            switch(conditionId)
            {
                case 0:
                case 1:
                    x86HasPOPCNT = cpu::X86.HasPOPCNT;
                    x86HasSSE41 = cpu::X86.HasSSE41;
                    x86HasFMA = cpu::X86.HasFMA;
                    break;
                case 2:
                    armHasVFPv4 = cpu::ARM.HasVFPv4;
                    break;
                case 3:
                    arm64HasATOMICS = cpu::ARM64.HasATOMICS;
                    break;
            }
        }
    }

    // getGodebugEarly extracts the environment variable GODEBUG from the environment on
    // Unix-like operating systems and returns it. This function exists to extract GODEBUG
    // early before much of the runtime is initialized.
    std::string getGodebugEarly()
    {
        auto prefix = "GODEBUG="s;
        std::string env = {};
        //Go switch emulation
        {
            auto condition = GOOS;
            int conditionId = -1;
            if(condition == "aix"s) { conditionId = 0; }
            else if(condition == "darwin"s) { conditionId = 1; }
            else if(condition == "ios"s) { conditionId = 2; }
            else if(condition == "dragonfly"s) { conditionId = 3; }
            else if(condition == "freebsd"s) { conditionId = 4; }
            else if(condition == "netbsd"s) { conditionId = 5; }
            else if(condition == "openbsd"s) { conditionId = 6; }
            else if(condition == "illumos"s) { conditionId = 7; }
            else if(condition == "solaris"s) { conditionId = 8; }
            else if(condition == "linux"s) { conditionId = 9; }
            switch(conditionId)
            {
                case 0:
                case 1:
                case 2:
                case 3:
                case 4:
                case 5:
                case 6:
                case 7:
                case 8:
                case 9:
                    auto n = int32_t(0);
                    for(; argv_index(argv, argc + 1 + n) != nullptr; )
                    {
                        n++;
                    }
                    for(auto i = int32_t(0); i < n; i++)
                    {
                        auto p = argv_index(argv, argc + 1 + i);
                        auto s = unsafe::String(p, findnull(p));
                        if(hasPrefix(s, prefix))
                        {
                            env = gostring(p).make_slice(len(prefix));
                            break;
                        }
                    }
                    break;
            }
        }
        return env;
    }

    // The bootstrap sequence is:
    //
    //	call osinit
    //	call schedinit
    //	make & queue new G
    //	call runtime·mstart
    //
    // The new G calls runtime·main.
    void schedinit()
    {
        lockInit(& sched.lock, lockRankSched);
        lockInit(& sched.sysmonlock, lockRankSysmon);
        lockInit(& sched.deferlock, lockRankDefer);
        lockInit(& sched.sudoglock, lockRankSudog);
        lockInit(& deadlock, lockRankDeadlock);
        lockInit(& paniclk, lockRankPanic);
        lockInit(& allglock, lockRankAllg);
        lockInit(& allpLock, lockRankAllp);
        lockInit(& reflectOffs.lock, lockRankReflectOffs);
        lockInit(& finlock, lockRankFin);
        lockInit(& cpuprof.lock, lockRankCpuprof);
        rec::init(gocpp::recv(allocmLock), lockRankAllocmR, lockRankAllocmRInternal, lockRankAllocmW);
        rec::init(gocpp::recv(execLock), lockRankExecR, lockRankExecRInternal, lockRankExecW);
        traceLockInit();
        lockInit(& memstats.heapStats.noPLock, lockRankLeafRank);
        auto gp = getg();
        if(raceenabled)
        {
            std::tie(gp->racectx, raceprocctx0) = raceinit();
        }
        sched.maxmcount = 10000;
        worldStopped();
        rec::init(gocpp::recv(ticks));
        moduledataverify();
        stackinit();
        mallocinit();
        auto godebug = getGodebugEarly();
        initPageTrace(godebug);
        cpuinit(godebug);
        randinit();
        alginit();
        mcommoninit(gp->m, - 1);
        modulesinit();
        typelinksinit();
        itabsinit();
        stkobjinit();
        sigsave(& gp->m->sigmask);
        initSigmask = gp->m->sigmask;
        goargs();
        goenvs();
        secure();
        checkfds();
        parsedebugvars();
        gcinit();
        gcrash.stack = stackalloc(16384);
        gcrash.stackguard0 = gcrash.stack.lo + 1000;
        gcrash.stackguard1 = gcrash.stack.lo + 1000;
        if(disableMemoryProfiling)
        {
            MemProfileRate = 0;
        }
        lock(& sched.lock);
        rec::Store(gocpp::recv(sched.lastpoll), nanotime());
        auto procs = ncpu;
        if(auto [n, ok] = atoi32(gogetenv("GOMAXPROCS"s)); ok && n > 0)
        {
            procs = n;
        }
        if(procresize(procs) != nullptr)
        {
            go_throw("unknown runnable goroutine during bootstrap"s);
        }
        unlock(& sched.lock);
        worldStarted();
        if(buildVersion == ""s)
        {
            buildVersion = "unknown"s;
        }
        if(len(modinfo) == 1)
        {
            modinfo = ""s;
        }
    }

    void dumpgstatus(struct g* gp)
    {
        auto thisg = getg();
        print("runtime:   gp: gp="s, gp, ", goid="s, gp->goid, ", gp->atomicstatus="s, readgstatus(gp), "\n"s);
        print("runtime: getg:  g="s, thisg, ", goid="s, thisg->goid, ",  g->atomicstatus="s, readgstatus(thisg), "\n"s);
    }

    // sched.lock must be held.
    void checkmcount()
    {
        assertLockHeld(& sched.lock);
        auto count = mcount() - int32_t(rec::Load(gocpp::recv(extraMInUse))) - int32_t(rec::Load(gocpp::recv(extraMLength)));
        if(count > sched.maxmcount)
        {
            print("runtime: program exceeds "s, sched.maxmcount, "-thread limit\n"s);
            go_throw("thread exhaustion"s);
        }
    }

    // mReserveID returns the next ID to use for a new m. This new m is immediately
    // considered 'running' by checkdead.
    //
    // sched.lock must be held.
    int64_t mReserveID()
    {
        assertLockHeld(& sched.lock);
        if(sched.mnext + 1 < sched.mnext)
        {
            go_throw("runtime: thread ID overflow"s);
        }
        auto id = sched.mnext;
        sched.mnext++;
        checkmcount();
        return id;
    }

    // Pre-allocated ID may be passed as 'id', or omitted by passing -1.
    void mcommoninit(struct m* mp, int64_t id)
    {
        auto gp = getg();
        if(gp != gp->m->g0)
        {
            callers(1, mp->createstack.make_slice(0));
        }
        lock(& sched.lock);
        if(id >= 0)
        {
            mp->id = id;
        }
        else
        {
            mp->id = mReserveID();
        }
        mrandinit(mp);
        mpreinit(mp);
        if(mp->gsignal != nullptr)
        {
            mp->gsignal->stackguard1 = mp->gsignal->stack.lo + stackGuard;
        }
        mp->alllink = allm;
        atomicstorep(unsafe::Pointer(& allm), unsafe::Pointer(mp));
        unlock(& sched.lock);
        if(iscgo || GOOS == "solaris"s || GOOS == "illumos"s || GOOS == "windows"s)
        {
            mp->cgoCallers = new(cgoCallers);
        }
    }

    void rec::becomeSpinning(struct m* mp)
    {
        mp->spinning = true;
        rec::Add(gocpp::recv(sched.nmspinning), 1);
        rec::Store(gocpp::recv(sched.needspinning), 0);
    }

    bool rec::hasCgoOnStack(struct m* mp)
    {
        return mp->ncgo > 0 || mp->isextra;
    }

    // osHasLowResTimer indicates that the platform's internal timer system has a low resolution,
    // typically on the order of 1 ms or more.
    // osHasLowResClockInt is osHasLowResClock but in integer form, so it can be used to create
    // constants conditionally.
    // osHasLowResClock indicates that timestamps produced by nanotime on the platform have a
    // low resolution, typically on the order of 1 ms or more.
    bool osHasLowResTimer = GOOS == "windows"s || GOOS == "openbsd"s || GOOS == "netbsd"s;
    // Mark gp ready to run.
    void ready(struct g* gp, int traceskip, bool next)
    {
        auto status = readgstatus(gp);
        auto mp = acquirem();
        if(status &^ _Gscan != _Gwaiting)
        {
            dumpgstatus(gp);
            go_throw("bad g->status in ready"s);
        }
        auto trace = traceAcquire();
        casgstatus(gp, _Gwaiting, _Grunnable);
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::GoUnpark(gocpp::recv(trace), gp, traceskip);
            traceRelease(trace);
        }
        runqput(rec::ptr(gocpp::recv(mp->p)), gp, next);
        wakep();
        releasem(mp);
    }

    // freezeStopWait is a large value that freezetheworld sets
    // sched.stopwait to in order to request that all Gs permanently stop.
    // freezing is set to non-zero if the runtime is trying to freeze the
    // world.
    atomic::Bool freezing;
    // Similar to stopTheWorld but best-effort and can be called several times.
    // There is no reverse operation, used during crashing.
    // This function must not lock any mutexes.
    void freezetheworld()
    {
        rec::Store(gocpp::recv(freezing), true);
        if(debug.dontfreezetheworld > 0)
        {
            usleep(1000);
            return;
        }
        for(auto i = 0; i < 5; i++)
        {
            sched.stopwait = freezeStopWait;
            rec::Store(gocpp::recv(sched.gcwaiting), true);
            if(! preemptall())
            {
                break;
            }
            usleep(1000);
        }
        usleep(1000);
        preemptall();
        usleep(1000);
    }

    // All reads and writes of g's status go through readgstatus, casgstatus
    // castogscanstatus, casfrom_Gscanstatus.
    //
    //go:nosplit
    uint32_t readgstatus(struct g* gp)
    {
        return rec::Load(gocpp::recv(gp->atomicstatus));
    }

    // The Gscanstatuses are acting like locks and this releases them.
    // If it proves to be a performance hit we should be able to make these
    // simple atomic stores but for now we are going to throw if
    // we see an inconsistent state.
    void casfrom_Gscanstatus(struct g* gp, uint32_t oldval, uint32_t newval)
    {
        auto success = false;
        //Go switch emulation
        {
            auto condition = oldval;
            int conditionId = -1;
            if(condition == _Gscanrunnable) { conditionId = 0; }
            else if(condition == _Gscanwaiting) { conditionId = 1; }
            else if(condition == _Gscanrunning) { conditionId = 2; }
            else if(condition == _Gscansyscall) { conditionId = 3; }
            else if(condition == _Gscanpreempted) { conditionId = 4; }
            switch(conditionId)
            {
                default:
                    print("runtime: casfrom_Gscanstatus bad oldval gp="s, gp, ", oldval="s, hex(oldval), ", newval="s, hex(newval), "\n"s);
                    dumpgstatus(gp);
                    go_throw("casfrom_Gscanstatus:top gp->status is not in scan state"s);
                    break;
                case 0:
                case 1:
                case 2:
                case 3:
                case 4:
                    if(newval == oldval &^ _Gscan)
                    {
                        success = rec::CompareAndSwap(gocpp::recv(gp->atomicstatus), oldval, newval);
                    }
                    break;
            }
        }
        if(! success)
        {
            print("runtime: casfrom_Gscanstatus failed gp="s, gp, ", oldval="s, hex(oldval), ", newval="s, hex(newval), "\n"s);
            dumpgstatus(gp);
            go_throw("casfrom_Gscanstatus: gp->status is not in scan state"s);
        }
        releaseLockRank(lockRankGscan);
    }

    // This will return false if the gp is not in the expected status and the cas fails.
    // This acts like a lock acquire while the casfromgstatus acts like a lock release.
    bool castogscanstatus(struct g* gp, uint32_t oldval, uint32_t newval)
    {
        //Go switch emulation
        {
            auto condition = oldval;
            int conditionId = -1;
            if(condition == _Grunnable) { conditionId = 0; }
            else if(condition == _Grunning) { conditionId = 1; }
            else if(condition == _Gwaiting) { conditionId = 2; }
            else if(condition == _Gsyscall) { conditionId = 3; }
            switch(conditionId)
            {
                case 0:
                case 1:
                case 2:
                case 3:
                    if(newval == oldval | _Gscan)
                    {
                        auto r = rec::CompareAndSwap(gocpp::recv(gp->atomicstatus), oldval, newval);
                        if(r)
                        {
                            acquireLockRank(lockRankGscan);
                        }
                        return r;
                    }
                    break;
            }
        }
        print("runtime: castogscanstatus oldval="s, hex(oldval), " newval="s, hex(newval), "\n"s);
        go_throw("castogscanstatus"s);
        gocpp::panic("not reached"s);
    }

    // casgstatusAlwaysTrack is a debug flag that causes casgstatus to always track
    // various latencies on every transition instead of sampling them.
    bool casgstatusAlwaysTrack = false;
    // If asked to move to or from a Gscanstatus this will throw. Use the castogscanstatus
    // and casfrom_Gscanstatus instead.
    // casgstatus will loop if the g->atomicstatus is in a Gscan status until the routine that
    // put it in the Gscan state is finished.
    //
    //go:nosplit
    void casgstatus(struct g* gp, uint32_t oldval, uint32_t newval)
    {
        if((oldval & _Gscan != 0) || (newval & _Gscan != 0) || oldval == newval)
        {
            systemstack([=]() mutable -> void
            {
                print("runtime: casgstatus: oldval="s, hex(oldval), " newval="s, hex(newval), "\n"s);
                go_throw("casgstatus: bad incoming values"s);
            });
        }
        acquireLockRank(lockRankGscan);
        releaseLockRank(lockRankGscan);
        // See https://golang.org/cl/21503 for justification of the yield delay.
        auto yieldDelay = 5 * 1000;
        int64_t nextYield = {};
        for(auto i = 0; ! rec::CompareAndSwap(gocpp::recv(gp->atomicstatus), oldval, newval); i++)
        {
            if(oldval == _Gwaiting && rec::Load(gocpp::recv(gp->atomicstatus)) == _Grunnable)
            {
                go_throw("casgstatus: waiting for Gwaiting but is Grunnable"s);
            }
            if(i == 0)
            {
                nextYield = nanotime() + yieldDelay;
            }
            if(nanotime() < nextYield)
            {
                for(auto x = 0; x < 10 && rec::Load(gocpp::recv(gp->atomicstatus)) != oldval; x++)
                {
                    procyield(1);
                }
            }
            else
            {
                osyield();
                nextYield = nanotime() + yieldDelay / 2;
            }
        }
        if(oldval == _Grunning)
        {
            if(casgstatusAlwaysTrack || gp->trackingSeq % gTrackingPeriod == 0)
            {
                gp->tracking = true;
            }
            gp->trackingSeq++;
        }
        if(! gp->tracking)
        {
            return;
        }
        //Go switch emulation
        {
            auto condition = oldval;
            int conditionId = -1;
            if(condition == _Grunnable) { conditionId = 0; }
            else if(condition == _Gwaiting) { conditionId = 1; }
            switch(conditionId)
            {
                case 0:
                    auto now = nanotime();
                    gp->runnableTime += now - gp->trackingStamp;
                    gp->trackingStamp = 0;
                    break;
                case 1:
                    if(! rec::isMutexWait(gocpp::recv(gp->waitreason)))
                    {
                        break;
                    }
                    auto now = nanotime();
                    rec::Add(gocpp::recv(sched.totalMutexWaitTime), (now - gp->trackingStamp) * gTrackingPeriod);
                    gp->trackingStamp = 0;
                    break;
            }
        }
        //Go switch emulation
        {
            auto condition = newval;
            int conditionId = -1;
            if(condition == _Gwaiting) { conditionId = 0; }
            else if(condition == _Grunnable) { conditionId = 1; }
            else if(condition == _Grunning) { conditionId = 2; }
            switch(conditionId)
            {
                case 0:
                    if(! rec::isMutexWait(gocpp::recv(gp->waitreason)))
                    {
                        break;
                    }
                    auto now = nanotime();
                    gp->trackingStamp = now;
                    break;
                case 1:
                    auto now = nanotime();
                    gp->trackingStamp = now;
                    break;
                case 2:
                    gp->tracking = false;
                    rec::record(gocpp::recv(sched.timeToRun), gp->runnableTime);
                    gp->runnableTime = 0;
                    break;
            }
        }
    }

    // casGToWaiting transitions gp from old to _Gwaiting, and sets the wait reason.
    //
    // Use this over casgstatus when possible to ensure that a waitreason is set.
    void casGToWaiting(struct g* gp, uint32_t old, golang::runtime::waitReason reason)
    {
        gp->waitreason = reason;
        casgstatus(gp, old, _Gwaiting);
    }

    // casgstatus(gp, oldstatus, Gcopystack), assuming oldstatus is Gwaiting or Grunnable.
    // Returns old status. Cannot call casgstatus directly, because we are racing with an
    // async wakeup that might come in from netpoll. If we see Gwaiting from the readgstatus,
    // it might have become Grunnable by the time we get to the cas. If we called casgstatus,
    // it would loop waiting for the status to go back to Gwaiting, which it never will.
    //
    //go:nosplit
    uint32_t casgcopystack(struct g* gp)
    {
        for(; ; )
        {
            auto oldstatus = readgstatus(gp) &^ _Gscan;
            if(oldstatus != _Gwaiting && oldstatus != _Grunnable)
            {
                go_throw("copystack: bad status, not Gwaiting or Grunnable"s);
            }
            if(rec::CompareAndSwap(gocpp::recv(gp->atomicstatus), oldstatus, _Gcopystack))
            {
                return oldstatus;
            }
        }
    }

    // casGToPreemptScan transitions gp from _Grunning to _Gscan|_Gpreempted.
    //
    // TODO(austin): This is the only status operation that both changes
    // the status and locks the _Gscan bit. Rethink this.
    void casGToPreemptScan(struct g* gp, uint32_t old, uint32_t go_new)
    {
        if(old != _Grunning || go_new != _Gscan | _Gpreempted)
        {
            go_throw("bad g transition"s);
        }
        acquireLockRank(lockRankGscan);
        for(; ! rec::CompareAndSwap(gocpp::recv(gp->atomicstatus), _Grunning, _Gscan | _Gpreempted); )
        {
        }
    }

    // casGFromPreempted attempts to transition gp from _Gpreempted to
    // _Gwaiting. If successful, the caller is responsible for
    // re-scheduling gp.
    bool casGFromPreempted(struct g* gp, uint32_t old, uint32_t go_new)
    {
        if(old != _Gpreempted || go_new != _Gwaiting)
        {
            go_throw("bad g transition"s);
        }
        gp->waitreason = waitReasonPreempted;
        return rec::CompareAndSwap(gocpp::recv(gp->atomicstatus), _Gpreempted, _Gwaiting);
    }

    // stwReason is an enumeration of reasons the world is stopping.
    // Reasons to stop-the-world.
    //
    // Avoid reusing reasons and add new ones instead.
    std::string rec::String(golang::runtime::stwReason r)
    {
        return stwReasonStrings[r];
    }

    bool rec::isGC(golang::runtime::stwReason r)
    {
        return r == stwGCMarkTerm || r == stwGCSweepTerm;
    }

    // If you add to this list, also add it to src/internal/trace/parser.go.
    // If you change the values of any of the stw* constants, bump the trace
    // version number and make a copy of this.
    gocpp::array<std::string, 17> stwReasonStrings = gocpp::Init<gocpp::array<std::string, 17>>([](auto& x) {
        x[stwUnknown] = "unknown"s;
        x[stwGCMarkTerm] = "GC mark termination"s;
        x[stwGCSweepTerm] = "GC sweep termination"s;
        x[stwWriteHeapDump] = "write heap dump"s;
        x[stwGoroutineProfile] = "goroutine profile"s;
        x[stwGoroutineProfileCleanup] = "goroutine profile cleanup"s;
        x[stwAllGoroutinesStack] = "all goroutines stack trace"s;
        x[stwReadMemStats] = "read mem stats"s;
        x[stwAllThreadsSyscall] = "AllThreadsSyscall"s;
        x[stwGOMAXPROCS] = "GOMAXPROCS"s;
        x[stwStartTrace] = "start trace"s;
        x[stwStopTrace] = "stop trace"s;
        x[stwForTestCountPagesInUse] = "CountPagesInUse (test)"s;
        x[stwForTestReadMetricsSlow] = "ReadMetricsSlow (test)"s;
        x[stwForTestReadMemStatsSlow] = "ReadMemStatsSlow (test)"s;
        x[stwForTestPageCachePagesLeaked] = "PageCachePagesLeaked (test)"s;
        x[stwForTestResetDebugLog] = "ResetDebugLog (test)"s;
    });
    // worldStop provides context from the stop-the-world required by the
    // start-the-world.
    
    template<typename T> requires gocpp::GoStruct<T>
    worldStop::operator T()
    {
        T result;
        result.reason = this->reason;
        result.start = this->start;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool worldStop::operator==(const T& ref) const
    {
        if (reason != ref.reason) return false;
        if (start != ref.start) return false;
        return true;
    }

    std::ostream& worldStop::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << reason;
        os << " " << start;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct worldStop& value)
    {
        return value.PrintTo(os);
    }

    // Temporary variable for stopTheWorld, when it can't write to the stack.
    //
    // Protected by worldsema.
    worldStop stopTheWorldContext;
    // stopTheWorld stops all P's from executing goroutines, interrupting
    // all goroutines at GC safe points and records reason as the reason
    // for the stop. On return, only the current goroutine's P is running.
    // stopTheWorld must not be called from a system stack and the caller
    // must not hold worldsema. The caller must call startTheWorld when
    // other P's should resume execution.
    //
    // stopTheWorld is safe for multiple goroutines to call at the
    // same time. Each will execute its own stop, and the stops will
    // be serialized.
    //
    // This is also used by routines that do stack dumps. If the system is
    // in panic or being exited, this may not reliably stop all
    // goroutines.
    //
    // Returns the STW context. When starting the world, this context must be
    // passed to startTheWorld.
    struct worldStop stopTheWorld(golang::runtime::stwReason reason)
    {
        semacquire(& worldsema);
        auto gp = getg();
        gp->m->preemptoff = rec::String(gocpp::recv(reason));
        systemstack([=]() mutable -> void
        {
            casGToWaiting(gp, _Grunning, waitReasonStoppingTheWorld);
            stopTheWorldContext = stopTheWorldWithSema(reason);
            casgstatus(gp, _Gwaiting, _Grunning);
        });
        return stopTheWorldContext;
    }

    // startTheWorld undoes the effects of stopTheWorld.
    //
    // w must be the worldStop returned by stopTheWorld.
    void startTheWorld(struct worldStop w)
    {
        systemstack([=]() mutable -> void
        {
            startTheWorldWithSema(0, w);
        });
        auto mp = acquirem();
        mp->preemptoff = ""s;
        semrelease1(& worldsema, true, 0);
        releasem(mp);
    }

    // stopTheWorldGC has the same effect as stopTheWorld, but blocks
    // until the GC is not running. It also blocks a GC from starting
    // until startTheWorldGC is called.
    struct worldStop stopTheWorldGC(golang::runtime::stwReason reason)
    {
        semacquire(& gcsema);
        return stopTheWorld(reason);
    }

    // startTheWorldGC undoes the effects of stopTheWorldGC.
    //
    // w must be the worldStop returned by stopTheWorld.
    void startTheWorldGC(struct worldStop w)
    {
        startTheWorld(w);
        semrelease(& gcsema);
    }

    // Holding worldsema grants an M the right to try to stop the world.
    uint32_t worldsema = 1;
    // Holding gcsema grants the M the right to block a GC, and blocks
    // until the current GC is done. In particular, it prevents gomaxprocs
    // from changing concurrently.
    //
    // TODO(mknyszek): Once gomaxprocs and the execution tracer can handle
    // being changed/enabled during a GC, remove this.
    uint32_t gcsema = 1;
    // stopTheWorldWithSema is the core implementation of stopTheWorld.
    // The caller is responsible for acquiring worldsema and disabling
    // preemption first and then should stopTheWorldWithSema on the system
    // stack:
    //
    //	semacquire(&worldsema, 0)
    //	m.preemptoff = "reason"
    //	var stw worldStop
    //	systemstack(func() {
    //		stw = stopTheWorldWithSema(reason)
    //	})
    //
    // When finished, the caller must either call startTheWorld or undo
    // these three operations separately:
    //
    //	m.preemptoff = ""
    //	systemstack(func() {
    //		now = startTheWorldWithSema(stw)
    //	})
    //	semrelease(&worldsema)
    //
    // It is allowed to acquire worldsema once and then execute multiple
    // startTheWorldWithSema/stopTheWorldWithSema pairs.
    // Other P's are able to execute between successive calls to
    // startTheWorldWithSema and stopTheWorldWithSema.
    // Holding worldsema causes any other goroutines invoking
    // stopTheWorld to block.
    //
    // Returns the STW context. When starting the world, this context must be
    // passed to startTheWorldWithSema.
    struct worldStop stopTheWorldWithSema(golang::runtime::stwReason reason)
    {
        auto trace = traceAcquire();
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::STWStart(gocpp::recv(trace), reason);
            traceRelease(trace);
        }
        auto gp = getg();
        if(gp->m->locks > 0)
        {
            go_throw("stopTheWorld: holding locks"s);
        }
        lock(& sched.lock);
        auto start = nanotime();
        sched.stopwait = gomaxprocs;
        rec::Store(gocpp::recv(sched.gcwaiting), true);
        preemptall();
        rec::ptr(gocpp::recv(gp->m->p))->status = _Pgcstop;
        sched.stopwait--;
        trace = traceAcquire();
        for(auto [gocpp_ignored, pp] : allp)
        {
            auto s = pp->status;
            if(s == _Psyscall && atomic::Cas(& pp->status, s, _Pgcstop))
            {
                if(rec::ok(gocpp::recv(trace)))
                {
                    rec::GoSysBlock(gocpp::recv(trace), pp);
                    rec::ProcSteal(gocpp::recv(trace), pp, false);
                }
                pp->syscalltick++;
                sched.stopwait--;
            }
        }
        if(rec::ok(gocpp::recv(trace)))
        {
            traceRelease(trace);
        }
        auto now = nanotime();
        for(; ; )
        {
            auto [pp, gocpp_id_1] = pidleget(now);
            if(pp == nullptr)
            {
                break;
            }
            pp->status = _Pgcstop;
            sched.stopwait--;
        }
        auto wait = sched.stopwait > 0;
        unlock(& sched.lock);
        if(wait)
        {
            for(; ; )
            {
                if(notetsleep(& sched.stopnote, 100 * 1000))
                {
                    noteclear(& sched.stopnote);
                    break;
                }
                preemptall();
            }
        }
        auto startTime = nanotime() - start;
        if(rec::isGC(gocpp::recv(reason)))
        {
            rec::record(gocpp::recv(sched.stwStoppingTimeGC), startTime);
        }
        else
        {
            rec::record(gocpp::recv(sched.stwStoppingTimeOther), startTime);
        }
        auto bad = ""s;
        if(sched.stopwait != 0)
        {
            bad = "stopTheWorld: not stopped (stopwait != 0)"s;
        }
        else
        {
            for(auto [gocpp_ignored, pp] : allp)
            {
                if(pp->status != _Pgcstop)
                {
                    bad = "stopTheWorld: not stopped (status != _Pgcstop)"s;
                }
            }
        }
        if(rec::Load(gocpp::recv(freezing)))
        {
            lock(& deadlock);
            lock(& deadlock);
        }
        if(bad != ""s)
        {
            go_throw(bad);
        }
        worldStopped();
        return gocpp::Init<worldStop>([=](auto& x) {
            x.reason = reason;
            x.start = start;
        });
    }

    // reason is the same STW reason passed to stopTheWorld. start is the start
    // time returned by stopTheWorld.
    //
    // now is the current time; prefer to pass 0 to capture a fresh timestamp.
    //
    // stattTheWorldWithSema returns now.
    int64_t startTheWorldWithSema(int64_t now, struct worldStop w)
    {
        assertWorldStopped();
        auto mp = acquirem();
        if(netpollinited())
        {
            auto [list, delta] = netpoll(0);
            injectglist(& list);
            netpollAdjustWaiters(delta);
        }
        lock(& sched.lock);
        auto procs = gomaxprocs;
        if(newprocs != 0)
        {
            procs = newprocs;
            newprocs = 0;
        }
        auto p1 = procresize(procs);
        rec::Store(gocpp::recv(sched.gcwaiting), false);
        if(rec::Load(gocpp::recv(sched.sysmonwait)))
        {
            rec::Store(gocpp::recv(sched.sysmonwait), false);
            notewakeup(& sched.sysmonnote);
        }
        unlock(& sched.lock);
        worldStarted();
        for(; p1 != nullptr; )
        {
            auto p = p1;
            p1 = rec::ptr(gocpp::recv(p1->link));
            if(p->m != 0)
            {
                auto mp = rec::ptr(gocpp::recv(p->m));
                p->m = 0;
                if(mp->nextp != 0)
                {
                    go_throw("startTheWorld: inconsistent mp->nextp"s);
                }
                rec::set(gocpp::recv(mp->nextp), p);
                notewakeup(& mp->park);
            }
            else
            {
                newm(nullptr, p, - 1);
            }
        }
        if(now == 0)
        {
            now = nanotime();
        }
        auto totalTime = now - w.start;
        if(rec::isGC(gocpp::recv(w.reason)))
        {
            rec::record(gocpp::recv(sched.stwTotalTimeGC), totalTime);
        }
        else
        {
            rec::record(gocpp::recv(sched.stwTotalTimeOther), totalTime);
        }
        auto trace = traceAcquire();
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::STWDone(gocpp::recv(trace));
            traceRelease(trace);
        }
        wakep();
        releasem(mp);
        return now;
    }

    // usesLibcall indicates whether this runtime performs system calls
    // via libcall.
    bool usesLibcall()
    {
        //Go switch emulation
        {
            auto condition = GOOS;
            int conditionId = -1;
            if(condition == "aix"s) { conditionId = 0; }
            else if(condition == "darwin"s) { conditionId = 1; }
            else if(condition == "illumos"s) { conditionId = 2; }
            else if(condition == "ios"s) { conditionId = 3; }
            else if(condition == "solaris"s) { conditionId = 4; }
            else if(condition == "windows"s) { conditionId = 5; }
            else if(condition == "openbsd"s) { conditionId = 6; }
            switch(conditionId)
            {
                case 0:
                case 1:
                case 2:
                case 3:
                case 4:
                case 5:
                    return true;
                    break;
                case 6:
                    return GOARCH != "mips64"s;
                    break;
            }
        }
        return false;
    }

    // mStackIsSystemAllocated indicates whether this runtime starts on a
    // system-allocated stack.
    bool mStackIsSystemAllocated()
    {
        //Go switch emulation
        {
            auto condition = GOOS;
            int conditionId = -1;
            if(condition == "aix"s) { conditionId = 0; }
            else if(condition == "darwin"s) { conditionId = 1; }
            else if(condition == "plan9"s) { conditionId = 2; }
            else if(condition == "illumos"s) { conditionId = 3; }
            else if(condition == "ios"s) { conditionId = 4; }
            else if(condition == "solaris"s) { conditionId = 5; }
            else if(condition == "windows"s) { conditionId = 6; }
            else if(condition == "openbsd"s) { conditionId = 7; }
            switch(conditionId)
            {
                case 0:
                case 1:
                case 2:
                case 3:
                case 4:
                case 5:
                case 6:
                    return true;
                    break;
                case 7:
                    return GOARCH != "mips64"s;
                    break;
            }
        }
        return false;
    }

    // mstart is the entry-point for new Ms.
    // It is written in assembly, uses ABI0, is marked TOPFRAME, and calls mstart0.
    void mstart()
    /* convertBlockStmt, nil block */;

    // mstart0 is the Go entry-point for new Ms.
    // This must not split the stack because we may not even have stack
    // bounds set up yet.
    //
    // May run during STW (because it doesn't have a P yet), so write
    // barriers are not allowed.
    //
    //go:nosplit
    //go:nowritebarrierrec
    void mstart0()
    {
        auto gp = getg();
        auto osStack = gp->stack.lo == 0;
        if(osStack)
        {
            auto size = gp->stack.hi;
            if(size == 0)
            {
                size = 16384 * sys::StackGuardMultiplier;
            }
            gp->stack.hi = uintptr_t(noescape(unsafe::Pointer(& size)));
            gp->stack.lo = gp->stack.hi - size + 1024;
        }
        gp->stackguard0 = gp->stack.lo + stackGuard;
        gp->stackguard1 = gp->stackguard0;
        mstart1();
        if(mStackIsSystemAllocated())
        {
            osStack = true;
        }
        mexit(osStack);
    }

    // The go:noinline is to guarantee the getcallerpc/getcallersp below are safe,
    // so that we can set up g0.sched to return to the call of mstart1 above.
    //
    //go:noinline
    void mstart1()
    {
        auto gp = getg();
        if(gp != gp->m->g0)
        {
            go_throw("bad runtime·mstart"s);
        }
        gp->sched.g = guintptr(unsafe::Pointer(gp));
        gp->sched.pc = getcallerpc();
        gp->sched.sp = getcallersp();
        asminit();
        minit();
        if(gp->m == & m0)
        {
            mstartm0();
        }
        if(auto fn = gp->m->mstartfn; fn != nullptr)
        {
            fn();
        }
        if(gp->m != & m0)
        {
            acquirep(rec::ptr(gocpp::recv(gp->m->nextp)));
            gp->m->nextp = 0;
        }
        schedule();
    }

    // mstartm0 implements part of mstart1 that only runs on the m0.
    //
    // Write barriers are allowed here because we know the GC can't be
    // running yet, so they'll be no-ops.
    //
    //go:yeswritebarrierrec
    void mstartm0()
    {
        if((iscgo || GOOS == "windows"s) && ! cgoHasExtraM)
        {
            cgoHasExtraM = true;
            newextram();
        }
        initsig(false);
    }

    // mPark causes a thread to park itself, returning once woken.
    //
    //go:nosplit
    void mPark()
    {
        auto gp = getg();
        notesleep(& gp->m->park);
        noteclear(& gp->m->park);
    }

    // mexit tears down and exits the current thread.
    //
    // Don't call this directly to exit the thread, since it must run at
    // the top of the thread stack. Instead, use gogo(&gp.m.g0.sched) to
    // unwind the stack to the point that exits the thread.
    //
    // It is entered with m.p != nil, so write barriers are allowed. It
    // will release the P before exiting.
    //
    //go:yeswritebarrierrec
    void mexit(bool osStack)
    {
        auto mp = getg()->m;
        if(mp == & m0)
        {
            handoffp(releasep());
            lock(& sched.lock);
            sched.nmfreed++;
            checkdead();
            unlock(& sched.lock);
            mPark();
            go_throw("locked m0 woke up"s);
        }
        sigblock(true);
        unminit();
        if(mp->gsignal != nullptr)
        {
            stackfree(mp->gsignal->stack);
            mp->gsignal = nullptr;
        }
        lock(& sched.lock);
        for(auto pprev = & allm; *pprev != nullptr; pprev = & (*pprev)->alllink)
        {
            if(*pprev == mp)
            {
                *pprev = mp->alllink;
                goto found;
            }
        }
        go_throw("m not found in allm"s);
        found:
        rec::Store(gocpp::recv(mp->freeWait), freeMWait);
        mp->freelink = sched.freem;
        sched.freem = mp;
        unlock(& sched.lock);
        atomic::Xadd64(& ncgocall, int64_t(mp->ncgocall));
        rec::Add(gocpp::recv(sched.totalRuntimeLockWaitTime), rec::Load(gocpp::recv(mp->mLockProfile.waitTime)));
        handoffp(releasep());
        lock(& sched.lock);
        sched.nmfreed++;
        checkdead();
        unlock(& sched.lock);
        if(GOOS == "darwin"s || GOOS == "ios"s)
        {
            if(rec::Load(gocpp::recv(mp->signalPending)) != 0)
            {
                rec::Add(gocpp::recv(pendingPreemptSignals), - 1);
            }
        }
        mdestroy(mp);
        if(osStack)
        {
            rec::Store(gocpp::recv(mp->freeWait), freeMRef);
            return;
        }
        exitThread(& mp->freeWait);
    }

    // forEachP calls fn(p) for every P p when p reaches a GC safe point.
    // If a P is currently executing code, this will bring the P to a GC
    // safe point and execute fn on that P. If the P is not executing code
    // (it is idle or in a syscall), this will call fn(p) directly while
    // preventing the P from exiting its state. This does not ensure that
    // fn will run on every CPU executing Go code, but it acts as a global
    // memory barrier. GC uses this as a "ragged barrier."
    //
    // The caller must hold worldsema. fn must not refer to any
    // part of the current goroutine's stack, since the GC may move it.
    void forEachP(golang::runtime::waitReason reason, std::function<void (p*)> fn)
    {
        systemstack([=]() mutable -> void
        {
            auto gp = getg()->m->curg;
            casGToWaiting(gp, _Grunning, reason);
            forEachPInternal(fn);
            casgstatus(gp, _Gwaiting, _Grunning);
        });
    }

    // forEachPInternal calls fn(p) for every P p when p reaches a GC safe point.
    // It is the internal implementation of forEachP.
    //
    // The caller must hold worldsema and either must ensure that a GC is not
    // running (otherwise this may deadlock with the GC trying to preempt this P)
    // or it must leave its goroutine in a preemptible state before it switches
    // to the systemstack. Due to these restrictions, prefer forEachP when possible.
    //
    //go:systemstack
    void forEachPInternal(std::function<void (p*)> fn)
    {
        auto mp = acquirem();
        auto pp = rec::ptr(gocpp::recv(getg()->m->p));
        lock(& sched.lock);
        if(sched.safePointWait != 0)
        {
            go_throw("forEachP: sched.safePointWait != 0"s);
        }
        sched.safePointWait = gomaxprocs - 1;
        sched.safePointFn = fn;
        for(auto [gocpp_ignored, p2] : allp)
        {
            if(p2 != pp)
            {
                atomic::Store(& p2->runSafePointFn, 1);
            }
        }
        preemptall();
        for(auto p = rec::ptr(gocpp::recv(sched.pidle)); p != nullptr; p = rec::ptr(gocpp::recv(p->link)))
        {
            if(atomic::Cas(& p->runSafePointFn, 1, 0))
            {
                fn(p);
                sched.safePointWait--;
            }
        }
        auto wait = sched.safePointWait > 0;
        unlock(& sched.lock);
        fn(pp);
        for(auto [gocpp_ignored, p2] : allp)
        {
            auto s = p2->status;
            auto trace = traceAcquire();
            if(s == _Psyscall && p2->runSafePointFn == 1 && atomic::Cas(& p2->status, s, _Pidle))
            {
                if(rec::ok(gocpp::recv(trace)))
                {
                    rec::GoSysBlock(gocpp::recv(trace), p2);
                    rec::ProcSteal(gocpp::recv(trace), p2, false);
                    traceRelease(trace);
                }
                p2->syscalltick++;
                handoffp(p2);
            }
            else
            if(rec::ok(gocpp::recv(trace)))
            {
                traceRelease(trace);
            }
        }
        if(wait)
        {
            for(; ; )
            {
                if(notetsleep(& sched.safePointNote, 100 * 1000))
                {
                    noteclear(& sched.safePointNote);
                    break;
                }
                preemptall();
            }
        }
        if(sched.safePointWait != 0)
        {
            go_throw("forEachP: not done"s);
        }
        for(auto [gocpp_ignored, p2] : allp)
        {
            if(p2->runSafePointFn != 0)
            {
                go_throw("forEachP: P did not run fn"s);
            }
        }
        lock(& sched.lock);
        sched.safePointFn = nullptr;
        unlock(& sched.lock);
        releasem(mp);
    }

    // runSafePointFn runs the safe point function, if any, for this P.
    // This should be called like
    //
    //	if getg().m.p.runSafePointFn != 0 {
    //	    runSafePointFn()
    //	}
    //
    // runSafePointFn must be checked on any transition in to _Pidle or
    // _Psyscall to avoid a race where forEachP sees that the P is running
    // just before the P goes into _Pidle/_Psyscall and neither forEachP
    // nor the P run the safe-point function.
    void runSafePointFn()
    {
        auto p = rec::ptr(gocpp::recv(getg()->m->p));
        if(! atomic::Cas(& p->runSafePointFn, 1, 0))
        {
            return;
        }
        rec::safePointFn(gocpp::recv(sched), p);
        lock(& sched.lock);
        sched.safePointWait--;
        if(sched.safePointWait == 0)
        {
            notewakeup(& sched.safePointNote);
        }
        unlock(& sched.lock);
    }

    // When running with cgo, we call _cgo_thread_start
    // to start threads for us so that we can play nicely with
    // foreign code.
    unsafe::Pointer cgoThreadStart;
    
    template<typename T> requires gocpp::GoStruct<T>
    cgothreadstart::operator T()
    {
        T result;
        result.g = this->g;
        result.tls = this->tls;
        result.fn = this->fn;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool cgothreadstart::operator==(const T& ref) const
    {
        if (g != ref.g) return false;
        if (tls != ref.tls) return false;
        if (fn != ref.fn) return false;
        return true;
    }

    std::ostream& cgothreadstart::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << g;
        os << " " << tls;
        os << " " << fn;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct cgothreadstart& value)
    {
        return value.PrintTo(os);
    }

    // Allocate a new m unassociated with any thread.
    // Can use p for allocation context if needed.
    // fn is recorded as the new m's m.mstartfn.
    // id is optional pre-allocated m ID. Omit by passing -1.
    //
    // This function is allowed to have write barriers even if the caller
    // isn't because it borrows pp.
    //
    //go:yeswritebarrierrec
    struct m* allocm(struct p* pp, std::function<void ()> fn, int64_t id)
    {
        rec::rlock(gocpp::recv(allocmLock));
        acquirem();
        auto gp = getg();
        if(gp->m->p == 0)
        {
            acquirep(pp);
        }
        if(sched.freem != nullptr)
        {
            lock(& sched.lock);
            m* newList = {};
            for(auto freem = sched.freem; freem != nullptr; )
            {
                auto wait = rec::Load(gocpp::recv(freem->freeWait));
                if(wait == freeMWait)
                {
                    auto next = freem->freelink;
                    freem->freelink = newList;
                    newList = freem;
                    freem = next;
                    continue;
                }
                if(traceEnabled() || traceShuttingDown())
                {
                    traceThreadDestroy(freem);
                }
                if(wait == freeMStack)
                {
                    systemstack([=]() mutable -> void
                    {
                        stackfree(freem->g0->stack);
                    });
                }
                freem = freem->freelink;
            }
            sched.freem = newList;
            unlock(& sched.lock);
        }
        auto mp = new(m);
        mp->mstartfn = fn;
        mcommoninit(mp, id);
        if(iscgo || mStackIsSystemAllocated())
        {
            mp->g0 = malg(- 1);
        }
        else
        {
            mp->g0 = malg(16384 * sys::StackGuardMultiplier);
        }
        mp->g0->m = mp;
        if(pp == rec::ptr(gocpp::recv(gp->m->p)))
        {
            releasep();
        }
        releasem(gp->m);
        rec::runlock(gocpp::recv(allocmLock));
        return mp;
    }

    // needm is called when a cgo callback happens on a
    // thread without an m (a thread not created by Go).
    // In this case, needm is expected to find an m to use
    // and return with m, g initialized correctly.
    // Since m and g are not set now (likely nil, but see below)
    // needm is limited in what routines it can call. In particular
    // it can only call nosplit functions (textflag 7) and cannot
    // do any scheduling that requires an m.
    //
    // In order to avoid needing heavy lifting here, we adopt
    // the following strategy: there is a stack of available m's
    // that can be stolen. Using compare-and-swap
    // to pop from the stack has ABA races, so we simulate
    // a lock by doing an exchange (via Casuintptr) to steal the stack
    // head and replace the top pointer with MLOCKED (1).
    // This serves as a simple spin lock that we can use even
    // without an m. The thread that locks the stack in this way
    // unlocks the stack by storing a valid stack head pointer.
    //
    // In order to make sure that there is always an m structure
    // available to be stolen, we maintain the invariant that there
    // is always one more than needed. At the beginning of the
    // program (if cgo is in use) the list is seeded with a single m.
    // If needm finds that it has taken the last m off the list, its job
    // is - once it has installed its own m so that it can do things like
    // allocate memory - to create a spare m and put it on the list.
    //
    // Each of these extra m's also has a g0 and a curg that are
    // pressed into service as the scheduling stack and current
    // goroutine for the duration of the cgo callback.
    //
    // It calls dropm to put the m back on the list,
    // 1. when the callback is done with the m in non-pthread platforms,
    // 2. or when the C thread exiting on pthread platforms.
    //
    // The signal argument indicates whether we're called from a signal
    // handler.
    //
    //go:nosplit
    void needm(bool signal)
    {
        if((iscgo || GOOS == "windows"s) && ! cgoHasExtraM)
        {
            writeErrStr("fatal error: cgo callback before cgo call\n"s);
            exit(1);
        }
        // Save and block signals before getting an M.
        // The signal handler may call needm itself,
        // and we must avoid a deadlock. Also, once g is installed,
        // any incoming signals will try to execute,
        // but we won't have the sigaltstack settings and other data
        // set up appropriately until the end of minit, which will
        // unblock the signals. This is the same dance as when
        // starting a new m to run Go code via newosproc.
        sigset sigmask = {};
        sigsave(& sigmask);
        sigblock(false);
        auto [mp, last] = getExtraM();
        mp->needextram = last;
        mp->sigmask = sigmask;
        osSetupTLS(mp);
        setg(mp->g0);
        auto sp = getcallersp();
        callbackUpdateSystemStack(mp, sp, signal);
        mp->isExtraInC = false;
        asminit();
        minit();
        // Emit a trace event for this dead -> syscall transition,
        // but only in the new tracer and only if we're not in a signal handler.
        //
        // N.B. the tracer can run on a bare M just fine, we just have
        // to make sure to do this before setg(nil) and unminit.
        traceLocker trace = {};
        if(goexperiment::ExecTracer2 && ! signal)
        {
            trace = traceAcquire();
        }
        casgstatus(mp->curg, _Gdead, _Gsyscall);
        rec::Add(gocpp::recv(sched.ngsys), - 1);
        if(goexperiment::ExecTracer2 && ! signal)
        {
            if(rec::ok(gocpp::recv(trace)))
            {
                rec::GoCreateSyscall(gocpp::recv(trace), mp->curg);
                traceRelease(trace);
            }
        }
        mp->isExtraInSig = signal;
    }

    // Acquire an extra m and bind it to the C thread when a pthread key has been created.
    //
    //go:nosplit
    void needAndBindM()
    {
        needm(false);
        if(_cgo_pthread_key_created != nullptr && *(uintptr_t*)(_cgo_pthread_key_created) != 0)
        {
            cgoBindM();
        }
    }

    // newextram allocates m's and puts them on the extra list.
    // It is called with a working local m, so that it can do things
    // like call schedlock and allocate.
    void newextram()
    {
        auto c = rec::Swap(gocpp::recv(extraMWaiters), 0);
        if(c > 0)
        {
            for(auto i = uint32_t(0); i < c; i++)
            {
                oneNewExtraM();
            }
        }
        else
        if(rec::Load(gocpp::recv(extraMLength)) == 0)
        {
            oneNewExtraM();
        }
    }

    // oneNewExtraM allocates an m and puts it on the extra list.
    void oneNewExtraM()
    {
        auto mp = allocm(nullptr, nullptr, - 1);
        auto gp = malg(4096);
        gp->sched.pc = abi::FuncPCABI0(goexit) + sys::PCQuantum;
        gp->sched.sp = gp->stack.hi;
        gp->sched.sp -= 4 * goarch::PtrSize;
        gp->sched.lr = 0;
        gp->sched.g = guintptr(unsafe::Pointer(gp));
        gp->syscallpc = gp->sched.pc;
        gp->syscallsp = gp->sched.sp;
        gp->stktopsp = gp->sched.sp;
        casgstatus(gp, _Gidle, _Gdead);
        gp->m = mp;
        mp->curg = gp;
        mp->isextra = true;
        mp->isExtraInC = true;
        mp->lockedInt++;
        rec::set(gocpp::recv(mp->lockedg), gp);
        rec::set(gocpp::recv(gp->lockedm), mp);
        gp->goid = rec::Add(gocpp::recv(sched.goidgen), 1);
        if(raceenabled)
        {
            gp->racectx = racegostart(abi::FuncPCABIInternal(newextram) + sys::PCQuantum);
        }
        auto trace = traceAcquire();
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::OneNewExtraM(gocpp::recv(trace), gp);
            traceRelease(trace);
        }
        allgadd(gp);
        rec::Add(gocpp::recv(sched.ngsys), 1);
        addExtraM(mp);
    }

    // dropm puts the current m back onto the extra list.
    //
    // 1. On systems without pthreads, like Windows
    // dropm is called when a cgo callback has called needm but is now
    // done with the callback and returning back into the non-Go thread.
    //
    // The main expense here is the call to signalstack to release the
    // m's signal stack, and then the call to needm on the next callback
    // from this thread. It is tempting to try to save the m for next time,
    // which would eliminate both these costs, but there might not be
    // a next time: the current thread (which Go does not control) might exit.
    // If we saved the m for that thread, there would be an m leak each time
    // such a thread exited. Instead, we acquire and release an m on each
    // call. These should typically not be scheduling operations, just a few
    // atomics, so the cost should be small.
    //
    // 2. On systems with pthreads
    // dropm is called while a non-Go thread is exiting.
    // We allocate a pthread per-thread variable using pthread_key_create,
    // to register a thread-exit-time destructor.
    // And store the g into a thread-specific value associated with the pthread key,
    // when first return back to C.
    // So that the destructor would invoke dropm while the non-Go thread is exiting.
    // This is much faster since it avoids expensive signal-related syscalls.
    //
    // This always runs without a P, so //go:nowritebarrierrec is required.
    //
    // This may run with a different stack than was recorded in g0 (there is no
    // call to callbackUpdateSystemStack prior to dropm), so this must be
    // //go:nosplit to avoid the stack bounds check.
    //
    //go:nowritebarrierrec
    //go:nosplit
    void dropm()
    {
        auto mp = getg()->m;
        // Emit a trace event for this syscall -> dead transition,
        // but only in the new tracer.
        //
        // N.B. the tracer can run on a bare M just fine, we just have
        // to make sure to do this before setg(nil) and unminit.
        traceLocker trace = {};
        if(goexperiment::ExecTracer2 && ! mp->isExtraInSig)
        {
            trace = traceAcquire();
        }
        casgstatus(mp->curg, _Gsyscall, _Gdead);
        mp->curg->preemptStop = false;
        rec::Add(gocpp::recv(sched.ngsys), 1);
        if(goexperiment::ExecTracer2 && ! mp->isExtraInSig)
        {
            if(rec::ok(gocpp::recv(trace)))
            {
                rec::GoDestroySyscall(gocpp::recv(trace));
                traceRelease(trace);
            }
        }
        if(goexperiment::ExecTracer2)
        {
            mp->syscalltick--;
        }
        rec::reset(gocpp::recv(mp->curg->trace));
        if(goexperiment::ExecTracer2 && (traceEnabled() || traceShuttingDown()))
        {
            lock(& sched.lock);
            traceThreadDestroy(mp);
            unlock(& sched.lock);
        }
        mp->isExtraInSig = false;
        auto sigmask = mp->sigmask;
        sigblock(false);
        unminit();
        setg(nullptr);
        auto g0 = mp->g0;
        g0->stack.hi = 0;
        g0->stack.lo = 0;
        g0->stackguard0 = 0;
        g0->stackguard1 = 0;
        putExtraM(mp);
        msigrestore(sigmask);
    }

    // bindm store the g0 of the current m into a thread-specific value.
    //
    // We allocate a pthread per-thread variable using pthread_key_create,
    // to register a thread-exit-time destructor.
    // We are here setting the thread-specific value of the pthread key, to enable the destructor.
    // So that the pthread_key_destructor would dropm while the C thread is exiting.
    //
    // And the saved g will be used in pthread_key_destructor,
    // since the g stored in the TLS by Go might be cleared in some platforms,
    // before the destructor invoked, so, we restore g by the stored g, before dropm.
    //
    // We store g0 instead of m, to make the assembly code simpler,
    // since we need to restore g0 in runtime.cgocallback.
    //
    // On systems without pthreads, like Windows, bindm shouldn't be used.
    //
    // NOTE: this always runs without a P, so, nowritebarrierrec required.
    //
    //go:nosplit
    //go:nowritebarrierrec
    void cgoBindM()
    {
        if(GOOS == "windows"s || GOOS == "plan9"s)
        {
            fatal("bindm in unexpected GOOS"s);
        }
        auto g = getg();
        if(g->m->g0 != g)
        {
            fatal("the current g is not g0"s);
        }
        if(_cgo_bindm != nullptr)
        {
            asmcgocall(_cgo_bindm, unsafe::Pointer(g));
        }
    }

    // A helper function for EnsureDropM.
    uintptr_t getm()
    {
        return uintptr_t(unsafe::Pointer(getg()->m));
    }

    // Locking linked list of extra M's, via mp.schedlink. Must be accessed
    // only via lockextra/unlockextra.
    //
    // Can't be atomic.Pointer[m] because we use an invalid pointer as a
    // "locked" sentinel value. M's on this list remain visible to the GC
    // because their mp.curg is on allgs.
    // Number of M's in the extraM list.
    // Number of waiters in lockextra.
    // Number of extra M's in use by threads.
    atomic::Uintptr extraM;
    atomic::Uint32 extraMLength;
    atomic::Uint32 extraMWaiters;
    atomic::Uint32 extraMInUse;
    // lockextra locks the extra list and returns the list head.
    // The caller must unlock the list by storing a new list head
    // to extram. If nilokay is true, then lockextra will
    // return a nil list head if that's what it finds. If nilokay is false,
    // lockextra will keep waiting until the list head is no longer nil.
    //
    //go:nosplit
    struct m* lockextra(bool nilokay)
    {
        auto locked = 1;
        auto incr = false;
        for(; ; )
        {
            auto old = rec::Load(gocpp::recv(extraM));
            if(old == locked)
            {
                osyield_no_g();
                continue;
            }
            if(old == 0 && ! nilokay)
            {
                if(! incr)
                {
                    rec::Add(gocpp::recv(extraMWaiters), 1);
                    incr = true;
                }
                usleep_no_g(1);
                continue;
            }
            if(rec::CompareAndSwap(gocpp::recv(extraM), old, locked))
            {
                return (m*)(unsafe::Pointer(old));
            }
            osyield_no_g();
            continue;
        }
    }

    //go:nosplit
    void unlockextra(struct m* mp, int32_t delta)
    {
        rec::Add(gocpp::recv(extraMLength), delta);
        rec::Store(gocpp::recv(extraM), uintptr_t(unsafe::Pointer(mp)));
    }

    // Return an M from the extra M list. Returns last == true if the list becomes
    // empty because of this call.
    //
    // Spins waiting for an extra M, so caller must ensure that the list always
    // contains or will soon contain at least one M.
    //
    //go:nosplit
    std::tuple<struct m*, bool> getExtraM()
    {
        struct m* mp;
        bool last;
        mp = lockextra(false);
        rec::Add(gocpp::recv(extraMInUse), 1);
        unlockextra(rec::ptr(gocpp::recv(mp->schedlink)), - 1);
        return {mp, rec::ptr(gocpp::recv(mp->schedlink)) == nullptr};
    }

    // Returns an extra M back to the list. mp must be from getExtraM. Newly
    // allocated M's should use addExtraM.
    //
    //go:nosplit
    void putExtraM(struct m* mp)
    {
        rec::Add(gocpp::recv(extraMInUse), - 1);
        addExtraM(mp);
    }

    // Adds a newly allocated M to the extra M list.
    //
    //go:nosplit
    void addExtraM(struct m* mp)
    {
        auto mnext = lockextra(true);
        rec::set(gocpp::recv(mp->schedlink), mnext);
        unlockextra(mp, 1);
    }

    // allocmLock is locked for read when creating new Ms in allocm and their
    // addition to allm. Thus acquiring this lock for write blocks the
    // creation of new Ms.
    // execLock serializes exec and clone to avoid bugs or unspecified
    // behaviour around exec'ing while creating/destroying threads. See
    // issue #19546.
    rwmutex allocmLock;
    rwmutex execLock;
    // These errors are reported (via writeErrStr) by some OS-specific
    // versions of newosproc and newosproc0.
    std::string failthreadcreate = "runtime: failed to create new OS thread\n"s;
    std::string failallocatestack = "runtime: failed to allocate stack for the new OS thread\n"s;
    struct gocpp_id_2
    {
        mutex lock;
        golang::runtime::muintptr newm;
        bool waiting;
        note wake;
        uint32_t haveTemplateThread;

        using isGoStruct = void;

        template<typename T> requires gocpp::GoStruct<T>
        operator T()
        {
            T result;
            result.lock = this->lock;
            result.newm = this->newm;
            result.waiting = this->waiting;
            result.wake = this->wake;
            result.haveTemplateThread = this->haveTemplateThread;
            return result;
        }

        template<typename T> requires gocpp::GoStruct<T>
        bool operator==(const T& ref) const
        {
            if (lock != ref.lock) return false;
            if (newm != ref.newm) return false;
            if (waiting != ref.waiting) return false;
            if (wake != ref.wake) return false;
            if (haveTemplateThread != ref.haveTemplateThread) return false;
            return true;
        }

        std::ostream& PrintTo(std::ostream& os) const
        {
            os << '{';
            os << "" << lock;
            os << " " << newm;
            os << " " << waiting;
            os << " " << wake;
            os << " " << haveTemplateThread;
            os << '}';
            return os;
        }
    };

    std::ostream& operator<<(std::ostream& os, const struct gocpp_id_2& value)
    {
        return value.PrintTo(os);
    }


    // newmHandoff contains a list of m structures that need new OS threads.
    // This is used by newm in situations where newm itself can't safely
    // start an OS thread.
    gocpp_id_2 newmHandoff;
    // Create a new m. It will start off with a call to fn, or else the scheduler.
    // fn needs to be static and not a heap allocated closure.
    // May run with m.p==nil, so write barriers are not allowed.
    //
    // id is optional pre-allocated m ID. Omit by passing -1.
    //
    //go:nowritebarrierrec
    void newm(std::function<void ()> fn, struct p* pp, int64_t id)
    {
        acquirem();
        auto mp = allocm(pp, fn, id);
        rec::set(gocpp::recv(mp->nextp), pp);
        mp->sigmask = initSigmask;
        if(auto gp = getg(); gp != nullptr && gp->m != nullptr && (gp->m->lockedExt != 0 || gp->m->incgo) && GOOS != "plan9"s)
        {
            lock(& newmHandoff.lock);
            if(newmHandoff.haveTemplateThread == 0)
            {
                go_throw("on a locked thread with no template thread"s);
            }
            mp->schedlink = newmHandoff.newm;
            rec::set(gocpp::recv(newmHandoff.newm), mp);
            if(newmHandoff.waiting)
            {
                newmHandoff.waiting = false;
                notewakeup(& newmHandoff.wake);
            }
            unlock(& newmHandoff.lock);
            releasem(getg()->m);
            return;
        }
        newm1(mp);
        releasem(getg()->m);
    }

    void newm1(struct m* mp)
    {
        if(iscgo)
        {
            cgothreadstart ts = {};
            if(_cgo_thread_start == nullptr)
            {
                go_throw("_cgo_thread_start missing"s);
            }
            rec::set(gocpp::recv(ts.g), mp->g0);
            ts.tls = (uint64_t*)(unsafe::Pointer(& mp->tls[0]));
            ts.fn = unsafe::Pointer(abi::FuncPCABI0(mstart));
            if(msanenabled)
            {
                msanwrite(unsafe::Pointer(& ts), gocpp::Sizeof<cgothreadstart>());
            }
            if(asanenabled)
            {
                asanwrite(unsafe::Pointer(& ts), gocpp::Sizeof<cgothreadstart>());
            }
            rec::rlock(gocpp::recv(execLock));
            asmcgocall(_cgo_thread_start, unsafe::Pointer(& ts));
            rec::runlock(gocpp::recv(execLock));
            return;
        }
        rec::rlock(gocpp::recv(execLock));
        newosproc(mp);
        rec::runlock(gocpp::recv(execLock));
    }

    // startTemplateThread starts the template thread if it is not already
    // running.
    //
    // The calling thread must itself be in a known-good state.
    void startTemplateThread()
    {
        if(GOARCH == "wasm"s)
        {
            return;
        }
        auto mp = acquirem();
        if(! atomic::Cas(& newmHandoff.haveTemplateThread, 0, 1))
        {
            releasem(mp);
            return;
        }
        newm(templateThread, nullptr, - 1);
        releasem(mp);
    }

    // templateThread is a thread in a known-good state that exists solely
    // to start new threads in known-good states when the calling thread
    // may not be in a good state.
    //
    // Many programs never need this, so templateThread is started lazily
    // when we first enter a state that might lead to running on a thread
    // in an unknown state.
    //
    // templateThread runs on an M without a P, so it must not have write
    // barriers.
    //
    //go:nowritebarrierrec
    void templateThread()
    {
        lock(& sched.lock);
        sched.nmsys++;
        checkdead();
        unlock(& sched.lock);
        for(; ; )
        {
            lock(& newmHandoff.lock);
            for(; newmHandoff.newm != 0; )
            {
                auto newm = rec::ptr(gocpp::recv(newmHandoff.newm));
                newmHandoff.newm = 0;
                unlock(& newmHandoff.lock);
                for(; newm != nullptr; )
                {
                    auto next = rec::ptr(gocpp::recv(newm->schedlink));
                    newm->schedlink = 0;
                    newm1(newm);
                    newm = next;
                }
                lock(& newmHandoff.lock);
            }
            newmHandoff.waiting = true;
            noteclear(& newmHandoff.wake);
            unlock(& newmHandoff.lock);
            notesleep(& newmHandoff.wake);
        }
    }

    // Stops execution of the current m until new work is available.
    // Returns with acquired P.
    void stopm()
    {
        auto gp = getg();
        if(gp->m->locks != 0)
        {
            go_throw("stopm holding locks"s);
        }
        if(gp->m->p != 0)
        {
            go_throw("stopm holding p"s);
        }
        if(gp->m->spinning)
        {
            go_throw("stopm spinning"s);
        }
        lock(& sched.lock);
        mput(gp->m);
        unlock(& sched.lock);
        mPark();
        acquirep(rec::ptr(gocpp::recv(gp->m->nextp)));
        gp->m->nextp = 0;
    }

    void mspinning()
    {
        getg()->m->spinning = true;
    }

    // Schedules some M to run the p (creates an M if necessary).
    // If p==nil, tries to get an idle P, if no idle P's does nothing.
    // May run with m.p==nil, so write barriers are not allowed.
    // If spinning is set, the caller has incremented nmspinning and must provide a
    // P. startm will set m.spinning in the newly started M.
    //
    // Callers passing a non-nil P must call from a non-preemptible context. See
    // comment on acquirem below.
    //
    // Argument lockheld indicates whether the caller already acquired the
    // scheduler lock. Callers holding the lock when making the call must pass
    // true. The lock might be temporarily dropped, but will be reacquired before
    // returning.
    //
    // Must not have write barriers because this may be called without a P.
    //
    //go:nowritebarrierrec
    void startm(struct p* pp, bool spinning, bool lockheld)
    {
        auto mp = acquirem();
        if(! lockheld)
        {
            lock(& sched.lock);
        }
        if(pp == nullptr)
        {
            if(spinning)
            {
                go_throw("startm: P required for spinning=true"s);
            }
            std::tie(pp, gocpp_id_3) = pidleget(0);
            if(pp == nullptr)
            {
                if(! lockheld)
                {
                    unlock(& sched.lock);
                }
                releasem(mp);
                return;
            }
        }
        auto nmp = mget();
        if(nmp == nullptr)
        {
            auto id = mReserveID();
            unlock(& sched.lock);
            std::function<void ()> fn = {};
            if(spinning)
            {
                fn = mspinning;
            }
            newm(fn, pp, id);
            if(lockheld)
            {
                lock(& sched.lock);
            }
            releasem(mp);
            return;
        }
        if(! lockheld)
        {
            unlock(& sched.lock);
        }
        if(nmp->spinning)
        {
            go_throw("startm: m is spinning"s);
        }
        if(nmp->nextp != 0)
        {
            go_throw("startm: m has p"s);
        }
        if(spinning && ! runqempty(pp))
        {
            go_throw("startm: p has runnable gs"s);
        }
        nmp->spinning = spinning;
        rec::set(gocpp::recv(nmp->nextp), pp);
        notewakeup(& nmp->park);
        releasem(mp);
    }

    // Hands off P from syscall or locked M.
    // Always runs without a P, so write barriers are not allowed.
    //
    //go:nowritebarrierrec
    void handoffp(struct p* pp)
    {
        if(! runqempty(pp) || sched.runqsize != 0)
        {
            startm(pp, false, false);
            return;
        }
        if((traceEnabled() || traceShuttingDown()) && traceReaderAvailable() != nullptr)
        {
            startm(pp, false, false);
            return;
        }
        if(gcBlackenEnabled != 0 && gcMarkWorkAvailable(pp))
        {
            startm(pp, false, false);
            return;
        }
        if(rec::Load(gocpp::recv(sched.nmspinning)) + rec::Load(gocpp::recv(sched.npidle)) == 0 && rec::CompareAndSwap(gocpp::recv(sched.nmspinning), 0, 1))
        {
            rec::Store(gocpp::recv(sched.needspinning), 0);
            startm(pp, true, false);
            return;
        }
        lock(& sched.lock);
        if(rec::Load(gocpp::recv(sched.gcwaiting)))
        {
            pp->status = _Pgcstop;
            sched.stopwait--;
            if(sched.stopwait == 0)
            {
                notewakeup(& sched.stopnote);
            }
            unlock(& sched.lock);
            return;
        }
        if(pp->runSafePointFn != 0 && atomic::Cas(& pp->runSafePointFn, 1, 0))
        {
            rec::safePointFn(gocpp::recv(sched), pp);
            sched.safePointWait--;
            if(sched.safePointWait == 0)
            {
                notewakeup(& sched.safePointNote);
            }
        }
        if(sched.runqsize != 0)
        {
            unlock(& sched.lock);
            startm(pp, false, false);
            return;
        }
        if(rec::Load(gocpp::recv(sched.npidle)) == gomaxprocs - 1 && rec::Load(gocpp::recv(sched.lastpoll)) != 0)
        {
            unlock(& sched.lock);
            startm(pp, false, false);
            return;
        }
        auto when = nobarrierWakeTime(pp);
        pidleput(pp, 0);
        unlock(& sched.lock);
        if(when != 0)
        {
            wakeNetPoller(when);
        }
    }

    // Tries to add one more P to execute G's.
    // Called when a G is made runnable (newproc, ready).
    // Must be called with a P.
    void wakep()
    {
        if(rec::Load(gocpp::recv(sched.nmspinning)) != 0 || ! rec::CompareAndSwap(gocpp::recv(sched.nmspinning), 0, 1))
        {
            return;
        }
        auto mp = acquirem();
        p* pp = {};
        lock(& sched.lock);
        std::tie(pp, gocpp_id_4) = pidlegetSpinning(0);
        if(pp == nullptr)
        {
            if(rec::Add(gocpp::recv(sched.nmspinning), - 1) < 0)
            {
                go_throw("wakep: negative nmspinning"s);
            }
            unlock(& sched.lock);
            releasem(mp);
            return;
        }
        unlock(& sched.lock);
        startm(pp, true, false);
        releasem(mp);
    }

    // Stops execution of the current m that is locked to a g until the g is runnable again.
    // Returns with acquired P.
    void stoplockedm()
    {
        auto gp = getg();
        if(gp->m->lockedg == 0 || rec::ptr(gocpp::recv(rec::ptr(gocpp::recv(gp->m->lockedg))->lockedm)) != gp->m)
        {
            go_throw("stoplockedm: inconsistent locking"s);
        }
        if(gp->m->p != 0)
        {
            auto pp = releasep();
            handoffp(pp);
        }
        incidlelocked(1);
        mPark();
        auto status = readgstatus(rec::ptr(gocpp::recv(gp->m->lockedg)));
        if(status &^ _Gscan != _Grunnable)
        {
            print("runtime:stoplockedm: lockedg (atomicstatus="s, status, ") is not Grunnable or Gscanrunnable\n"s);
            dumpgstatus(rec::ptr(gocpp::recv(gp->m->lockedg)));
            go_throw("stoplockedm: not runnable"s);
        }
        acquirep(rec::ptr(gocpp::recv(gp->m->nextp)));
        gp->m->nextp = 0;
    }

    // Schedules the locked m to run the locked gp.
    // May run during STW, so write barriers are not allowed.
    //
    //go:nowritebarrierrec
    void startlockedm(struct g* gp)
    {
        auto mp = rec::ptr(gocpp::recv(gp->lockedm));
        if(mp == getg()->m)
        {
            go_throw("startlockedm: locked to me"s);
        }
        if(mp->nextp != 0)
        {
            go_throw("startlockedm: m has p"s);
        }
        incidlelocked(- 1);
        auto pp = releasep();
        rec::set(gocpp::recv(mp->nextp), pp);
        notewakeup(& mp->park);
        stopm();
    }

    // Stops the current m for stopTheWorld.
    // Returns when the world is restarted.
    void gcstopm()
    {
        auto gp = getg();
        if(! rec::Load(gocpp::recv(sched.gcwaiting)))
        {
            go_throw("gcstopm: not waiting for gc"s);
        }
        if(gp->m->spinning)
        {
            gp->m->spinning = false;
            if(rec::Add(gocpp::recv(sched.nmspinning), - 1) < 0)
            {
                go_throw("gcstopm: negative nmspinning"s);
            }
        }
        auto pp = releasep();
        lock(& sched.lock);
        pp->status = _Pgcstop;
        sched.stopwait--;
        if(sched.stopwait == 0)
        {
            notewakeup(& sched.stopnote);
        }
        unlock(& sched.lock);
        stopm();
    }

    // Schedules gp to run on the current M.
    // If inheritTime is true, gp inherits the remaining time in the
    // current time slice. Otherwise, it starts a new time slice.
    // Never returns.
    //
    // Write barriers are allowed because this is called immediately after
    // acquiring a P in several places.
    //
    //go:yeswritebarrierrec
    void execute(struct g* gp, bool inheritTime)
    {
        auto mp = getg()->m;
        if(goroutineProfile.active)
        {
            tryRecordGoroutineProfile(gp, osyield);
        }
        mp->curg = gp;
        gp->m = mp;
        casgstatus(gp, _Grunnable, _Grunning);
        gp->waitsince = 0;
        gp->preempt = false;
        gp->stackguard0 = gp->stack.lo + stackGuard;
        if(! inheritTime)
        {
            rec::ptr(gocpp::recv(mp->p))->schedtick++;
        }
        auto hz = sched.profilehz;
        if(mp->profilehz != hz)
        {
            setThreadCPUProfiler(hz);
        }
        auto trace = traceAcquire();
        if(rec::ok(gocpp::recv(trace)))
        {
            if(! goexperiment::ExecTracer2 && gp->syscallsp != 0)
            {
                rec::GoSysExit(gocpp::recv(trace), true);
            }
            rec::GoStart(gocpp::recv(trace));
            traceRelease(trace);
        }
        gogo(& gp->sched);
    }

    // Finds a runnable goroutine to execute.
    // Tries to steal from other P's, get g from local or global queue, poll network.
    // tryWakeP indicates that the returned goroutine is not normal (GC worker, trace
    // reader) so the caller should try to wake a P.
    std::tuple<struct g*, bool, bool> findRunnable()
    {
        struct g* gp;
        bool inheritTime;
        bool tryWakeP;
        auto mp = getg()->m;
        top:
        auto pp = rec::ptr(gocpp::recv(mp->p));
        if(rec::Load(gocpp::recv(sched.gcwaiting)))
        {
            gcstopm();
            goto top;
        }
        if(pp->runSafePointFn != 0)
        {
            runSafePointFn();
        }
        auto [now, pollUntil, gocpp_id_6] = checkTimers(pp, 0);
        if(traceEnabled() || traceShuttingDown())
        {
            auto gp = traceReader();
            if(gp != nullptr)
            {
                auto trace = traceAcquire();
                casgstatus(gp, _Gwaiting, _Grunnable);
                if(rec::ok(gocpp::recv(trace)))
                {
                    rec::GoUnpark(gocpp::recv(trace), gp, 0);
                    traceRelease(trace);
                }
                return {gp, false, true};
            }
        }
        if(gcBlackenEnabled != 0)
        {
            auto [gp, tnow] = rec::findRunnableGCWorker(gocpp::recv(gcController), pp, now);
            if(gp != nullptr)
            {
                return {gp, false, true};
            }
            now = tnow;
        }
        if(pp->schedtick % 61 == 0 && sched.runqsize > 0)
        {
            lock(& sched.lock);
            auto gp = globrunqget(pp, 1);
            unlock(& sched.lock);
            if(gp != nullptr)
            {
                return {gp, false, false};
            }
        }
        if(rec::Load(gocpp::recv(fingStatus)) & (fingWait | fingWake) == fingWait | fingWake)
        {
            if(auto gp = wakefing(); gp != nullptr)
            {
                ready(gp, 0, true);
            }
        }
        if(*cgo_yield != nullptr)
        {
            asmcgocall(*cgo_yield, nullptr);
        }
        if(auto [gp, inheritTime] = runqget(pp); gp != nullptr)
        {
            return {gp, inheritTime, false};
        }
        if(sched.runqsize != 0)
        {
            lock(& sched.lock);
            auto gp = globrunqget(pp, 0);
            unlock(& sched.lock);
            if(gp != nullptr)
            {
                return {gp, false, false};
            }
        }
        if(netpollinited() && netpollAnyWaiters() && rec::Load(gocpp::recv(sched.lastpoll)) != 0)
        {
            if(auto [list, delta] = netpoll(0); ! rec::empty(gocpp::recv(list)))
            {
                auto gp = rec::pop(gocpp::recv(list));
                injectglist(& list);
                netpollAdjustWaiters(delta);
                auto trace = traceAcquire();
                casgstatus(gp, _Gwaiting, _Grunnable);
                if(rec::ok(gocpp::recv(trace)))
                {
                    rec::GoUnpark(gocpp::recv(trace), gp, 0);
                    traceRelease(trace);
                }
                return {gp, false, false};
            }
        }
        if(mp->spinning || 2 * rec::Load(gocpp::recv(sched.nmspinning)) < gomaxprocs - rec::Load(gocpp::recv(sched.npidle)))
        {
            if(! mp->spinning)
            {
                rec::becomeSpinning(gocpp::recv(mp));
            }
            auto [gp, inheritTime, tnow, w, newWork] = stealWork(now);
            if(gp != nullptr)
            {
                return {gp, inheritTime, false};
            }
            if(newWork)
            {
                goto top;
            }
            now = tnow;
            if(w != 0 && (pollUntil == 0 || w < pollUntil))
            {
                pollUntil = w;
            }
        }
        if(gcBlackenEnabled != 0 && gcMarkWorkAvailable(pp) && rec::addIdleMarkWorker(gocpp::recv(gcController)))
        {
            auto node = (gcBgMarkWorkerNode*)(rec::pop(gocpp::recv(gcBgMarkWorkerPool)));
            if(node != nullptr)
            {
                pp->gcMarkWorkerMode = gcMarkWorkerIdleMode;
                auto gp = rec::ptr(gocpp::recv(node->gp));
                auto trace = traceAcquire();
                casgstatus(gp, _Gwaiting, _Grunnable);
                if(rec::ok(gocpp::recv(trace)))
                {
                    rec::GoUnpark(gocpp::recv(trace), gp, 0);
                    traceRelease(trace);
                }
                return {gp, false, false};
            }
            rec::removeIdleMarkWorker(gocpp::recv(gcController));
        }
        auto [gp, otherReady] = beforeIdle(now, pollUntil);
        if(gp != nullptr)
        {
            auto trace = traceAcquire();
            casgstatus(gp, _Gwaiting, _Grunnable);
            if(rec::ok(gocpp::recv(trace)))
            {
                rec::GoUnpark(gocpp::recv(trace), gp, 0);
                traceRelease(trace);
            }
            return {gp, false, false};
        }
        if(otherReady)
        {
            goto top;
        }
        auto allpSnapshot = allp;
        auto idlepMaskSnapshot = idlepMask;
        auto timerpMaskSnapshot = timerpMask;
        lock(& sched.lock);
        if(rec::Load(gocpp::recv(sched.gcwaiting)) || pp->runSafePointFn != 0)
        {
            unlock(& sched.lock);
            goto top;
        }
        if(sched.runqsize != 0)
        {
            auto gp = globrunqget(pp, 0);
            unlock(& sched.lock);
            return {gp, false, false};
        }
        if(! mp->spinning && rec::Load(gocpp::recv(sched.needspinning)) == 1)
        {
            rec::becomeSpinning(gocpp::recv(mp));
            unlock(& sched.lock);
            goto top;
        }
        if(releasep() != pp)
        {
            go_throw("findrunnable: wrong p"s);
        }
        now = pidleput(pp, now);
        unlock(& sched.lock);
        auto wasSpinning = mp->spinning;
        if(mp->spinning)
        {
            mp->spinning = false;
            if(rec::Add(gocpp::recv(sched.nmspinning), - 1) < 0)
            {
                go_throw("findrunnable: negative nmspinning"s);
            }
            lock(& sched.lock);
            if(sched.runqsize != 0)
            {
                auto [pp, gocpp_id_8] = pidlegetSpinning(0);
                if(pp != nullptr)
                {
                    auto gp = globrunqget(pp, 0);
                    if(gp == nullptr)
                    {
                        go_throw("global runq empty with non-zero runqsize"s);
                    }
                    unlock(& sched.lock);
                    acquirep(pp);
                    rec::becomeSpinning(gocpp::recv(mp));
                    return {gp, false, false};
                }
            }
            unlock(& sched.lock);
            auto pp = checkRunqsNoP(allpSnapshot, idlepMaskSnapshot);
            if(pp != nullptr)
            {
                acquirep(pp);
                rec::becomeSpinning(gocpp::recv(mp));
                goto top;
            }
            g* gp;
            std::tie(pp, gp) = checkIdleGCNoP();
            if(pp != nullptr)
            {
                acquirep(pp);
                rec::becomeSpinning(gocpp::recv(mp));
                pp->gcMarkWorkerMode = gcMarkWorkerIdleMode;
                auto trace = traceAcquire();
                casgstatus(gp, _Gwaiting, _Grunnable);
                if(rec::ok(gocpp::recv(trace)))
                {
                    rec::GoUnpark(gocpp::recv(trace), gp, 0);
                    traceRelease(trace);
                }
                return {gp, false, false};
            }
            pollUntil = checkTimersNoP(allpSnapshot, timerpMaskSnapshot, pollUntil);
        }
        if(netpollinited() && (netpollAnyWaiters() || pollUntil != 0) && rec::Swap(gocpp::recv(sched.lastpoll), 0) != 0)
        {
            rec::Store(gocpp::recv(sched.pollUntil), pollUntil);
            if(mp->p != 0)
            {
                go_throw("findrunnable: netpoll with p"s);
            }
            if(mp->spinning)
            {
                go_throw("findrunnable: netpoll with spinning"s);
            }
            auto delay = int64_t(- 1);
            if(pollUntil != 0)
            {
                if(now == 0)
                {
                    now = nanotime();
                }
                delay = pollUntil - now;
                if(delay < 0)
                {
                    delay = 0;
                }
            }
            if(faketime != 0)
            {
                delay = 0;
            }
            auto [list, delta] = netpoll(delay);
            now = nanotime();
            rec::Store(gocpp::recv(sched.pollUntil), 0);
            rec::Store(gocpp::recv(sched.lastpoll), now);
            if(faketime != 0 && rec::empty(gocpp::recv(list)))
            {
                stopm();
                goto top;
            }
            lock(& sched.lock);
            auto [pp, gocpp_id_10] = pidleget(now);
            unlock(& sched.lock);
            if(pp == nullptr)
            {
                injectglist(& list);
                netpollAdjustWaiters(delta);
            }
            else
            {
                acquirep(pp);
                if(! rec::empty(gocpp::recv(list)))
                {
                    auto gp = rec::pop(gocpp::recv(list));
                    injectglist(& list);
                    netpollAdjustWaiters(delta);
                    auto trace = traceAcquire();
                    casgstatus(gp, _Gwaiting, _Grunnable);
                    if(rec::ok(gocpp::recv(trace)))
                    {
                        rec::GoUnpark(gocpp::recv(trace), gp, 0);
                        traceRelease(trace);
                    }
                    return {gp, false, false};
                }
                if(wasSpinning)
                {
                    rec::becomeSpinning(gocpp::recv(mp));
                }
                goto top;
            }
        }
        else
        if(pollUntil != 0 && netpollinited())
        {
            auto pollerPollUntil = rec::Load(gocpp::recv(sched.pollUntil));
            if(pollerPollUntil == 0 || pollerPollUntil > pollUntil)
            {
                netpollBreak();
            }
        }
        stopm();
        goto top;
    }

    // pollWork reports whether there is non-background work this P could
    // be doing. This is a fairly lightweight check to be used for
    // background work loops, like idle GC. It checks a subset of the
    // conditions checked by the actual scheduler.
    bool pollWork()
    {
        if(sched.runqsize != 0)
        {
            return true;
        }
        auto p = rec::ptr(gocpp::recv(getg()->m->p));
        if(! runqempty(p))
        {
            return true;
        }
        if(netpollinited() && netpollAnyWaiters() && rec::Load(gocpp::recv(sched.lastpoll)) != 0)
        {
            if(auto [list, delta] = netpoll(0); ! rec::empty(gocpp::recv(list)))
            {
                injectglist(& list);
                netpollAdjustWaiters(delta);
                return true;
            }
        }
        return false;
    }

    // stealWork attempts to steal a runnable goroutine or timer from any P.
    //
    // If newWork is true, new work may have been readied.
    //
    // If now is not 0 it is the current time. stealWork returns the passed time or
    // the current time if now was passed as 0.
    std::tuple<struct g*, bool, int64_t, int64_t, bool> stealWork(int64_t now)
    {
        struct g* gp;
        bool inheritTime;
        int64_t rnow;
        int64_t pollUntil;
        bool newWork;
        auto pp = rec::ptr(gocpp::recv(getg()->m->p));
        auto ranTimer = false;
        auto stealTries = 4;
        for(auto i = 0; i < stealTries; i++)
        {
            auto stealTimersOrRunNextG = i == stealTries - 1;
            for(auto go_enum = rec::start(gocpp::recv(stealOrder), cheaprand()); ! rec::done(gocpp::recv(go_enum)); rec::next(gocpp::recv(go_enum)))
            {
                if(rec::Load(gocpp::recv(sched.gcwaiting)))
                {
                    return {nullptr, false, now, pollUntil, true};
                }
                auto p2 = allp[rec::position(gocpp::recv(go_enum))];
                if(pp == p2)
                {
                    continue;
                }
                if(stealTimersOrRunNextG && rec::read(gocpp::recv(timerpMask), rec::position(gocpp::recv(go_enum))))
                {
                    auto [tnow, w, ran] = checkTimers(p2, now);
                    now = tnow;
                    if(w != 0 && (pollUntil == 0 || w < pollUntil))
                    {
                        pollUntil = w;
                    }
                    if(ran)
                    {
                        if(auto [gp, inheritTime] = runqget(pp); gp != nullptr)
                        {
                            return {gp, inheritTime, now, pollUntil, ranTimer};
                        }
                        ranTimer = true;
                    }
                }
                if(! rec::read(gocpp::recv(idlepMask), rec::position(gocpp::recv(go_enum))))
                {
                    if(auto gp = runqsteal(pp, p2, stealTimersOrRunNextG); gp != nullptr)
                    {
                        return {gp, false, now, pollUntil, ranTimer};
                    }
                }
            }
        }
        return {nullptr, false, now, pollUntil, ranTimer};
    }

    // Check all Ps for a runnable G to steal.
    //
    // On entry we have no P. If a G is available to steal and a P is available,
    // the P is returned which the caller should acquire and attempt to steal the
    // work to.
    struct p* checkRunqsNoP(gocpp::slice<p*> allpSnapshot, pMask idlepMaskSnapshot)
    {
        for(auto [id, p2] : allpSnapshot)
        {
            if(! rec::read(gocpp::recv(idlepMaskSnapshot), uint32_t(id)) && ! runqempty(p2))
            {
                lock(& sched.lock);
                auto [pp, gocpp_id_12] = pidlegetSpinning(0);
                if(pp == nullptr)
                {
                    unlock(& sched.lock);
                    return nullptr;
                }
                unlock(& sched.lock);
                return pp;
            }
        }
        return nullptr;
    }

    // Check all Ps for a timer expiring sooner than pollUntil.
    //
    // Returns updated pollUntil value.
    int64_t checkTimersNoP(gocpp::slice<p*> allpSnapshot, pMask timerpMaskSnapshot, int64_t pollUntil)
    {
        for(auto [id, p2] : allpSnapshot)
        {
            if(rec::read(gocpp::recv(timerpMaskSnapshot), uint32_t(id)))
            {
                auto w = nobarrierWakeTime(p2);
                if(w != 0 && (pollUntil == 0 || w < pollUntil))
                {
                    pollUntil = w;
                }
            }
        }
        return pollUntil;
    }

    // Check for idle-priority GC, without a P on entry.
    //
    // If some GC work, a P, and a worker G are all available, the P and G will be
    // returned. The returned P has not been wired yet.
    std::tuple<struct p*, struct g*> checkIdleGCNoP()
    {
        if(atomic::Load(& gcBlackenEnabled) == 0 || ! rec::needIdleMarkWorker(gocpp::recv(gcController)))
        {
            return {nullptr, nullptr};
        }
        if(! gcMarkWorkAvailable(nullptr))
        {
            return {nullptr, nullptr};
        }
        lock(& sched.lock);
        auto [pp, now] = pidlegetSpinning(0);
        if(pp == nullptr)
        {
            unlock(& sched.lock);
            return {nullptr, nullptr};
        }
        if(gcBlackenEnabled == 0 || ! rec::addIdleMarkWorker(gocpp::recv(gcController)))
        {
            pidleput(pp, now);
            unlock(& sched.lock);
            return {nullptr, nullptr};
        }
        auto node = (gcBgMarkWorkerNode*)(rec::pop(gocpp::recv(gcBgMarkWorkerPool)));
        if(node == nullptr)
        {
            pidleput(pp, now);
            unlock(& sched.lock);
            rec::removeIdleMarkWorker(gocpp::recv(gcController));
            return {nullptr, nullptr};
        }
        unlock(& sched.lock);
        return {pp, rec::ptr(gocpp::recv(node->gp))};
    }

    // wakeNetPoller wakes up the thread sleeping in the network poller if it isn't
    // going to wake up before the when argument; or it wakes an idle P to service
    // timers and the network poller if there isn't one already.
    void wakeNetPoller(int64_t when)
    {
        if(rec::Load(gocpp::recv(sched.lastpoll)) == 0)
        {
            auto pollerPollUntil = rec::Load(gocpp::recv(sched.pollUntil));
            if(pollerPollUntil == 0 || pollerPollUntil > when)
            {
                netpollBreak();
            }
        }
        else
        {
            if(GOOS != "plan9"s)
            {
                wakep();
            }
        }
    }

    void resetspinning()
    {
        auto gp = getg();
        if(! gp->m->spinning)
        {
            go_throw("resetspinning: not a spinning m"s);
        }
        gp->m->spinning = false;
        auto nmspinning = rec::Add(gocpp::recv(sched.nmspinning), - 1);
        if(nmspinning < 0)
        {
            go_throw("findrunnable: negative nmspinning"s);
        }
        wakep();
    }

    // injectglist adds each runnable G on the list to some run queue,
    // and clears glist. If there is no current P, they are added to the
    // global queue, and up to npidle M's are started to run them.
    // Otherwise, for each idle P, this adds a G to the global queue
    // and starts an M. Any remaining G's are added to the current P's
    // local run queue.
    // This may temporarily acquire sched.lock.
    // Can run concurrently with GC.
    void injectglist(struct gList* glist)
    {
        if(rec::empty(gocpp::recv(glist)))
        {
            return;
        }
        auto trace = traceAcquire();
        if(rec::ok(gocpp::recv(trace)))
        {
            for(auto gp = rec::ptr(gocpp::recv(glist->head)); gp != nullptr; gp = rec::ptr(gocpp::recv(gp->schedlink)))
            {
                rec::GoUnpark(gocpp::recv(trace), gp, 0);
            }
            traceRelease(trace);
        }
        auto head = rec::ptr(gocpp::recv(glist->head));
        g* tail = {};
        auto qsize = 0;
        for(auto gp = head; gp != nullptr; gp = rec::ptr(gocpp::recv(gp->schedlink)))
        {
            tail = gp;
            qsize++;
            casgstatus(gp, _Gwaiting, _Grunnable);
        }
        // Turn the gList into a gQueue.
        gQueue q = {};
        rec::set(gocpp::recv(q.head), head);
        rec::set(gocpp::recv(q.tail), tail);
        *glist = gList {};
        auto startIdle = [=](int n) mutable -> void
        {
            for(auto i = 0; i < n; i++)
            {
                auto mp = acquirem();
                lock(& sched.lock);
                auto [pp, gocpp_id_14] = pidlegetSpinning(0);
                if(pp == nullptr)
                {
                    unlock(& sched.lock);
                    releasem(mp);
                    break;
                }
                startm(pp, false, true);
                unlock(& sched.lock);
                releasem(mp);
            }
        };
        auto pp = rec::ptr(gocpp::recv(getg()->m->p));
        if(pp == nullptr)
        {
            lock(& sched.lock);
            globrunqputbatch(& q, int32_t(qsize));
            unlock(& sched.lock);
            startIdle(qsize);
            return;
        }
        auto npidle = int(rec::Load(gocpp::recv(sched.npidle)));
        gQueue globq = {};
        int n = {};
        for(n = 0; n < npidle && ! rec::empty(gocpp::recv(q)); n++)
        {
            auto g = rec::pop(gocpp::recv(q));
            rec::pushBack(gocpp::recv(globq), g);
        }
        if(n > 0)
        {
            lock(& sched.lock);
            globrunqputbatch(& globq, int32_t(n));
            unlock(& sched.lock);
            startIdle(n);
            qsize -= n;
        }
        if(! rec::empty(gocpp::recv(q)))
        {
            runqputbatch(pp, & q, qsize);
        }
    }

    // One round of scheduler: find a runnable goroutine and execute it.
    // Never returns.
    void schedule()
    {
        auto mp = getg()->m;
        if(mp->locks != 0)
        {
            go_throw("schedule: holding locks"s);
        }
        if(mp->lockedg != 0)
        {
            stoplockedm();
            execute(rec::ptr(gocpp::recv(mp->lockedg)), false);
        }
        if(mp->incgo)
        {
            go_throw("schedule: in cgo"s);
        }
        top:
        auto pp = rec::ptr(gocpp::recv(mp->p));
        pp->preempt = false;
        if(mp->spinning && (pp->runnext != 0 || pp->runqhead != pp->runqtail))
        {
            go_throw("schedule: spinning with local work"s);
        }
        auto [gp, inheritTime, tryWakeP] = findRunnable();
        if(debug.dontfreezetheworld > 0 && rec::Load(gocpp::recv(freezing)))
        {
            lock(& deadlock);
            lock(& deadlock);
        }
        if(mp->spinning)
        {
            resetspinning();
        }
        if(sched.disable.user && ! schedEnabled(gp))
        {
            lock(& sched.lock);
            if(schedEnabled(gp))
            {
                unlock(& sched.lock);
            }
            else
            {
                rec::pushBack(gocpp::recv(sched.disable.runnable), gp);
                sched.disable.n++;
                unlock(& sched.lock);
                goto top;
            }
        }
        if(tryWakeP)
        {
            wakep();
        }
        if(gp->lockedm != 0)
        {
            startlockedm(gp);
            goto top;
        }
        execute(gp, inheritTime);
    }

    // dropg removes the association between m and the current goroutine m->curg (gp for short).
    // Typically a caller sets gp's status away from Grunning and then
    // immediately calls dropg to finish the job. The caller is also responsible
    // for arranging that gp will be restarted using ready at an
    // appropriate time. After calling dropg and arranging for gp to be
    // readied later, the caller can do other work but eventually should
    // call schedule to restart the scheduling of goroutines on this m.
    void dropg()
    {
        auto gp = getg();
        setMNoWB(& gp->m->curg->m, nullptr);
        setGNoWB(& gp->m->curg, nullptr);
    }

    // checkTimers runs any timers for the P that are ready.
    // If now is not 0 it is the current time.
    // It returns the passed time or the current time if now was passed as 0.
    // and the time when the next timer should run or 0 if there is no next timer,
    // and reports whether it ran any timers.
    // If the time when the next timer should run is not 0,
    // it is always larger than the returned time.
    // We pass now in and out to avoid extra calls of nanotime.
    //
    //go:yeswritebarrierrec
    std::tuple<int64_t, int64_t, bool> checkTimers(struct p* pp, int64_t now)
    {
        int64_t rnow;
        int64_t pollUntil;
        bool ran;
        auto next = rec::Load(gocpp::recv(pp->timer0When));
        auto nextAdj = rec::Load(gocpp::recv(pp->timerModifiedEarliest));
        if(next == 0 || (nextAdj != 0 && nextAdj < next))
        {
            next = nextAdj;
        }
        if(next == 0)
        {
            return {now, 0, false};
        }
        if(now == 0)
        {
            now = nanotime();
        }
        if(now < next)
        {
            if(pp != rec::ptr(gocpp::recv(getg()->m->p)) || int(rec::Load(gocpp::recv(pp->deletedTimers))) <= int(rec::Load(gocpp::recv(pp->numTimers)) / 4))
            {
                return {now, next, false};
            }
        }
        lock(& pp->timersLock);
        if(len(pp->timers) > 0)
        {
            adjusttimers(pp, now);
            for(; len(pp->timers) > 0; )
            {
                if(auto tw = runtimer(pp, now); tw != 0)
                {
                    if(tw > 0)
                    {
                        pollUntil = tw;
                    }
                    break;
                }
                ran = true;
            }
        }
        if(pp == rec::ptr(gocpp::recv(getg()->m->p)) && int(rec::Load(gocpp::recv(pp->deletedTimers))) > len(pp->timers) / 4)
        {
            clearDeletedTimers(pp);
        }
        unlock(& pp->timersLock);
        return {now, pollUntil, ran};
    }

    bool parkunlock_c(struct g* gp, unsafe::Pointer lock)
    {
        unlock((mutex*)(lock));
        return true;
    }

    // park continuation on g0.
    void park_m(struct g* gp)
    {
        auto mp = getg()->m;
        auto trace = traceAcquire();
        casgstatus(gp, _Grunning, _Gwaiting);
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::GoPark(gocpp::recv(trace), mp->waitTraceBlockReason, mp->waitTraceSkip);
            traceRelease(trace);
        }
        dropg();
        if(auto fn = mp->waitunlockf; fn != nullptr)
        {
            auto ok = fn(gp, mp->waitlock);
            mp->waitunlockf = nullptr;
            mp->waitlock = nullptr;
            if(! ok)
            {
                auto trace = traceAcquire();
                casgstatus(gp, _Gwaiting, _Grunnable);
                if(rec::ok(gocpp::recv(trace)))
                {
                    rec::GoUnpark(gocpp::recv(trace), gp, 2);
                    traceRelease(trace);
                }
                execute(gp, true);
            }
        }
        schedule();
    }

    void goschedImpl(struct g* gp, bool preempted)
    {
        auto trace = traceAcquire();
        auto status = readgstatus(gp);
        if(status &^ _Gscan != _Grunning)
        {
            dumpgstatus(gp);
            go_throw("bad g status"s);
        }
        casgstatus(gp, _Grunning, _Grunnable);
        if(rec::ok(gocpp::recv(trace)))
        {
            if(preempted)
            {
                rec::GoPreempt(gocpp::recv(trace));
            }
            else
            {
                rec::GoSched(gocpp::recv(trace));
            }
            traceRelease(trace);
        }
        dropg();
        lock(& sched.lock);
        globrunqput(gp);
        unlock(& sched.lock);
        if(mainStarted)
        {
            wakep();
        }
        schedule();
    }

    // Gosched continuation on g0.
    void gosched_m(struct g* gp)
    {
        goschedImpl(gp, false);
    }

    // goschedguarded is a forbidden-states-avoided version of gosched_m.
    void goschedguarded_m(struct g* gp)
    {
        if(! canPreemptM(gp->m))
        {
            gogo(& gp->sched);
        }
        goschedImpl(gp, false);
    }

    void gopreempt_m(struct g* gp)
    {
        goschedImpl(gp, true);
    }

    // preemptPark parks gp and puts it in _Gpreempted.
    //
    //go:systemstack
    void preemptPark(struct g* gp)
    {
        auto status = readgstatus(gp);
        if(status &^ _Gscan != _Grunning)
        {
            dumpgstatus(gp);
            go_throw("bad g status"s);
        }
        if(gp->asyncSafePoint)
        {
            auto f = findfunc(gp->sched.pc);
            if(! rec::valid(gocpp::recv(f)))
            {
                go_throw("preempt at unknown pc"s);
            }
            if(f.flag & abi::FuncFlagSPWrite != 0)
            {
                println("runtime: unexpected SPWRITE function"s, funcname(f), "in async preempt"s);
                go_throw("preempt SPWRITE"s);
            }
        }
        casGToPreemptScan(gp, _Grunning, _Gscan | _Gpreempted);
        dropg();
        auto trace = traceAcquire();
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::GoPark(gocpp::recv(trace), traceBlockPreempted, 0);
        }
        casfrom_Gscanstatus(gp, _Gscan | _Gpreempted, _Gpreempted);
        if(rec::ok(gocpp::recv(trace)))
        {
            traceRelease(trace);
        }
        schedule();
    }

    // goyield is like Gosched, but it:
    // - emits a GoPreempt trace event instead of a GoSched trace event
    // - puts the current G on the runq of the current P instead of the globrunq
    void goyield()
    {
        checkTimeouts();
        mcall(goyield_m);
    }

    void goyield_m(struct g* gp)
    {
        auto trace = traceAcquire();
        auto pp = rec::ptr(gocpp::recv(gp->m->p));
        casgstatus(gp, _Grunning, _Grunnable);
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::GoPreempt(gocpp::recv(trace));
            traceRelease(trace);
        }
        dropg();
        runqput(pp, gp, false);
        schedule();
    }

    // Finishes execution of the current goroutine.
    void goexit1()
    {
        if(raceenabled)
        {
            racegoend();
        }
        auto trace = traceAcquire();
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::GoEnd(gocpp::recv(trace));
            traceRelease(trace);
        }
        mcall(goexit0);
    }

    // goexit continuation on g0.
    void goexit0(struct g* gp)
    {
        gdestroy(gp);
        schedule();
    }

    void gdestroy(struct g* gp)
    {
        auto mp = getg()->m;
        auto pp = rec::ptr(gocpp::recv(mp->p));
        casgstatus(gp, _Grunning, _Gdead);
        rec::addScannableStack(gocpp::recv(gcController), pp, - int64_t(gp->stack.hi - gp->stack.lo));
        if(isSystemGoroutine(gp, false))
        {
            rec::Add(gocpp::recv(sched.ngsys), - 1);
        }
        gp->m = nullptr;
        auto locked = gp->lockedm != 0;
        gp->lockedm = 0;
        mp->lockedg = 0;
        gp->preemptStop = false;
        gp->paniconfault = false;
        gp->_defer = nullptr;
        gp->_panic = nullptr;
        gp->writebuf = nullptr;
        gp->waitreason = waitReasonZero;
        gp->param = nullptr;
        gp->labels = nullptr;
        gp->timer = nullptr;
        if(gcBlackenEnabled != 0 && gp->gcAssistBytes > 0)
        {
            auto assistWorkPerByte = rec::Load(gocpp::recv(gcController.assistWorkPerByte));
            auto scanCredit = int64_t(assistWorkPerByte * double(gp->gcAssistBytes));
            rec::Add(gocpp::recv(gcController.bgScanCredit), scanCredit);
            gp->gcAssistBytes = 0;
        }
        dropg();
        if(GOARCH == "wasm"s)
        {
            gfput(pp, gp);
            return;
        }
        if(mp->lockedInt != 0)
        {
            print("invalid m->lockedInt = "s, mp->lockedInt, "\n"s);
            go_throw("internal lockOSThread error"s);
        }
        gfput(pp, gp);
        if(locked)
        {
            if(GOOS != "plan9"s)
            {
                gogo(& mp->g0->sched);
            }
            else
            {
                mp->lockedExt = 0;
            }
        }
    }

    // save updates getg().sched to refer to pc and sp so that a following
    // gogo will restore pc and sp.
    //
    // save must not have write barriers because invoking a write barrier
    // can clobber getg().sched.
    //
    //go:nosplit
    //go:nowritebarrierrec
    void save(uintptr_t pc, uintptr_t sp)
    {
        auto gp = getg();
        if(gp == gp->m->g0 || gp == gp->m->gsignal)
        {
            go_throw("save on system g not allowed"s);
        }
        gp->sched.pc = pc;
        gp->sched.sp = sp;
        gp->sched.lr = 0;
        gp->sched.ret = 0;
        if(gp->sched.ctxt != nullptr)
        {
            badctxt();
        }
    }

    // The goroutine g is about to enter a system call.
    // Record that it's not using the cpu anymore.
    // This is called only from the go syscall library and cgocall,
    // not from the low-level system calls used by the runtime.
    //
    // Entersyscall cannot split the stack: the save must
    // make g->sched refer to the caller's stack segment, because
    // entersyscall is going to return immediately after.
    //
    // Nothing entersyscall calls can split the stack either.
    // We cannot safely move the stack during an active call to syscall,
    // because we do not know which of the uintptr arguments are
    // really pointers (back into the stack).
    // In practice, this means that we make the fast path run through
    // entersyscall doing no-split things, and the slow path has to use systemstack
    // to run bigger things on the system stack.
    //
    // reentersyscall is the entry point used by cgo callbacks, where explicitly
    // saved SP and PC are restored. This is needed when exitsyscall will be called
    // from a function further up in the call stack than the parent, as g->syscallsp
    // must always point to a valid stack frame. entersyscall below is the normal
    // entry point for syscalls, which obtains the SP and PC from the caller.
    //
    // Syscall tracing (old tracer):
    // At the start of a syscall we emit traceGoSysCall to capture the stack trace.
    // If the syscall does not block, that is it, we do not emit any other events.
    // If the syscall blocks (that is, P is retaken), retaker emits traceGoSysBlock;
    // when syscall returns we emit traceGoSysExit and when the goroutine starts running
    // (potentially instantly, if exitsyscallfast returns true) we emit traceGoStart.
    // To ensure that traceGoSysExit is emitted strictly after traceGoSysBlock,
    // we remember current value of syscalltick in m (gp.m.syscalltick = gp.m.p.ptr().syscalltick),
    // whoever emits traceGoSysBlock increments p.syscalltick afterwards;
    // and we wait for the increment before emitting traceGoSysExit.
    // Note that the increment is done even if tracing is not enabled,
    // because tracing can be enabled in the middle of syscall. We don't want the wait to hang.
    //
    //go:nosplit
    void reentersyscall(uintptr_t pc, uintptr_t sp)
    {
        auto trace = traceAcquire();
        auto gp = getg();
        gp->m->locks++;
        gp->stackguard0 = stackPreempt;
        gp->throwsplit = true;
        save(pc, sp);
        gp->syscallsp = sp;
        gp->syscallpc = pc;
        casgstatus(gp, _Grunning, _Gsyscall);
        if(staticLockRanking)
        {
            save(pc, sp);
        }
        if(gp->syscallsp < gp->stack.lo || gp->stack.hi < gp->syscallsp)
        {
            systemstack([=]() mutable -> void
            {
                print("entersyscall inconsistent "s, hex(gp->syscallsp), " ["s, hex(gp->stack.lo), ","s, hex(gp->stack.hi), "]\n"s);
                go_throw("entersyscall"s);
            });
        }
        if(rec::ok(gocpp::recv(trace)))
        {
            systemstack([=]() mutable -> void
            {
                rec::GoSysCall(gocpp::recv(trace));
                traceRelease(trace);
            });
            save(pc, sp);
        }
        if(rec::Load(gocpp::recv(sched.sysmonwait)))
        {
            systemstack(entersyscall_sysmon);
            save(pc, sp);
        }
        if(rec::ptr(gocpp::recv(gp->m->p))->runSafePointFn != 0)
        {
            systemstack(runSafePointFn);
            save(pc, sp);
        }
        gp->m->syscalltick = rec::ptr(gocpp::recv(gp->m->p))->syscalltick;
        auto pp = rec::ptr(gocpp::recv(gp->m->p));
        pp->m = 0;
        rec::set(gocpp::recv(gp->m->oldp), pp);
        gp->m->p = 0;
        atomic::Store(& pp->status, _Psyscall);
        if(rec::Load(gocpp::recv(sched.gcwaiting)))
        {
            systemstack(entersyscall_gcwait);
            save(pc, sp);
        }
        gp->m->locks--;
    }

    // Standard syscall entry used by the go syscall library and normal cgo calls.
    //
    // This is exported via linkname to assembly in the syscall package and x/sys.
    //
    //go:nosplit
    //go:linkname entersyscall
    void entersyscall()
    {
        reentersyscall(getcallerpc(), getcallersp());
    }

    void entersyscall_sysmon()
    {
        lock(& sched.lock);
        if(rec::Load(gocpp::recv(sched.sysmonwait)))
        {
            rec::Store(gocpp::recv(sched.sysmonwait), false);
            notewakeup(& sched.sysmonnote);
        }
        unlock(& sched.lock);
    }

    void entersyscall_gcwait()
    {
        auto gp = getg();
        auto pp = rec::ptr(gocpp::recv(gp->m->oldp));
        lock(& sched.lock);
        auto trace = traceAcquire();
        if(sched.stopwait > 0 && atomic::Cas(& pp->status, _Psyscall, _Pgcstop))
        {
            if(rec::ok(gocpp::recv(trace)))
            {
                if(goexperiment::ExecTracer2)
                {
                    rec::ProcSteal(gocpp::recv(trace), pp, true);
                }
                else
                {
                    rec::GoSysBlock(gocpp::recv(trace), pp);
                    rec::ProcStop(gocpp::recv(trace), pp);
                }
                traceRelease(trace);
            }
            pp->syscalltick++;
            if(sched.stopwait--; sched.stopwait == 0)
            {
                notewakeup(& sched.stopnote);
            }
        }
        else
        if(rec::ok(gocpp::recv(trace)))
        {
            traceRelease(trace);
        }
        unlock(& sched.lock);
    }

    // The same as entersyscall(), but with a hint that the syscall is blocking.
    //
    //go:nosplit
    void entersyscallblock()
    {
        auto gp = getg();
        gp->m->locks++;
        gp->throwsplit = true;
        gp->stackguard0 = stackPreempt;
        gp->m->syscalltick = rec::ptr(gocpp::recv(gp->m->p))->syscalltick;
        rec::ptr(gocpp::recv(gp->m->p))->syscalltick++;
        auto pc = getcallerpc();
        auto sp = getcallersp();
        save(pc, sp);
        gp->syscallsp = gp->sched.sp;
        gp->syscallpc = gp->sched.pc;
        if(gp->syscallsp < gp->stack.lo || gp->stack.hi < gp->syscallsp)
        {
            auto sp1 = sp;
            auto sp2 = gp->sched.sp;
            auto sp3 = gp->syscallsp;
            systemstack([=]() mutable -> void
            {
                print("entersyscallblock inconsistent "s, hex(sp1), " "s, hex(sp2), " "s, hex(sp3), " ["s, hex(gp->stack.lo), ","s, hex(gp->stack.hi), "]\n"s);
                go_throw("entersyscallblock"s);
            });
        }
        casgstatus(gp, _Grunning, _Gsyscall);
        if(gp->syscallsp < gp->stack.lo || gp->stack.hi < gp->syscallsp)
        {
            systemstack([=]() mutable -> void
            {
                print("entersyscallblock inconsistent "s, hex(sp), " "s, hex(gp->sched.sp), " "s, hex(gp->syscallsp), " ["s, hex(gp->stack.lo), ","s, hex(gp->stack.hi), "]\n"s);
                go_throw("entersyscallblock"s);
            });
        }
        systemstack(entersyscallblock_handoff);
        save(getcallerpc(), getcallersp());
        gp->m->locks--;
    }

    void entersyscallblock_handoff()
    {
        auto trace = traceAcquire();
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::GoSysCall(gocpp::recv(trace));
            rec::GoSysBlock(gocpp::recv(trace), rec::ptr(gocpp::recv(getg()->m->p)));
            traceRelease(trace);
        }
        handoffp(releasep());
    }

    // The goroutine g exited its system call.
    // Arrange for it to run on a cpu again.
    // This is called only from the go syscall library, not
    // from the low-level system calls used by the runtime.
    //
    // Write barriers are not allowed because our P may have been stolen.
    //
    // This is exported via linkname to assembly in the syscall package.
    //
    //go:nosplit
    //go:nowritebarrierrec
    //go:linkname exitsyscall
    void exitsyscall()
    {
        auto gp = getg();
        gp->m->locks++;
        if(getcallersp() > gp->syscallsp)
        {
            go_throw("exitsyscall: syscall frame is no longer valid"s);
        }
        gp->waitsince = 0;
        auto oldp = rec::ptr(gocpp::recv(gp->m->oldp));
        gp->m->oldp = 0;
        if(exitsyscallfast(oldp))
        {
            if(goroutineProfile.active)
            {
                systemstack([=]() mutable -> void
                {
                    tryRecordGoroutineProfileWB(gp);
                });
            }
            auto trace = traceAcquire();
            if(rec::ok(gocpp::recv(trace)))
            {
                auto lostP = oldp != rec::ptr(gocpp::recv(gp->m->p)) || gp->m->syscalltick != rec::ptr(gocpp::recv(gp->m->p))->syscalltick;
                systemstack([=]() mutable -> void
                {
                    if(goexperiment::ExecTracer2)
                    {
                        rec::GoSysExit(gocpp::recv(trace), lostP);
                    }
                    if(lostP)
                    {
                        rec::GoStart(gocpp::recv(trace));
                    }
                });
            }
            rec::ptr(gocpp::recv(gp->m->p))->syscalltick++;
            casgstatus(gp, _Gsyscall, _Grunning);
            if(rec::ok(gocpp::recv(trace)))
            {
                traceRelease(trace);
            }
            gp->syscallsp = 0;
            gp->m->locks--;
            if(gp->preempt)
            {
                gp->stackguard0 = stackPreempt;
            }
            else
            {
                gp->stackguard0 = gp->stack.lo + stackGuard;
            }
            gp->throwsplit = false;
            if(sched.disable.user && ! schedEnabled(gp))
            {
                Gosched();
            }
            return;
        }
        if(! goexperiment::ExecTracer2)
        {
            auto trace = traceAcquire();
            if(rec::ok(gocpp::recv(trace)))
            {
                rec::RecordSyscallExitedTime(gocpp::recv(trace), gp, oldp);
                traceRelease(trace);
            }
        }
        gp->m->locks--;
        mcall(exitsyscall0);
        gp->syscallsp = 0;
        rec::ptr(gocpp::recv(gp->m->p))->syscalltick++;
        gp->throwsplit = false;
    }

    //go:nosplit
    bool exitsyscallfast(struct p* oldp)
    {
        auto gp = getg();
        if(sched.stopwait == freezeStopWait)
        {
            return false;
        }
        auto trace = traceAcquire();
        if(oldp != nullptr && oldp->status == _Psyscall && atomic::Cas(& oldp->status, _Psyscall, _Pidle))
        {
            wirep(oldp);
            exitsyscallfast_reacquired(trace);
            if(rec::ok(gocpp::recv(trace)))
            {
                traceRelease(trace);
            }
            return true;
        }
        if(rec::ok(gocpp::recv(trace)))
        {
            traceRelease(trace);
        }
        if(sched.pidle != 0)
        {
            bool ok = {};
            systemstack([=]() mutable -> void
            {
                ok = exitsyscallfast_pidle();
                if(ok && ! goexperiment::ExecTracer2)
                {
                    auto trace = traceAcquire();
                    if(rec::ok(gocpp::recv(trace)))
                    {
                        if(oldp != nullptr)
                        {
                            for(; oldp->syscalltick == gp->m->syscalltick; )
                            {
                                osyield();
                            }
                        }
                        rec::GoSysExit(gocpp::recv(trace), true);
                        traceRelease(trace);
                    }
                }
            });
            if(ok)
            {
                return true;
            }
        }
        return false;
    }

    // exitsyscallfast_reacquired is the exitsyscall path on which this G
    // has successfully reacquired the P it was running on before the
    // syscall.
    //
    //go:nosplit
    void exitsyscallfast_reacquired(struct traceLocker trace)
    {
        auto gp = getg();
        if(gp->m->syscalltick != rec::ptr(gocpp::recv(gp->m->p))->syscalltick)
        {
            if(rec::ok(gocpp::recv(trace)))
            {
                systemstack([=]() mutable -> void
                {
                    if(goexperiment::ExecTracer2)
                    {
                        rec::ProcSteal(gocpp::recv(trace), rec::ptr(gocpp::recv(gp->m->p)), true);
                        rec::ProcStart(gocpp::recv(trace));
                    }
                    else
                    {
                        rec::GoSysBlock(gocpp::recv(trace), rec::ptr(gocpp::recv(gp->m->p)));
                        rec::GoSysExit(gocpp::recv(trace), true);
                    }
                });
            }
            rec::ptr(gocpp::recv(gp->m->p))->syscalltick++;
        }
    }

    bool exitsyscallfast_pidle()
    {
        lock(& sched.lock);
        auto [pp, gocpp_id_16] = pidleget(0);
        if(pp != nullptr && rec::Load(gocpp::recv(sched.sysmonwait)))
        {
            rec::Store(gocpp::recv(sched.sysmonwait), false);
            notewakeup(& sched.sysmonnote);
        }
        unlock(& sched.lock);
        if(pp != nullptr)
        {
            acquirep(pp);
            return true;
        }
        return false;
    }

    // exitsyscall slow path on g0.
    // Failed to acquire P, enqueue gp as runnable.
    //
    // Called via mcall, so gp is the calling g from this M.
    //
    //go:nowritebarrierrec
    void exitsyscall0(struct g* gp)
    {
        traceLocker trace = {};
        if(goexperiment::ExecTracer2)
        {
            traceExitingSyscall();
            trace = traceAcquire();
        }
        casgstatus(gp, _Gsyscall, _Grunnable);
        if(goexperiment::ExecTracer2)
        {
            traceExitedSyscall();
            if(rec::ok(gocpp::recv(trace)))
            {
                rec::GoSysExit(gocpp::recv(trace), true);
                traceRelease(trace);
            }
        }
        dropg();
        lock(& sched.lock);
        p* pp = {};
        if(schedEnabled(gp))
        {
            std::tie(pp, gocpp_id_17) = pidleget(0);
        }
        bool locked = {};
        if(pp == nullptr)
        {
            globrunqput(gp);
            locked = gp->lockedm != 0;
        }
        else
        if(rec::Load(gocpp::recv(sched.sysmonwait)))
        {
            rec::Store(gocpp::recv(sched.sysmonwait), false);
            notewakeup(& sched.sysmonnote);
        }
        unlock(& sched.lock);
        if(pp != nullptr)
        {
            acquirep(pp);
            execute(gp, false);
        }
        if(locked)
        {
            stoplockedm();
            execute(gp, false);
        }
        stopm();
        schedule();
    }

    // Called from syscall package before fork.
    //
    //go:linkname syscall_runtime_BeforeFork syscall.runtime_BeforeFork
    //go:nosplit
    void syscall_runtime_BeforeFork()
    {
        auto gp = getg()->m->curg;
        gp->m->locks++;
        sigsave(& gp->m->sigmask);
        sigblock(false);
        gp->stackguard0 = stackFork;
    }

    // Called from syscall package after fork in parent.
    //
    //go:linkname syscall_runtime_AfterFork syscall.runtime_AfterFork
    //go:nosplit
    void syscall_runtime_AfterFork()
    {
        auto gp = getg()->m->curg;
        gp->stackguard0 = gp->stack.lo + stackGuard;
        msigrestore(gp->m->sigmask);
        gp->m->locks--;
    }

    // inForkedChild is true while manipulating signals in the child process.
    // This is used to avoid calling libc functions in case we are using vfork.
    bool inForkedChild;
    // Called from syscall package after fork in child.
    // It resets non-sigignored signals to the default handler, and
    // restores the signal mask in preparation for the exec.
    //
    // Because this might be called during a vfork, and therefore may be
    // temporarily sharing address space with the parent process, this must
    // not change any global variables or calling into C code that may do so.
    //
    //go:linkname syscall_runtime_AfterForkInChild syscall.runtime_AfterForkInChild
    //go:nosplit
    //go:nowritebarrierrec
    void syscall_runtime_AfterForkInChild()
    {
        inForkedChild = true;
        clearSignalHandlers();
        msigrestore(getg()->m->sigmask);
        inForkedChild = false;
    }

    // pendingPreemptSignals is the number of preemption signals
    // that have been sent but not received. This is only used on Darwin.
    // For #41702.
    atomic::Int32 pendingPreemptSignals;
    // Called from syscall package before Exec.
    //
    //go:linkname syscall_runtime_BeforeExec syscall.runtime_BeforeExec
    void syscall_runtime_BeforeExec()
    {
        rec::lock(gocpp::recv(execLock));
        if(GOOS == "darwin"s || GOOS == "ios"s)
        {
            for(; rec::Load(gocpp::recv(pendingPreemptSignals)) > 0; )
            {
                osyield();
            }
        }
    }

    // Called from syscall package after Exec.
    //
    //go:linkname syscall_runtime_AfterExec syscall.runtime_AfterExec
    void syscall_runtime_AfterExec()
    {
        rec::unlock(gocpp::recv(execLock));
    }

    // Allocate a new g, with a stack big enough for stacksize bytes.
    struct g* malg(int32_t stacksize)
    {
        auto newg = new(g);
        if(stacksize >= 0)
        {
            stacksize = round2(stackSystem + stacksize);
            systemstack([=]() mutable -> void
            {
                newg->stack = stackalloc(uint32_t(stacksize));
            });
            newg->stackguard0 = newg->stack.lo + stackGuard;
            newg->stackguard1 = ~ uintptr_t(0);
            *(uintptr_t*)(unsafe::Pointer(newg->stack.lo)) = 0;
        }
        return newg;
    }

    // Create a new g running fn.
    // Put it on the queue of g's waiting to run.
    // The compiler turns a go statement into a call to this.
    void newproc(struct funcval* fn)
    {
        auto gp = getg();
        auto pc = getcallerpc();
        systemstack([=]() mutable -> void
        {
            auto newg = newproc1(fn, gp, pc);
            auto pp = rec::ptr(gocpp::recv(getg()->m->p));
            runqput(pp, newg, true);
            if(mainStarted)
            {
                wakep();
            }
        });
    }

    // Create a new g in state _Grunnable, starting at fn. callerpc is the
    // address of the go statement that created this. The caller is responsible
    // for adding the new g to the scheduler.
    struct g* newproc1(struct funcval* fn, struct g* callergp, uintptr_t callerpc)
    {
        if(fn == nullptr)
        {
            fatal("go of nil func value"s);
        }
        auto mp = acquirem();
        auto pp = rec::ptr(gocpp::recv(mp->p));
        auto newg = gfget(pp);
        if(newg == nullptr)
        {
            newg = malg(stackMin);
            casgstatus(newg, _Gidle, _Gdead);
            allgadd(newg);
        }
        if(newg->stack.hi == 0)
        {
            go_throw("newproc1: newg missing stack"s);
        }
        if(readgstatus(newg) != _Gdead)
        {
            go_throw("newproc1: new g is not Gdead"s);
        }
        auto totalSize = uintptr_t(4 * goarch::PtrSize + sys::MinFrameSize);
        totalSize = alignUp(totalSize, sys::StackAlign);
        auto sp = newg->stack.hi - totalSize;
        if(usesLR)
        {
            *(uintptr_t*)(unsafe::Pointer(sp)) = 0;
            prepGoExitFrame(sp);
        }
        if(GOARCH == "arm64"s)
        {
            *(uintptr_t*)(unsafe::Pointer(sp - goarch::PtrSize)) = 0;
        }
        memclrNoHeapPointers(unsafe::Pointer(& newg->sched), gocpp::Sizeof<gobuf>());
        newg->sched.sp = sp;
        newg->stktopsp = sp;
        newg->sched.pc = abi::FuncPCABI0(goexit) + sys::PCQuantum;
        newg->sched.g = guintptr(unsafe::Pointer(newg));
        gostartcallfn(& newg->sched, fn);
        newg->parentGoid = callergp->goid;
        newg->gopc = callerpc;
        newg->ancestors = saveAncestors(callergp);
        newg->startpc = fn->fn;
        if(isSystemGoroutine(newg, false))
        {
            rec::Add(gocpp::recv(sched.ngsys), 1);
        }
        else
        {
            if(mp->curg != nullptr)
            {
                newg->labels = mp->curg->labels;
            }
            if(goroutineProfile.active)
            {
                rec::Store(gocpp::recv(newg->goroutineProfiled), goroutineProfileSatisfied);
            }
        }
        newg->trackingSeq = uint8_t(cheaprand());
        if(newg->trackingSeq % gTrackingPeriod == 0)
        {
            newg->tracking = true;
        }
        rec::addScannableStack(gocpp::recv(gcController), pp, int64_t(newg->stack.hi - newg->stack.lo));
        auto trace = traceAcquire();
        casgstatus(newg, _Gdead, _Grunnable);
        if(pp->goidcache == pp->goidcacheend)
        {
            pp->goidcache = rec::Add(gocpp::recv(sched.goidgen), _GoidCacheBatch);
            pp->goidcache -= _GoidCacheBatch - 1;
            pp->goidcacheend = pp->goidcache + _GoidCacheBatch;
        }
        newg->goid = pp->goidcache;
        pp->goidcache++;
        rec::reset(gocpp::recv(newg->trace));
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::GoCreate(gocpp::recv(trace), newg, newg->startpc);
            traceRelease(trace);
        }
        if(raceenabled)
        {
            newg->racectx = racegostart(callerpc);
            newg->raceignore = 0;
            if(newg->labels != nullptr)
            {
                racereleasemergeg(newg, unsafe::Pointer(& labelSync));
            }
        }
        releasem(mp);
        return newg;
    }

    // saveAncestors copies previous ancestors of the given caller g and
    // includes info for the current caller into a new set of tracebacks for
    // a g being created.
    gocpp::slice<ancestorInfo>* saveAncestors(struct g* callergp)
    {
        if(debug.tracebackancestors <= 0 || callergp->goid == 0)
        {
            return nullptr;
        }
        gocpp::slice<ancestorInfo> callerAncestors = {};
        if(callergp->ancestors != nullptr)
        {
            callerAncestors = *callergp->ancestors;
        }
        auto n = int32_t(len(callerAncestors)) + 1;
        if(n > debug.tracebackancestors)
        {
            n = debug.tracebackancestors;
        }
        auto ancestors = gocpp::make(gocpp::Tag<gocpp::slice<ancestorInfo>>(), n);
        copy(ancestors.make_slice(1), callerAncestors);
        gocpp::array<uintptr_t, tracebackInnerFrames> pcs = {};
        auto npcs = gcallers(callergp, 0, pcs.make_slice(0));
        auto ipcs = gocpp::make(gocpp::Tag<gocpp::slice<uintptr_t>>(), npcs);
        copy(ipcs, pcs.make_slice(0));
        ancestors[0] = gocpp::Init<ancestorInfo>([=](auto& x) {
            x.pcs = ipcs;
            x.goid = callergp->goid;
            x.gopc = callergp->gopc;
        });
        auto ancestorsp = new(gocpp::Tag<gocpp::slice<ancestorInfo>>());
        *ancestorsp = ancestors;
        return ancestorsp;
    }

    // Put on gfree list.
    // If local list is too long, transfer a batch to the global list.
    void gfput(struct p* pp, struct g* gp)
    {
        if(readgstatus(gp) != _Gdead)
        {
            go_throw("gfput: bad status (not Gdead)"s);
        }
        auto stksize = gp->stack.hi - gp->stack.lo;
        if(stksize != uintptr_t(startingStackSize))
        {
            stackfree(gp->stack);
            gp->stack.lo = 0;
            gp->stack.hi = 0;
            gp->stackguard0 = 0;
        }
        rec::push(gocpp::recv(pp->gFree), gp);
        pp->gFree.n++;
        if(pp->gFree.n >= 64)
        {
            int32_t inc = {};
            gQueue stackQ = {};
            gQueue noStackQ = {};
            for(; pp->gFree.n >= 32; )
            {
                auto gp = rec::pop(gocpp::recv(pp->gFree));
                pp->gFree.n--;
                if(gp->stack.lo == 0)
                {
                    rec::push(gocpp::recv(noStackQ), gp);
                }
                else
                {
                    rec::push(gocpp::recv(stackQ), gp);
                }
                inc++;
            }
            lock(& sched.gFree.lock);
            rec::pushAll(gocpp::recv(sched.gFree.noStack), noStackQ);
            rec::pushAll(gocpp::recv(sched.gFree.stack), stackQ);
            sched.gFree.n += inc;
            unlock(& sched.gFree.lock);
        }
    }

    // Get from gfree list.
    // If local list is empty, grab a batch from global list.
    struct g* gfget(struct p* pp)
    {
        retry:
        if(rec::empty(gocpp::recv(pp->gFree)) && (! rec::empty(gocpp::recv(sched.gFree.stack)) || ! rec::empty(gocpp::recv(sched.gFree.noStack))))
        {
            lock(& sched.gFree.lock);
            for(; pp->gFree.n < 32; )
            {
                auto gp = rec::pop(gocpp::recv(sched.gFree.stack));
                if(gp == nullptr)
                {
                    gp = rec::pop(gocpp::recv(sched.gFree.noStack));
                    if(gp == nullptr)
                    {
                        break;
                    }
                }
                sched.gFree.n--;
                rec::push(gocpp::recv(pp->gFree), gp);
                pp->gFree.n++;
            }
            unlock(& sched.gFree.lock);
            goto retry;
        }
        auto gp = rec::pop(gocpp::recv(pp->gFree));
        if(gp == nullptr)
        {
            return nullptr;
        }
        pp->gFree.n--;
        if(gp->stack.lo != 0 && gp->stack.hi - gp->stack.lo != uintptr_t(startingStackSize))
        {
            systemstack([=]() mutable -> void
            {
                stackfree(gp->stack);
                gp->stack.lo = 0;
                gp->stack.hi = 0;
                gp->stackguard0 = 0;
            });
        }
        if(gp->stack.lo == 0)
        {
            systemstack([=]() mutable -> void
            {
                gp->stack = stackalloc(startingStackSize);
            });
            gp->stackguard0 = gp->stack.lo + stackGuard;
        }
        else
        {
            if(raceenabled)
            {
                racemalloc(unsafe::Pointer(gp->stack.lo), gp->stack.hi - gp->stack.lo);
            }
            if(msanenabled)
            {
                msanmalloc(unsafe::Pointer(gp->stack.lo), gp->stack.hi - gp->stack.lo);
            }
            if(asanenabled)
            {
                asanunpoison(unsafe::Pointer(gp->stack.lo), gp->stack.hi - gp->stack.lo);
            }
        }
        return gp;
    }

    // Purge all cached G's from gfree list to the global list.
    void gfpurge(struct p* pp)
    {
        int32_t inc = {};
        gQueue stackQ = {};
        gQueue noStackQ = {};
        for(; ! rec::empty(gocpp::recv(pp->gFree)); )
        {
            auto gp = rec::pop(gocpp::recv(pp->gFree));
            pp->gFree.n--;
            if(gp->stack.lo == 0)
            {
                rec::push(gocpp::recv(noStackQ), gp);
            }
            else
            {
                rec::push(gocpp::recv(stackQ), gp);
            }
            inc++;
        }
        lock(& sched.gFree.lock);
        rec::pushAll(gocpp::recv(sched.gFree.noStack), noStackQ);
        rec::pushAll(gocpp::recv(sched.gFree.stack), stackQ);
        sched.gFree.n += inc;
        unlock(& sched.gFree.lock);
    }

    // Breakpoint executes a breakpoint trap.
    void Breakpoint()
    {
        breakpoint();
    }

    // dolockOSThread is called by LockOSThread and lockOSThread below
    // after they modify m.locked. Do not allow preemption during this call,
    // or else the m might be different in this function than in the caller.
    //
    //go:nosplit
    void dolockOSThread()
    {
        if(GOARCH == "wasm"s)
        {
            return;
        }
        auto gp = getg();
        rec::set(gocpp::recv(gp->m->lockedg), gp);
        rec::set(gocpp::recv(gp->lockedm), gp->m);
    }

    // LockOSThread wires the calling goroutine to its current operating system thread.
    // The calling goroutine will always execute in that thread,
    // and no other goroutine will execute in it,
    // until the calling goroutine has made as many calls to
    // [UnlockOSThread] as to LockOSThread.
    // If the calling goroutine exits without unlocking the thread,
    // the thread will be terminated.
    //
    // All init functions are run on the startup thread. Calling LockOSThread
    // from an init function will cause the main function to be invoked on
    // that thread.
    //
    // A goroutine should call LockOSThread before calling OS services or
    // non-Go library functions that depend on per-thread state.
    //
    //go:nosplit
    void LockOSThread()
    {
        if(atomic::Load(& newmHandoff.haveTemplateThread) == 0 && GOOS != "plan9"s)
        {
            startTemplateThread();
        }
        auto gp = getg();
        gp->m->lockedExt++;
        if(gp->m->lockedExt == 0)
        {
            gp->m->lockedExt--;
            gocpp::panic("LockOSThread nesting overflow"s);
        }
        dolockOSThread();
    }

    //go:nosplit
    void lockOSThread()
    {
        getg()->m->lockedInt++;
        dolockOSThread();
    }

    // dounlockOSThread is called by UnlockOSThread and unlockOSThread below
    // after they update m->locked. Do not allow preemption during this call,
    // or else the m might be in different in this function than in the caller.
    //
    //go:nosplit
    void dounlockOSThread()
    {
        if(GOARCH == "wasm"s)
        {
            return;
        }
        auto gp = getg();
        if(gp->m->lockedInt != 0 || gp->m->lockedExt != 0)
        {
            return;
        }
        gp->m->lockedg = 0;
        gp->lockedm = 0;
    }

    // UnlockOSThread undoes an earlier call to LockOSThread.
    // If this drops the number of active LockOSThread calls on the
    // calling goroutine to zero, it unwires the calling goroutine from
    // its fixed operating system thread.
    // If there are no active LockOSThread calls, this is a no-op.
    //
    // Before calling UnlockOSThread, the caller must ensure that the OS
    // thread is suitable for running other goroutines. If the caller made
    // any permanent changes to the state of the thread that would affect
    // other goroutines, it should not call this function and thus leave
    // the goroutine locked to the OS thread until the goroutine (and
    // hence the thread) exits.
    //
    //go:nosplit
    void UnlockOSThread()
    {
        auto gp = getg();
        if(gp->m->lockedExt == 0)
        {
            return;
        }
        gp->m->lockedExt--;
        dounlockOSThread();
    }

    //go:nosplit
    void unlockOSThread()
    {
        auto gp = getg();
        if(gp->m->lockedInt == 0)
        {
            systemstack(badunlockosthread);
        }
        gp->m->lockedInt--;
        dounlockOSThread();
    }

    void badunlockosthread()
    {
        go_throw("runtime: internal error: misuse of lockOSThread/unlockOSThread"s);
    }

    int32_t gcount()
    {
        auto n = int32_t(atomic::Loaduintptr(& allglen)) - sched.gFree.n - rec::Load(gocpp::recv(sched.ngsys));
        for(auto [gocpp_ignored, pp] : allp)
        {
            n -= pp->gFree.n;
        }
        if(n < 1)
        {
            n = 1;
        }
        return n;
    }

    int32_t mcount()
    {
        return int32_t(sched.mnext - sched.nmfreed);
    }

    struct gocpp_id_18
    {
        atomic::Uint32 signalLock;
        atomic::Int32 hz;

        using isGoStruct = void;

        template<typename T> requires gocpp::GoStruct<T>
        operator T()
        {
            T result;
            result.signalLock = this->signalLock;
            result.hz = this->hz;
            return result;
        }

        template<typename T> requires gocpp::GoStruct<T>
        bool operator==(const T& ref) const
        {
            if (signalLock != ref.signalLock) return false;
            if (hz != ref.hz) return false;
            return true;
        }

        std::ostream& PrintTo(std::ostream& os) const
        {
            os << '{';
            os << "" << signalLock;
            os << " " << hz;
            os << '}';
            return os;
        }
    };

    std::ostream& operator<<(std::ostream& os, const struct gocpp_id_18& value)
    {
        return value.PrintTo(os);
    }


    gocpp_id_18 prof;
    void _System()
    {
        _System();
    }

    void _ExternalCode()
    {
        _ExternalCode();
    }

    void _LostExternalCode()
    {
        _LostExternalCode();
    }

    void _GC()
    {
        _GC();
    }

    void _LostSIGPROFDuringAtomic64()
    {
        _LostSIGPROFDuringAtomic64();
    }

    void _LostContendedRuntimeLock()
    {
        _LostContendedRuntimeLock();
    }

    void _VDSO()
    {
        _VDSO();
    }

    // Called if we receive a SIGPROF signal.
    // Called by the signal handler, may run during STW.
    //
    //go:nowritebarrierrec
    void sigprof(uintptr_t pc, uintptr_t sp, uintptr_t lr, struct g* gp, struct m* mp)
    {
        if(rec::Load(gocpp::recv(prof.hz)) == 0)
        {
            return;
        }
        if(mp != nullptr && mp->profilehz == 0)
        {
            return;
        }
        if(GOARCH == "mips"s || GOARCH == "mipsle"s || GOARCH == "arm"s)
        {
            if(auto f = findfunc(pc); rec::valid(gocpp::recv(f)))
            {
                if(hasPrefix(funcname(f), "runtime/internal/atomic"s))
                {
                    cpuprof.lostAtomic++;
                    return;
                }
            }
            if(GOARCH == "arm"s && goarm < 7 && GOOS == "linux"s && pc & 0xffff0000 == 0xffff0000)
            {
                cpuprof.lostAtomic++;
                return;
            }
        }
        getg()->m->mallocing++;
        unwinder u = {};
        gocpp::array<uintptr_t, maxCPUProfStack> stk = {};
        auto n = 0;
        if(mp->ncgo > 0 && mp->curg != nullptr && mp->curg->syscallpc != 0 && mp->curg->syscallsp != 0)
        {
            auto cgoOff = 0;
            if(rec::Load(gocpp::recv(mp->cgoCallersUse)) == 0 && mp->cgoCallers != nullptr && mp->cgoCallers[0] != 0)
            {
                for(; cgoOff < len(mp->cgoCallers) && mp->cgoCallers[cgoOff] != 0; )
                {
                    cgoOff++;
                }
                n += copy(stk.make_slice(0), mp->cgoCallers.make_slice(0, cgoOff));
                mp->cgoCallers[0] = 0;
            }
            rec::initAt(gocpp::recv(u), mp->curg->syscallpc, mp->curg->syscallsp, 0, mp->curg, unwindSilentErrors);
        }
        else
        if(usesLibcall() && mp->libcallg != 0 && mp->libcallpc != 0 && mp->libcallsp != 0)
        {
            rec::initAt(gocpp::recv(u), mp->libcallpc, mp->libcallsp, 0, rec::ptr(gocpp::recv(mp->libcallg)), unwindSilentErrors);
        }
        else
        if(mp != nullptr && mp->vdsoSP != 0)
        {
            rec::initAt(gocpp::recv(u), mp->vdsoPC, mp->vdsoSP, 0, gp, unwindSilentErrors | unwindJumpStack);
        }
        else
        {
            rec::initAt(gocpp::recv(u), pc, sp, lr, gp, unwindSilentErrors | unwindTrap | unwindJumpStack);
        }
        n += tracebackPCs(& u, 0, stk.make_slice(n));
        if(n <= 0)
        {
            n = 2;
            if(inVDSOPage(pc))
            {
                pc = abi::FuncPCABIInternal(_VDSO) + sys::PCQuantum;
            }
            else
            if(pc > firstmoduledata.etext)
            {
                pc = abi::FuncPCABIInternal(_ExternalCode) + sys::PCQuantum;
            }
            stk[0] = pc;
            if(mp->preemptoff != ""s)
            {
                stk[1] = abi::FuncPCABIInternal(_GC) + sys::PCQuantum;
            }
            else
            {
                stk[1] = abi::FuncPCABIInternal(_System) + sys::PCQuantum;
            }
        }
        if(rec::Load(gocpp::recv(prof.hz)) != 0)
        {
            // Note: it can happen on Windows that we interrupted a system thread
            // with no g, so gp could nil. The other nil checks are done out of
            // caution, but not expected to be nil in practice.
            unsafe::Pointer* tagPtr = {};
            if(gp != nullptr && gp->m != nullptr && gp->m->curg != nullptr)
            {
                tagPtr = & gp->m->curg->labels;
            }
            rec::add(gocpp::recv(cpuprof), tagPtr, stk.make_slice(0, n));
            auto gprof = gp;
            m* mp = {};
            p* pp = {};
            if(gp != nullptr && gp->m != nullptr)
            {
                if(gp->m->curg != nullptr)
                {
                    gprof = gp->m->curg;
                }
                mp = gp->m;
                pp = rec::ptr(gocpp::recv(gp->m->p));
            }
            traceCPUSample(gprof, mp, pp, stk.make_slice(0, n));
        }
        getg()->m->mallocing--;
    }

    // setcpuprofilerate sets the CPU profiling rate to hz times per second.
    // If hz <= 0, setcpuprofilerate turns off CPU profiling.
    void setcpuprofilerate(int32_t hz)
    {
        if(hz < 0)
        {
            hz = 0;
        }
        auto gp = getg();
        gp->m->locks++;
        setThreadCPUProfiler(0);
        for(; ! rec::CompareAndSwap(gocpp::recv(prof.signalLock), 0, 1); )
        {
            osyield();
        }
        if(rec::Load(gocpp::recv(prof.hz)) != hz)
        {
            setProcessCPUProfiler(hz);
            rec::Store(gocpp::recv(prof.hz), hz);
        }
        rec::Store(gocpp::recv(prof.signalLock), 0);
        lock(& sched.lock);
        sched.profilehz = hz;
        unlock(& sched.lock);
        if(hz != 0)
        {
            setThreadCPUProfiler(hz);
        }
        gp->m->locks--;
    }

    // init initializes pp, which may be a freshly allocated p or a
    // previously destroyed p, and transitions it to status _Pgcstop.
    void rec::init(struct p* pp, int32_t id)
    {
        pp->id = id;
        pp->status = _Pgcstop;
        pp->sudogcache = pp->sudogbuf.make_slice(0, 0);
        pp->deferpool = pp->deferpoolbuf.make_slice(0, 0);
        rec::reset(gocpp::recv(pp->wbBuf));
        if(pp->mcache == nullptr)
        {
            if(id == 0)
            {
                if(mcache0 == nullptr)
                {
                    go_throw("missing mcache?"s);
                }
                pp->mcache = mcache0;
            }
            else
            {
                pp->mcache = allocmcache();
            }
        }
        if(raceenabled && pp->raceprocctx == 0)
        {
            if(id == 0)
            {
                pp->raceprocctx = raceprocctx0;
                raceprocctx0 = 0;
            }
            else
            {
                pp->raceprocctx = raceproccreate();
            }
        }
        lockInit(& pp->timersLock, lockRankTimers);
        rec::set(gocpp::recv(timerpMask), id);
        rec::clear(gocpp::recv(idlepMask), id);
    }

    // destroy releases all of the resources associated with pp and
    // transitions it to status _Pdead.
    //
    // sched.lock must be held and the world must be stopped.
    void rec::destroy(struct p* pp)
    {
        assertLockHeld(& sched.lock);
        assertWorldStopped();
        for(; pp->runqhead != pp->runqtail; )
        {
            pp->runqtail--;
            auto gp = rec::ptr(gocpp::recv(pp->runq[pp->runqtail % uint32_t(len(pp->runq))]));
            globrunqputhead(gp);
        }
        if(pp->runnext != 0)
        {
            globrunqputhead(rec::ptr(gocpp::recv(pp->runnext)));
            pp->runnext = 0;
        }
        if(len(pp->timers) > 0)
        {
            auto plocal = rec::ptr(gocpp::recv(getg()->m->p));
            lock(& plocal->timersLock);
            lock(& pp->timersLock);
            moveTimers(plocal, pp->timers);
            pp->timers = nullptr;
            rec::Store(gocpp::recv(pp->numTimers), 0);
            rec::Store(gocpp::recv(pp->deletedTimers), 0);
            rec::Store(gocpp::recv(pp->timer0When), 0);
            unlock(& pp->timersLock);
            unlock(& plocal->timersLock);
        }
        if(gcphase != _GCoff)
        {
            wbBufFlush1(pp);
            rec::dispose(gocpp::recv(pp->gcw));
        }
        for(auto [i, gocpp_ignored] : pp->sudogbuf)
        {
            pp->sudogbuf[i] = nullptr;
        }
        pp->sudogcache = pp->sudogbuf.make_slice(0, 0);
        pp->pinnerCache = nullptr;
        for(auto [j, gocpp_ignored] : pp->deferpoolbuf)
        {
            pp->deferpoolbuf[j] = nullptr;
        }
        pp->deferpool = pp->deferpoolbuf.make_slice(0, 0);
        systemstack([=]() mutable -> void
        {
            for(auto i = 0; i < pp->mspancache.len; i++)
            {
                rec::free(gocpp::recv(mheap_.spanalloc), unsafe::Pointer(pp->mspancache.buf[i]));
            }
            pp->mspancache.len = 0;
            lock(& mheap_.lock);
            rec::flush(gocpp::recv(pp->pcache), & mheap_.pages);
            unlock(& mheap_.lock);
        });
        freemcache(pp->mcache);
        pp->mcache = nullptr;
        gfpurge(pp);
        traceProcFree(pp);
        if(raceenabled)
        {
            if(pp->timerRaceCtx != 0)
            {
                auto mp = getg()->m;
                auto phold = rec::ptr(gocpp::recv(mp->p));
                rec::set(gocpp::recv(mp->p), pp);
                racectxend(pp->timerRaceCtx);
                pp->timerRaceCtx = 0;
                rec::set(gocpp::recv(mp->p), phold);
            }
            raceprocdestroy(pp->raceprocctx);
            pp->raceprocctx = 0;
        }
        pp->gcAssistTime = 0;
        pp->status = _Pdead;
    }

    // Change number of processors.
    //
    // sched.lock must be held, and the world must be stopped.
    //
    // gcworkbufs must not be being modified by either the GC or the write barrier
    // code, so the GC must not be running if the number of Ps actually changes.
    //
    // Returns list of Ps with local work, they need to be scheduled by the caller.
    struct p* procresize(int32_t nprocs)
    {
        assertLockHeld(& sched.lock);
        assertWorldStopped();
        auto old = gomaxprocs;
        if(old < 0 || nprocs <= 0)
        {
            go_throw("procresize: invalid arg"s);
        }
        auto trace = traceAcquire();
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::Gomaxprocs(gocpp::recv(trace), nprocs);
            traceRelease(trace);
        }
        auto now = nanotime();
        if(sched.procresizetime != 0)
        {
            sched.totaltime += int64_t(old) * (now - sched.procresizetime);
        }
        sched.procresizetime = now;
        auto maskWords = (nprocs + 31) / 32;
        if(nprocs > int32_t(len(allp)))
        {
            lock(& allpLock);
            if(nprocs <= int32_t(cap(allp)))
            {
                allp = allp.make_slice(0, nprocs);
            }
            else
            {
                auto nallp = gocpp::make(gocpp::Tag<gocpp::slice<p*>>(), nprocs);
                copy(nallp, allp.make_slice(0, cap(allp)));
                allp = nallp;
            }
            if(maskWords <= int32_t(cap(idlepMask)))
            {
                idlepMask = idlepMask.make_slice(0, maskWords);
                timerpMask = timerpMask.make_slice(0, maskWords);
            }
            else
            {
                auto nidlepMask = gocpp::make(gocpp::Tag<gocpp::slice<uint32_t>>(), maskWords);
                copy(nidlepMask, idlepMask);
                idlepMask = nidlepMask;
                auto ntimerpMask = gocpp::make(gocpp::Tag<gocpp::slice<uint32_t>>(), maskWords);
                copy(ntimerpMask, timerpMask);
                timerpMask = ntimerpMask;
            }
            unlock(& allpLock);
        }
        for(auto i = old; i < nprocs; i++)
        {
            auto pp = allp[i];
            if(pp == nullptr)
            {
                pp = new(p);
            }
            rec::init(gocpp::recv(pp), i);
            atomicstorep(unsafe::Pointer(& allp[i]), unsafe::Pointer(pp));
        }
        auto gp = getg();
        if(gp->m->p != 0 && rec::ptr(gocpp::recv(gp->m->p))->id < nprocs)
        {
            rec::ptr(gocpp::recv(gp->m->p))->status = _Prunning;
            rec::prepareForSweep(gocpp::recv(rec::ptr(gocpp::recv(gp->m->p))->mcache));
        }
        else
        {
            if(gp->m->p != 0)
            {
                auto trace = traceAcquire();
                if(rec::ok(gocpp::recv(trace)))
                {
                    rec::GoSched(gocpp::recv(trace));
                    rec::ProcStop(gocpp::recv(trace), rec::ptr(gocpp::recv(gp->m->p)));
                    traceRelease(trace);
                }
                rec::ptr(gocpp::recv(gp->m->p))->m = 0;
            }
            gp->m->p = 0;
            auto pp = allp[0];
            pp->m = 0;
            pp->status = _Pidle;
            acquirep(pp);
            auto trace = traceAcquire();
            if(rec::ok(gocpp::recv(trace)))
            {
                rec::GoStart(gocpp::recv(trace));
                traceRelease(trace);
            }
        }
        mcache0 = nullptr;
        for(auto i = nprocs; i < old; i++)
        {
            auto pp = allp[i];
            rec::destroy(gocpp::recv(pp));
        }
        if(int32_t(len(allp)) != nprocs)
        {
            lock(& allpLock);
            allp = allp.make_slice(0, nprocs);
            idlepMask = idlepMask.make_slice(0, maskWords);
            timerpMask = timerpMask.make_slice(0, maskWords);
            unlock(& allpLock);
        }
        p* runnablePs = {};
        for(auto i = nprocs - 1; i >= 0; i--)
        {
            auto pp = allp[i];
            if(rec::ptr(gocpp::recv(gp->m->p)) == pp)
            {
                continue;
            }
            pp->status = _Pidle;
            if(runqempty(pp))
            {
                pidleput(pp, now);
            }
            else
            {
                rec::set(gocpp::recv(pp->m), mget());
                rec::set(gocpp::recv(pp->link), runnablePs);
                runnablePs = pp;
            }
        }
        rec::reset(gocpp::recv(stealOrder), uint32_t(nprocs));
        int32_t* int32p = & gomaxprocs;
        atomic::Store((uint32_t*)(unsafe::Pointer(int32p)), uint32_t(nprocs));
        if(old != nprocs)
        {
            rec::resetCapacity(gocpp::recv(gcCPULimiter), now, nprocs);
        }
        return runnablePs;
    }

    // Associate p and the current m.
    //
    // This function is allowed to have write barriers even if the caller
    // isn't because it immediately acquires pp.
    //
    //go:yeswritebarrierrec
    void acquirep(struct p* pp)
    {
        wirep(pp);
        rec::prepareForSweep(gocpp::recv(pp->mcache));
        auto trace = traceAcquire();
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::ProcStart(gocpp::recv(trace));
            traceRelease(trace);
        }
    }

    // wirep is the first step of acquirep, which actually associates the
    // current M to pp. This is broken out so we can disallow write
    // barriers for this part, since we don't yet have a P.
    //
    //go:nowritebarrierrec
    //go:nosplit
    void wirep(struct p* pp)
    {
        auto gp = getg();
        if(gp->m->p != 0)
        {
            systemstack([=]() mutable -> void
            {
                go_throw("wirep: already in go"s);
            });
        }
        if(pp->m != 0 || pp->status != _Pidle)
        {
            systemstack([=]() mutable -> void
            {
                auto id = int64_t(0);
                if(pp->m != 0)
                {
                    id = rec::ptr(gocpp::recv(pp->m))->id;
                }
                print("wirep: p->m="s, pp->m, "("s, id, ") p->status="s, pp->status, "\n"s);
                go_throw("wirep: invalid p state"s);
            });
        }
        rec::set(gocpp::recv(gp->m->p), pp);
        rec::set(gocpp::recv(pp->m), gp->m);
        pp->status = _Prunning;
    }

    // Disassociate p and the current m.
    struct p* releasep()
    {
        auto trace = traceAcquire();
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::ProcStop(gocpp::recv(trace), rec::ptr(gocpp::recv(getg()->m->p)));
            traceRelease(trace);
        }
        return releasepNoTrace();
    }

    // Disassociate p and the current m without tracing an event.
    struct p* releasepNoTrace()
    {
        auto gp = getg();
        if(gp->m->p == 0)
        {
            go_throw("releasep: invalid arg"s);
        }
        auto pp = rec::ptr(gocpp::recv(gp->m->p));
        if(rec::ptr(gocpp::recv(pp->m)) != gp->m || pp->status != _Prunning)
        {
            print("releasep: m="s, gp->m, " m->p="s, rec::ptr(gocpp::recv(gp->m->p)), " p->m="s, hex(pp->m), " p->status="s, pp->status, "\n"s);
            go_throw("releasep: invalid p state"s);
        }
        gp->m->p = 0;
        pp->m = 0;
        pp->status = _Pidle;
        return pp;
    }

    void incidlelocked(int32_t v)
    {
        lock(& sched.lock);
        sched.nmidlelocked += v;
        if(v > 0)
        {
            checkdead();
        }
        unlock(& sched.lock);
    }

    // Check for deadlock situation.
    // The check is based on number of running M's, if 0 -> deadlock.
    // sched.lock must be held.
    void checkdead()
    {
        assertLockHeld(& sched.lock);
        if(islibrary || isarchive)
        {
            return;
        }
        if(rec::Load(gocpp::recv(panicking)) > 0)
        {
            return;
        }
        // If we are not running under cgo, but we have an extra M then account
        // for it. (It is possible to have an extra M on Windows without cgo to
        // accommodate callbacks created by syscall.NewCallback. See issue #6751
        // for details.)
        int32_t run0 = {};
        if(! iscgo && cgoHasExtraM && rec::Load(gocpp::recv(extraMLength)) > 0)
        {
            run0 = 1;
        }
        auto run = mcount() - sched.nmidle - sched.nmidlelocked - sched.nmsys;
        if(run > run0)
        {
            return;
        }
        if(run < 0)
        {
            print("runtime: checkdead: nmidle="s, sched.nmidle, " nmidlelocked="s, sched.nmidlelocked, " mcount="s, mcount(), " nmsys="s, sched.nmsys, "\n"s);
            unlock(& sched.lock);
            go_throw("checkdead: inconsistent counts"s);
        }
        auto grunning = 0;
        forEachG([=](struct g* gp) mutable -> void
        {
            if(isSystemGoroutine(gp, false))
            {
                return;
            }
            auto s = readgstatus(gp);
            //Go switch emulation
            {
                auto condition = s &^ _Gscan;
                int conditionId = -1;
                if(condition == _Gwaiting) { conditionId = 0; }
                else if(condition == _Gpreempted) { conditionId = 1; }
                else if(condition == _Grunnable) { conditionId = 2; }
                else if(condition == _Grunning) { conditionId = 3; }
                else if(condition == _Gsyscall) { conditionId = 4; }
                switch(conditionId)
                {
                    case 0:
                    case 1:
                        grunning++;
                        break;
                    case 2:
                    case 3:
                    case 4:
                        print("runtime: checkdead: find g "s, gp->goid, " in status "s, s, "\n"s);
                        unlock(& sched.lock);
                        go_throw("checkdead: runnable g"s);
                        break;
                }
            }
        });
        if(grunning == 0)
        {
            unlock(& sched.lock);
            fatal("no goroutines (main called runtime.Goexit) - deadlock!"s);
        }
        if(faketime != 0)
        {
            if(auto when = timeSleepUntil(); when < maxWhen)
            {
                faketime = when;
                auto [pp, gocpp_id_20] = pidleget(faketime);
                if(pp == nullptr)
                {
                    unlock(& sched.lock);
                    go_throw("checkdead: no p for timer"s);
                }
                auto mp = mget();
                if(mp == nullptr)
                {
                    unlock(& sched.lock);
                    go_throw("checkdead: no m for timer"s);
                }
                rec::Add(gocpp::recv(sched.nmspinning), 1);
                mp->spinning = true;
                rec::set(gocpp::recv(mp->nextp), pp);
                notewakeup(& mp->park);
                return;
            }
        }
        for(auto [gocpp_ignored, pp] : allp)
        {
            if(len(pp->timers) > 0)
            {
                return;
            }
        }
        unlock(& sched.lock);
        fatal("all goroutines are asleep - deadlock!"s);
    }

    // forcegcperiod is the maximum time in nanoseconds between garbage
    // collections. If we go this long without a garbage collection, one
    // is forced to run.
    //
    // This is a variable for testing purposes. It normally doesn't change.
    int64_t forcegcperiod = 2 * 60 * 1e9;
    // needSysmonWorkaround is true if the workaround for
    // golang.org/issue/42515 is needed on NetBSD.
    bool needSysmonWorkaround = false;
    // Always runs without a P, so write barriers are not allowed.
    //
    //go:nowritebarrierrec
    void sysmon()
    {
        lock(& sched.lock);
        sched.nmsys++;
        checkdead();
        unlock(& sched.lock);
        auto lasttrace = int64_t(0);
        auto idle = 0;
        auto delay = uint32_t(0);
        for(; ; )
        {
            if(idle == 0)
            {
                delay = 20;
            }
            else
            if(idle > 50)
            {
                delay *= 2;
            }
            if(delay > 10 * 1000)
            {
                delay = 10 * 1000;
            }
            usleep(delay);
            auto now = nanotime();
            if(debug.schedtrace <= 0 && (rec::Load(gocpp::recv(sched.gcwaiting)) || rec::Load(gocpp::recv(sched.npidle)) == gomaxprocs))
            {
                lock(& sched.lock);
                if(rec::Load(gocpp::recv(sched.gcwaiting)) || rec::Load(gocpp::recv(sched.npidle)) == gomaxprocs)
                {
                    auto syscallWake = false;
                    auto next = timeSleepUntil();
                    if(next > now)
                    {
                        rec::Store(gocpp::recv(sched.sysmonwait), true);
                        unlock(& sched.lock);
                        auto sleep = forcegcperiod / 2;
                        if(next - now < sleep)
                        {
                            sleep = next - now;
                        }
                        auto shouldRelax = sleep >= osRelaxMinNS;
                        if(shouldRelax)
                        {
                            osRelax(true);
                        }
                        syscallWake = notetsleep(& sched.sysmonnote, sleep);
                        if(shouldRelax)
                        {
                            osRelax(false);
                        }
                        lock(& sched.lock);
                        rec::Store(gocpp::recv(sched.sysmonwait), false);
                        noteclear(& sched.sysmonnote);
                    }
                    if(syscallWake)
                    {
                        idle = 0;
                        delay = 20;
                    }
                }
                unlock(& sched.lock);
            }
            lock(& sched.sysmonlock);
            now = nanotime();
            if(*cgo_yield != nullptr)
            {
                asmcgocall(*cgo_yield, nullptr);
            }
            auto lastpoll = rec::Load(gocpp::recv(sched.lastpoll));
            if(netpollinited() && lastpoll != 0 && lastpoll + 10 * 1000 * 1000 < now)
            {
                rec::CompareAndSwap(gocpp::recv(sched.lastpoll), lastpoll, now);
                auto [list, delta] = netpoll(0);
                if(! rec::empty(gocpp::recv(list)))
                {
                    incidlelocked(- 1);
                    injectglist(& list);
                    incidlelocked(1);
                    netpollAdjustWaiters(delta);
                }
            }
            if(GOOS == "netbsd"s && needSysmonWorkaround)
            {
                if(auto next = timeSleepUntil(); next < now)
                {
                    startm(nullptr, false, false);
                }
            }
            if(rec::Load(gocpp::recv(scavenger.sysmonWake)) != 0)
            {
                rec::wake(gocpp::recv(scavenger));
            }
            if(retake(now) != 0)
            {
                idle = 0;
            }
            else
            {
                idle++;
            }
            if(auto t = (gocpp::Init<gcTrigger>([=](auto& x) {
                x.kind = gcTriggerTime;
                x.now = now;
            })); rec::test(gocpp::recv(t)) && rec::Load(gocpp::recv(forcegc.idle)))
            {
                lock(& forcegc.lock);
                rec::Store(gocpp::recv(forcegc.idle), false);
                gList list = {};
                rec::push(gocpp::recv(list), forcegc.g);
                injectglist(& list);
                unlock(& forcegc.lock);
            }
            if(debug.schedtrace > 0 && lasttrace + int64_t(debug.schedtrace) * 1000000 <= now)
            {
                lasttrace = now;
                schedtrace(debug.scheddetail > 0);
            }
            unlock(& sched.sysmonlock);
        }
    }

    
    template<typename T> requires gocpp::GoStruct<T>
    sysmontick::operator T()
    {
        T result;
        result.schedtick = this->schedtick;
        result.schedwhen = this->schedwhen;
        result.syscalltick = this->syscalltick;
        result.syscallwhen = this->syscallwhen;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool sysmontick::operator==(const T& ref) const
    {
        if (schedtick != ref.schedtick) return false;
        if (schedwhen != ref.schedwhen) return false;
        if (syscalltick != ref.syscalltick) return false;
        if (syscallwhen != ref.syscallwhen) return false;
        return true;
    }

    std::ostream& sysmontick::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << schedtick;
        os << " " << schedwhen;
        os << " " << syscalltick;
        os << " " << syscallwhen;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct sysmontick& value)
    {
        return value.PrintTo(os);
    }

    // forcePreemptNS is the time slice given to a G before it is
    // preempted.
    uint32_t retake(int64_t now)
    {
        auto n = 0;
        lock(& allpLock);
        for(auto i = 0; i < len(allp); i++)
        {
            auto pp = allp[i];
            if(pp == nullptr)
            {
                continue;
            }
            auto pd = & pp->sysmontick;
            auto s = pp->status;
            auto sysretake = false;
            if(s == _Prunning || s == _Psyscall)
            {
                auto t = int64_t(pp->schedtick);
                if(int64_t(pd->schedtick) != t)
                {
                    pd->schedtick = uint32_t(t);
                    pd->schedwhen = now;
                }
                else
                if(pd->schedwhen + forcePreemptNS <= now)
                {
                    preemptone(pp);
                    sysretake = true;
                }
            }
            if(s == _Psyscall)
            {
                auto t = int64_t(pp->syscalltick);
                if(! sysretake && int64_t(pd->syscalltick) != t)
                {
                    pd->syscalltick = uint32_t(t);
                    pd->syscallwhen = now;
                    continue;
                }
                if(runqempty(pp) && rec::Load(gocpp::recv(sched.nmspinning)) + rec::Load(gocpp::recv(sched.npidle)) > 0 && pd->syscallwhen + 10 * 1000 * 1000 > now)
                {
                    continue;
                }
                unlock(& allpLock);
                incidlelocked(- 1);
                auto trace = traceAcquire();
                if(atomic::Cas(& pp->status, s, _Pidle))
                {
                    if(rec::ok(gocpp::recv(trace)))
                    {
                        rec::GoSysBlock(gocpp::recv(trace), pp);
                        rec::ProcSteal(gocpp::recv(trace), pp, false);
                        traceRelease(trace);
                    }
                    n++;
                    pp->syscalltick++;
                    handoffp(pp);
                }
                else
                if(rec::ok(gocpp::recv(trace)))
                {
                    traceRelease(trace);
                }
                incidlelocked(1);
                lock(& allpLock);
            }
        }
        unlock(& allpLock);
        return uint32_t(n);
    }

    // Tell all goroutines that they have been preempted and they should stop.
    // This function is purely best-effort. It can fail to inform a goroutine if a
    // processor just started running it.
    // No locks need to be held.
    // Returns true if preemption request was issued to at least one goroutine.
    bool preemptall()
    {
        auto res = false;
        for(auto [gocpp_ignored, pp] : allp)
        {
            if(pp->status != _Prunning)
            {
                continue;
            }
            if(preemptone(pp))
            {
                res = true;
            }
        }
        return res;
    }

    // Tell the goroutine running on processor P to stop.
    // This function is purely best-effort. It can incorrectly fail to inform the
    // goroutine. It can inform the wrong goroutine. Even if it informs the
    // correct goroutine, that goroutine might ignore the request if it is
    // simultaneously executing newstack.
    // No lock needs to be held.
    // Returns true if preemption request was issued.
    // The actual preemption will happen at some point in the future
    // and will be indicated by the gp->status no longer being
    // Grunning
    bool preemptone(struct p* pp)
    {
        auto mp = rec::ptr(gocpp::recv(pp->m));
        if(mp == nullptr || mp == getg()->m)
        {
            return false;
        }
        auto gp = mp->curg;
        if(gp == nullptr || gp == mp->g0)
        {
            return false;
        }
        gp->preempt = true;
        gp->stackguard0 = stackPreempt;
        if(preemptMSupported && debug.asyncpreemptoff == 0)
        {
            pp->preempt = true;
            preemptM(mp);
        }
        return true;
    }

    int64_t starttime;
    void schedtrace(bool detailed)
    {
        auto now = nanotime();
        if(starttime == 0)
        {
            starttime = now;
        }
        lock(& sched.lock);
        print("SCHED "s, (now - starttime) / 1e6, "ms: gomaxprocs="s, gomaxprocs, " idleprocs="s, rec::Load(gocpp::recv(sched.npidle)), " threads="s, mcount(), " spinningthreads="s, rec::Load(gocpp::recv(sched.nmspinning)), " needspinning="s, rec::Load(gocpp::recv(sched.needspinning)), " idlethreads="s, sched.nmidle, " runqueue="s, sched.runqsize);
        if(detailed)
        {
            print(" gcwaiting="s, rec::Load(gocpp::recv(sched.gcwaiting)), " nmidlelocked="s, sched.nmidlelocked, " stopwait="s, sched.stopwait, " sysmonwait="s, rec::Load(gocpp::recv(sched.sysmonwait)), "\n"s);
        }
        for(auto [i, pp] : allp)
        {
            auto mp = rec::ptr(gocpp::recv(pp->m));
            auto h = atomic::Load(& pp->runqhead);
            auto t = atomic::Load(& pp->runqtail);
            if(detailed)
            {
                print("  P"s, i, ": status="s, pp->status, " schedtick="s, pp->schedtick, " syscalltick="s, pp->syscalltick, " m="s);
                if(mp != nullptr)
                {
                    print(mp->id);
                }
                else
                {
                    print("nil"s);
                }
                print(" runqsize="s, t - h, " gfreecnt="s, pp->gFree.n, " timerslen="s, len(pp->timers), "\n"s);
            }
            else
            {
                print(" "s);
                if(i == 0)
                {
                    print("["s);
                }
                print(t - h);
                if(i == len(allp) - 1)
                {
                    print("]\n"s);
                }
            }
        }
        if(! detailed)
        {
            unlock(& sched.lock);
            return;
        }
        for(auto mp = allm; mp != nullptr; mp = mp->alllink)
        {
            auto pp = rec::ptr(gocpp::recv(mp->p));
            print("  M"s, mp->id, ": p="s);
            if(pp != nullptr)
            {
                print(pp->id);
            }
            else
            {
                print("nil"s);
            }
            print(" curg="s);
            if(mp->curg != nullptr)
            {
                print(mp->curg->goid);
            }
            else
            {
                print("nil"s);
            }
            print(" mallocing="s, mp->mallocing, " throwing="s, mp->throwing, " preemptoff="s, mp->preemptoff, " locks="s, mp->locks, " dying="s, mp->dying, " spinning="s, mp->spinning, " blocked="s, mp->blocked, " lockedg="s);
            if(auto lockedg = rec::ptr(gocpp::recv(mp->lockedg)); lockedg != nullptr)
            {
                print(lockedg->goid);
            }
            else
            {
                print("nil"s);
            }
            print("\n"s);
        }
        forEachG([=](struct g* gp) mutable -> void
        {
            print("  G"s, gp->goid, ": status="s, readgstatus(gp), "("s, rec::String(gocpp::recv(gp->waitreason)), ") m="s);
            if(gp->m != nullptr)
            {
                print(gp->m->id);
            }
            else
            {
                print("nil"s);
            }
            print(" lockedm="s);
            if(auto lockedm = rec::ptr(gocpp::recv(gp->lockedm)); lockedm != nullptr)
            {
                print(lockedm->id);
            }
            else
            {
                print("nil"s);
            }
            print("\n"s);
        });
        unlock(& sched.lock);
    }

    // schedEnableUser enables or disables the scheduling of user
    // goroutines.
    //
    // This does not stop already running user goroutines, so the caller
    // should first stop the world when disabling user goroutines.
    void schedEnableUser(bool enable)
    {
        lock(& sched.lock);
        if(sched.disable.user == ! enable)
        {
            unlock(& sched.lock);
            return;
        }
        sched.disable.user = ! enable;
        if(enable)
        {
            auto n = sched.disable.n;
            sched.disable.n = 0;
            globrunqputbatch(& sched.disable.runnable, n);
            unlock(& sched.lock);
            for(; n != 0 && rec::Load(gocpp::recv(sched.npidle)) != 0; n--)
            {
                startm(nullptr, false, false);
            }
        }
        else
        {
            unlock(& sched.lock);
        }
    }

    // schedEnabled reports whether gp should be scheduled. It returns
    // false is scheduling of gp is disabled.
    //
    // sched.lock must be held.
    bool schedEnabled(struct g* gp)
    {
        assertLockHeld(& sched.lock);
        if(sched.disable.user)
        {
            return isSystemGoroutine(gp, true);
        }
        return true;
    }

    // Put mp on midle list.
    // sched.lock must be held.
    // May run during STW, so write barriers are not allowed.
    //
    //go:nowritebarrierrec
    void mput(struct m* mp)
    {
        assertLockHeld(& sched.lock);
        mp->schedlink = sched.midle;
        rec::set(gocpp::recv(sched.midle), mp);
        sched.nmidle++;
        checkdead();
    }

    // Try to get an m from midle list.
    // sched.lock must be held.
    // May run during STW, so write barriers are not allowed.
    //
    //go:nowritebarrierrec
    struct m* mget()
    {
        assertLockHeld(& sched.lock);
        auto mp = rec::ptr(gocpp::recv(sched.midle));
        if(mp != nullptr)
        {
            sched.midle = mp->schedlink;
            sched.nmidle--;
        }
        return mp;
    }

    // Put gp on the global runnable queue.
    // sched.lock must be held.
    // May run during STW, so write barriers are not allowed.
    //
    //go:nowritebarrierrec
    void globrunqput(struct g* gp)
    {
        assertLockHeld(& sched.lock);
        rec::pushBack(gocpp::recv(sched.runq), gp);
        sched.runqsize++;
    }

    // Put gp at the head of the global runnable queue.
    // sched.lock must be held.
    // May run during STW, so write barriers are not allowed.
    //
    //go:nowritebarrierrec
    void globrunqputhead(struct g* gp)
    {
        assertLockHeld(& sched.lock);
        rec::push(gocpp::recv(sched.runq), gp);
        sched.runqsize++;
    }

    // Put a batch of runnable goroutines on the global runnable queue.
    // This clears *batch.
    // sched.lock must be held.
    // May run during STW, so write barriers are not allowed.
    //
    //go:nowritebarrierrec
    void globrunqputbatch(struct gQueue* batch, int32_t n)
    {
        assertLockHeld(& sched.lock);
        rec::pushBackAll(gocpp::recv(sched.runq), *batch);
        sched.runqsize += n;
        *batch = gQueue {};
    }

    // Try get a batch of G's from the global runnable queue.
    // sched.lock must be held.
    struct g* globrunqget(struct p* pp, int32_t max)
    {
        assertLockHeld(& sched.lock);
        if(sched.runqsize == 0)
        {
            return nullptr;
        }
        auto n = sched.runqsize / gomaxprocs + 1;
        if(n > sched.runqsize)
        {
            n = sched.runqsize;
        }
        if(max > 0 && n > max)
        {
            n = max;
        }
        if(n > int32_t(len(pp->runq)) / 2)
        {
            n = int32_t(len(pp->runq)) / 2;
        }
        sched.runqsize -= n;
        auto gp = rec::pop(gocpp::recv(sched.runq));
        n--;
        for(; n > 0; n--)
        {
            auto gp1 = rec::pop(gocpp::recv(sched.runq));
            runqput(pp, gp1, false);
        }
        return gp;
    }

    // pMask is an atomic bitstring with one bit per P.
    // read returns true if P id's bit is set.
    bool rec::read(golang::runtime::pMask p, uint32_t id)
    {
        auto word = id / 32;
        auto mask = uint32_t(1) << (id % 32);
        return (atomic::Load(& p[word]) & mask) != 0;
    }

    // set sets P id's bit.
    void rec::set(golang::runtime::pMask p, int32_t id)
    {
        auto word = id / 32;
        auto mask = uint32_t(1) << (id % 32);
        atomic::Or(& p[word], mask);
    }

    // clear clears P id's bit.
    void rec::clear(golang::runtime::pMask p, int32_t id)
    {
        auto word = id / 32;
        auto mask = uint32_t(1) << (id % 32);
        atomic::And(& p[word], ~ mask);
    }

    // updateTimerPMask clears pp's timer mask if it has no timers on its heap.
    //
    // Ideally, the timer mask would be kept immediately consistent on any timer
    // operations. Unfortunately, updating a shared global data structure in the
    // timer hot path adds too much overhead in applications frequently switching
    // between no timers and some timers.
    //
    // As a compromise, the timer mask is updated only on pidleget / pidleput. A
    // running P (returned by pidleget) may add a timer at any time, so its mask
    // must be set. An idle P (passed to pidleput) cannot add new timers while
    // idle, so if it has no timers at that time, its mask may be cleared.
    //
    // Thus, we get the following effects on timer-stealing in findrunnable:
    //
    //   - Idle Ps with no timers when they go idle are never checked in findrunnable
    //     (for work- or timer-stealing; this is the ideal case).
    //   - Running Ps must always be checked.
    //   - Idle Ps whose timers are stolen must continue to be checked until they run
    //     again, even after timer expiration.
    //
    // When the P starts running again, the mask should be set, as a timer may be
    // added at any time.
    //
    // TODO(prattmic): Additional targeted updates may improve the above cases.
    // e.g., updating the mask when stealing a timer.
    void updateTimerPMask(struct p* pp)
    {
        if(rec::Load(gocpp::recv(pp->numTimers)) > 0)
        {
            return;
        }
        lock(& pp->timersLock);
        if(rec::Load(gocpp::recv(pp->numTimers)) == 0)
        {
            rec::clear(gocpp::recv(timerpMask), pp->id);
        }
        unlock(& pp->timersLock);
    }

    // pidleput puts p on the _Pidle list. now must be a relatively recent call
    // to nanotime or zero. Returns now or the current time if now was zero.
    //
    // This releases ownership of p. Once sched.lock is released it is no longer
    // safe to use p.
    //
    // sched.lock must be held.
    //
    // May run during STW, so write barriers are not allowed.
    //
    //go:nowritebarrierrec
    int64_t pidleput(struct p* pp, int64_t now)
    {
        assertLockHeld(& sched.lock);
        if(! runqempty(pp))
        {
            go_throw("pidleput: P has non-empty run queue"s);
        }
        if(now == 0)
        {
            now = nanotime();
        }
        updateTimerPMask(pp);
        rec::set(gocpp::recv(idlepMask), pp->id);
        pp->link = sched.pidle;
        rec::set(gocpp::recv(sched.pidle), pp);
        rec::Add(gocpp::recv(sched.npidle), 1);
        if(! rec::start(gocpp::recv(pp->limiterEvent), limiterEventIdle, now))
        {
            go_throw("must be able to track idle limiter event"s);
        }
        return now;
    }

    // pidleget tries to get a p from the _Pidle list, acquiring ownership.
    //
    // sched.lock must be held.
    //
    // May run during STW, so write barriers are not allowed.
    //
    //go:nowritebarrierrec
    std::tuple<struct p*, int64_t> pidleget(int64_t now)
    {
        assertLockHeld(& sched.lock);
        auto pp = rec::ptr(gocpp::recv(sched.pidle));
        if(pp != nullptr)
        {
            if(now == 0)
            {
                now = nanotime();
            }
            rec::set(gocpp::recv(timerpMask), pp->id);
            rec::clear(gocpp::recv(idlepMask), pp->id);
            sched.pidle = pp->link;
            rec::Add(gocpp::recv(sched.npidle), - 1);
            rec::stop(gocpp::recv(pp->limiterEvent), limiterEventIdle, now);
        }
        return {pp, now};
    }

    // pidlegetSpinning tries to get a p from the _Pidle list, acquiring ownership.
    // This is called by spinning Ms (or callers than need a spinning M) that have
    // found work. If no P is available, this must synchronized with non-spinning
    // Ms that may be preparing to drop their P without discovering this work.
    //
    // sched.lock must be held.
    //
    // May run during STW, so write barriers are not allowed.
    //
    //go:nowritebarrierrec
    std::tuple<struct p*, int64_t> pidlegetSpinning(int64_t now)
    {
        assertLockHeld(& sched.lock);
        auto [pp, now] = pidleget(now);
        if(pp == nullptr)
        {
            rec::Store(gocpp::recv(sched.needspinning), 1);
            return {nullptr, now};
        }
        return {pp, now};
    }

    // runqempty reports whether pp has no Gs on its local run queue.
    // It never returns true spuriously.
    bool runqempty(struct p* pp)
    {
        for(; ; )
        {
            auto head = atomic::Load(& pp->runqhead);
            auto tail = atomic::Load(& pp->runqtail);
            auto runnext = atomic::Loaduintptr((uintptr_t*)(unsafe::Pointer(& pp->runnext)));
            if(tail == atomic::Load(& pp->runqtail))
            {
                return head == tail && runnext == 0;
            }
        }
    }

    // To shake out latent assumptions about scheduling order,
    // we introduce some randomness into scheduling decisions
    // when running with the race detector.
    // The need for this was made obvious by changing the
    // (deterministic) scheduling order in Go 1.5 and breaking
    // many poorly-written tests.
    // With the randomness here, as long as the tests pass
    // consistently with -race, they shouldn't have latent scheduling
    // assumptions.
    // runqput tries to put g on the local runnable queue.
    // If next is false, runqput adds g to the tail of the runnable queue.
    // If next is true, runqput puts g in the pp.runnext slot.
    // If the run queue is full, runnext puts g on the global queue.
    // Executed only by the owner P.
    void runqput(struct p* pp, struct g* gp, bool next)
    {
        if(randomizeScheduler && next && randn(2) == 0)
        {
            next = false;
        }
        if(next)
        {
            retryNext:
            auto oldnext = pp->runnext;
            if(! rec::cas(gocpp::recv(pp->runnext), oldnext, guintptr(unsafe::Pointer(gp))))
            {
                goto retryNext;
            }
            if(oldnext == 0)
            {
                return;
            }
            gp = rec::ptr(gocpp::recv(oldnext));
        }
        retry:
        auto h = atomic::LoadAcq(& pp->runqhead);
        auto t = pp->runqtail;
        if(t - h < uint32_t(len(pp->runq)))
        {
            rec::set(gocpp::recv(pp->runq[t % uint32_t(len(pp->runq))]), gp);
            atomic::StoreRel(& pp->runqtail, t + 1);
            return;
        }
        if(runqputslow(pp, gp, h, t))
        {
            return;
        }
        goto retry;
    }

    // Put g and a batch of work from local runnable queue on global queue.
    // Executed only by the owner P.
    bool runqputslow(struct p* pp, struct g* gp, uint32_t h, uint32_t t)
    {
        gocpp::array<g*, len(pp->runq) / 2 + 1> batch = {};
        auto n = t - h;
        n = n / 2;
        if(n != uint32_t(len(pp->runq) / 2))
        {
            go_throw("runqputslow: queue is not full"s);
        }
        for(auto i = uint32_t(0); i < n; i++)
        {
            batch[i] = rec::ptr(gocpp::recv(pp->runq[(h + i) % uint32_t(len(pp->runq))]));
        }
        if(! atomic::CasRel(& pp->runqhead, h, h + n))
        {
            return false;
        }
        batch[n] = gp;
        if(randomizeScheduler)
        {
            for(auto i = uint32_t(1); i <= n; i++)
            {
                auto j = cheaprandn(i + 1);
                std::tie(batch[i], batch[j]) = std::tuple{batch[j], batch[i]};
            }
        }
        for(auto i = uint32_t(0); i < n; i++)
        {
            rec::set(gocpp::recv(batch[i]->schedlink), batch[i + 1]);
        }
        gQueue q = {};
        rec::set(gocpp::recv(q.head), batch[0]);
        rec::set(gocpp::recv(q.tail), batch[n]);
        lock(& sched.lock);
        globrunqputbatch(& q, int32_t(n + 1));
        unlock(& sched.lock);
        return true;
    }

    // runqputbatch tries to put all the G's on q on the local runnable queue.
    // If the queue is full, they are put on the global queue; in that case
    // this will temporarily acquire the scheduler lock.
    // Executed only by the owner P.
    void runqputbatch(struct p* pp, struct gQueue* q, int qsize)
    {
        auto h = atomic::LoadAcq(& pp->runqhead);
        auto t = pp->runqtail;
        auto n = uint32_t(0);
        for(; ! rec::empty(gocpp::recv(q)) && t - h < uint32_t(len(pp->runq)); )
        {
            auto gp = rec::pop(gocpp::recv(q));
            rec::set(gocpp::recv(pp->runq[t % uint32_t(len(pp->runq))]), gp);
            t++;
            n++;
        }
        qsize -= int(n);
        if(randomizeScheduler)
        {
            auto off = [=](uint32_t o) mutable -> uint32_t
            {
                return (pp->runqtail + o) % uint32_t(len(pp->runq));
            };
            for(auto i = uint32_t(1); i < n; i++)
            {
                auto j = cheaprandn(i + 1);
                std::tie(pp->runq[off(i)], pp->runq[off(j)]) = std::tuple{pp->runq[off(j)], pp->runq[off(i)]};
            }
        }
        atomic::StoreRel(& pp->runqtail, t);
        if(! rec::empty(gocpp::recv(q)))
        {
            lock(& sched.lock);
            globrunqputbatch(q, int32_t(qsize));
            unlock(& sched.lock);
        }
    }

    // Get g from local runnable queue.
    // If inheritTime is true, gp should inherit the remaining time in the
    // current time slice. Otherwise, it should start a new time slice.
    // Executed only by the owner P.
    std::tuple<struct g*, bool> runqget(struct p* pp)
    {
        struct g* gp;
        bool inheritTime;
        auto next = pp->runnext;
        if(next != 0 && rec::cas(gocpp::recv(pp->runnext), next, 0))
        {
            return {rec::ptr(gocpp::recv(next)), true};
        }
        for(; ; )
        {
            auto h = atomic::LoadAcq(& pp->runqhead);
            auto t = pp->runqtail;
            if(t == h)
            {
                return {nullptr, false};
            }
            auto gp = rec::ptr(gocpp::recv(pp->runq[h % uint32_t(len(pp->runq))]));
            if(atomic::CasRel(& pp->runqhead, h, h + 1))
            {
                return {gp, false};
            }
        }
    }

    // runqdrain drains the local runnable queue of pp and returns all goroutines in it.
    // Executed only by the owner P.
    std::tuple<struct gQueue, uint32_t> runqdrain(struct p* pp)
    {
        struct gQueue drainQ;
        uint32_t n;
        auto oldNext = pp->runnext;
        if(oldNext != 0 && rec::cas(gocpp::recv(pp->runnext), oldNext, 0))
        {
            rec::pushBack(gocpp::recv(drainQ), rec::ptr(gocpp::recv(oldNext)));
            n++;
        }
        retry:
        auto h = atomic::LoadAcq(& pp->runqhead);
        auto t = pp->runqtail;
        auto qn = t - h;
        if(qn == 0)
        {
            return {drainQ, n};
        }
        if(qn > uint32_t(len(pp->runq)))
        {
            goto retry;
        }
        if(! atomic::CasRel(& pp->runqhead, h, h + qn))
        {
            goto retry;
        }
        for(auto i = uint32_t(0); i < qn; i++)
        {
            auto gp = rec::ptr(gocpp::recv(pp->runq[(h + i) % uint32_t(len(pp->runq))]));
            rec::pushBack(gocpp::recv(drainQ), gp);
            n++;
        }
        return {drainQ, n};
    }

    // Grabs a batch of goroutines from pp's runnable queue into batch.
    // Batch is a ring buffer starting at batchHead.
    // Returns number of grabbed goroutines.
    // Can be executed by any P.
    uint32_t runqgrab(struct p* pp, gocpp::array<golang::runtime::guintptr, 256>* batch, uint32_t batchHead, bool stealRunNextG)
    {
        for(; ; )
        {
            auto h = atomic::LoadAcq(& pp->runqhead);
            auto t = atomic::LoadAcq(& pp->runqtail);
            auto n = t - h;
            n = n - n / 2;
            if(n == 0)
            {
                if(stealRunNextG)
                {
                    if(auto next = pp->runnext; next != 0)
                    {
                        if(pp->status == _Prunning)
                        {
                            if(! osHasLowResTimer)
                            {
                                usleep(3);
                            }
                            else
                            {
                                osyield();
                            }
                        }
                        if(! rec::cas(gocpp::recv(pp->runnext), next, 0))
                        {
                            continue;
                        }
                        batch[batchHead % uint32_t(len(batch))] = next;
                        return 1;
                    }
                }
                return 0;
            }
            if(n > uint32_t(len(pp->runq) / 2))
            {
                continue;
            }
            for(auto i = uint32_t(0); i < n; i++)
            {
                auto g = pp->runq[(h + i) % uint32_t(len(pp->runq))];
                batch[(batchHead + i) % uint32_t(len(batch))] = g;
            }
            if(atomic::CasRel(& pp->runqhead, h, h + n))
            {
                return n;
            }
        }
    }

    // Steal half of elements from local runnable queue of p2
    // and put onto local runnable queue of p.
    // Returns one of the stolen elements (or nil if failed).
    struct g* runqsteal(struct p* pp, struct p* p2, bool stealRunNextG)
    {
        auto t = pp->runqtail;
        auto n = runqgrab(p2, & pp->runq, t, stealRunNextG);
        if(n == 0)
        {
            return nullptr;
        }
        n--;
        auto gp = rec::ptr(gocpp::recv(pp->runq[(t + n) % uint32_t(len(pp->runq))]));
        if(n == 0)
        {
            return gp;
        }
        auto h = atomic::LoadAcq(& pp->runqhead);
        if(t - h + n >= uint32_t(len(pp->runq)))
        {
            go_throw("runqsteal: runq overflow"s);
        }
        atomic::StoreRel(& pp->runqtail, t + n);
        return gp;
    }

    // A gQueue is a dequeue of Gs linked through g.schedlink. A G can only
    // be on one gQueue or gList at a time.
    
    template<typename T> requires gocpp::GoStruct<T>
    gQueue::operator T()
    {
        T result;
        result.head = this->head;
        result.tail = this->tail;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool gQueue::operator==(const T& ref) const
    {
        if (head != ref.head) return false;
        if (tail != ref.tail) return false;
        return true;
    }

    std::ostream& gQueue::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << head;
        os << " " << tail;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct gQueue& value)
    {
        return value.PrintTo(os);
    }

    // empty reports whether q is empty.
    bool rec::empty(struct gQueue* q)
    {
        return q->head == 0;
    }

    // push adds gp to the head of q.
    void rec::push(struct gQueue* q, struct g* gp)
    {
        gp->schedlink = q->head;
        rec::set(gocpp::recv(q->head), gp);
        if(q->tail == 0)
        {
            rec::set(gocpp::recv(q->tail), gp);
        }
    }

    // pushBack adds gp to the tail of q.
    void rec::pushBack(struct gQueue* q, struct g* gp)
    {
        gp->schedlink = 0;
        if(q->tail != 0)
        {
            rec::set(gocpp::recv(rec::ptr(gocpp::recv(q->tail))->schedlink), gp);
        }
        else
        {
            rec::set(gocpp::recv(q->head), gp);
        }
        rec::set(gocpp::recv(q->tail), gp);
    }

    // pushBackAll adds all Gs in q2 to the tail of q. After this q2 must
    // not be used.
    void rec::pushBackAll(struct gQueue* q, struct gQueue q2)
    {
        if(q2.tail == 0)
        {
            return;
        }
        rec::ptr(gocpp::recv(q2.tail))->schedlink = 0;
        if(q->tail != 0)
        {
            rec::ptr(gocpp::recv(q->tail))->schedlink = q2.head;
        }
        else
        {
            q->head = q2.head;
        }
        q->tail = q2.tail;
    }

    // pop removes and returns the head of queue q. It returns nil if
    // q is empty.
    struct g* rec::pop(struct gQueue* q)
    {
        auto gp = rec::ptr(gocpp::recv(q->head));
        if(gp != nullptr)
        {
            q->head = gp->schedlink;
            if(q->head == 0)
            {
                q->tail = 0;
            }
        }
        return gp;
    }

    // popList takes all Gs in q and returns them as a gList.
    struct gList rec::popList(struct gQueue* q)
    {
        auto stack = gList {q->head};
        *q = gQueue {};
        return stack;
    }

    // A gList is a list of Gs linked through g.schedlink. A G can only be
    // on one gQueue or gList at a time.
    
    template<typename T> requires gocpp::GoStruct<T>
    gList::operator T()
    {
        T result;
        result.head = this->head;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool gList::operator==(const T& ref) const
    {
        if (head != ref.head) return false;
        return true;
    }

    std::ostream& gList::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << head;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct gList& value)
    {
        return value.PrintTo(os);
    }

    // empty reports whether l is empty.
    bool rec::empty(struct gList* l)
    {
        return l->head == 0;
    }

    // push adds gp to the head of l.
    void rec::push(struct gList* l, struct g* gp)
    {
        gp->schedlink = l->head;
        rec::set(gocpp::recv(l->head), gp);
    }

    // pushAll prepends all Gs in q to l.
    void rec::pushAll(struct gList* l, struct gQueue q)
    {
        if(! rec::empty(gocpp::recv(q)))
        {
            rec::ptr(gocpp::recv(q.tail))->schedlink = l->head;
            l->head = q.head;
        }
    }

    // pop removes and returns the head of l. If l is empty, it returns nil.
    struct g* rec::pop(struct gList* l)
    {
        auto gp = rec::ptr(gocpp::recv(l->head));
        if(gp != nullptr)
        {
            l->head = gp->schedlink;
        }
        return gp;
    }

    //go:linkname setMaxThreads runtime/debug.setMaxThreads
    int setMaxThreads(int in)
    {
        int out;
        lock(& sched.lock);
        out = int(sched.maxmcount);
        if(in > 0x7fffffff)
        {
            sched.maxmcount = 0x7fffffff;
        }
        else
        {
            sched.maxmcount = int32_t(in);
        }
        checkmcount();
        unlock(& sched.lock);
        return out;
    }

    //go:nosplit
    int procPin()
    {
        auto gp = getg();
        auto mp = gp->m;
        mp->locks++;
        return int(rec::ptr(gocpp::recv(mp->p))->id);
    }

    //go:nosplit
    void procUnpin()
    {
        auto gp = getg();
        gp->m->locks--;
    }

    //go:linkname sync_runtime_procPin sync.runtime_procPin
    //go:nosplit
    int sync_runtime_procPin()
    {
        return procPin();
    }

    //go:linkname sync_runtime_procUnpin sync.runtime_procUnpin
    //go:nosplit
    void sync_runtime_procUnpin()
    {
        procUnpin();
    }

    //go:linkname sync_atomic_runtime_procPin sync/atomic.runtime_procPin
    //go:nosplit
    int sync_atomic_runtime_procPin()
    {
        return procPin();
    }

    //go:linkname sync_atomic_runtime_procUnpin sync/atomic.runtime_procUnpin
    //go:nosplit
    void sync_atomic_runtime_procUnpin()
    {
        procUnpin();
    }

    // Active spinning for sync.Mutex.
    //
    //go:linkname sync_runtime_canSpin sync.runtime_canSpin
    //go:nosplit
    bool sync_runtime_canSpin(int i)
    {
        if(i >= active_spin || ncpu <= 1 || gomaxprocs <= rec::Load(gocpp::recv(sched.npidle)) + rec::Load(gocpp::recv(sched.nmspinning)) + 1)
        {
            return false;
        }
        if(auto p = rec::ptr(gocpp::recv(getg()->m->p)); ! runqempty(p))
        {
            return false;
        }
        return true;
    }

    //go:linkname sync_runtime_doSpin sync.runtime_doSpin
    //go:nosplit
    void sync_runtime_doSpin()
    {
        procyield(active_spin_cnt);
    }

    randomOrder stealOrder;
    // randomOrder/randomEnum are helper types for randomized work stealing.
    // They allow to enumerate all Ps in different pseudo-random orders without repetitions.
    // The algorithm is based on the fact that if we have X such that X and GOMAXPROCS
    // are coprime, then a sequences of (i + X) % GOMAXPROCS gives the required enumeration.
    
    template<typename T> requires gocpp::GoStruct<T>
    randomOrder::operator T()
    {
        T result;
        result.count = this->count;
        result.coprimes = this->coprimes;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool randomOrder::operator==(const T& ref) const
    {
        if (count != ref.count) return false;
        if (coprimes != ref.coprimes) return false;
        return true;
    }

    std::ostream& randomOrder::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << count;
        os << " " << coprimes;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct randomOrder& value)
    {
        return value.PrintTo(os);
    }

    
    template<typename T> requires gocpp::GoStruct<T>
    randomEnum::operator T()
    {
        T result;
        result.i = this->i;
        result.count = this->count;
        result.pos = this->pos;
        result.inc = this->inc;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool randomEnum::operator==(const T& ref) const
    {
        if (i != ref.i) return false;
        if (count != ref.count) return false;
        if (pos != ref.pos) return false;
        if (inc != ref.inc) return false;
        return true;
    }

    std::ostream& randomEnum::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << i;
        os << " " << count;
        os << " " << pos;
        os << " " << inc;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct randomEnum& value)
    {
        return value.PrintTo(os);
    }

    void rec::reset(struct randomOrder* ord, uint32_t count)
    {
        ord->count = count;
        ord->coprimes = ord->coprimes.make_slice(0, 0);
        for(auto i = uint32_t(1); i <= count; i++)
        {
            if(gcd(i, count) == 1)
            {
                ord->coprimes = append(ord->coprimes, i);
            }
        }
    }

    struct randomEnum rec::start(struct randomOrder* ord, uint32_t i)
    {
        return gocpp::Init<randomEnum>([=](auto& x) {
            x.count = ord->count;
            x.pos = i % ord->count;
            x.inc = ord->coprimes[i / ord->count % uint32_t(len(ord->coprimes))];
        });
    }

    bool rec::done(struct randomEnum* go_enum)
    {
        return go_enum->i == go_enum->count;
    }

    void rec::next(struct randomEnum* go_enum)
    {
        go_enum->i++;
        go_enum->pos = (go_enum->pos + go_enum->inc) % go_enum->count;
    }

    uint32_t rec::position(struct randomEnum* go_enum)
    {
        return go_enum->pos;
    }

    uint32_t gcd(uint32_t a, uint32_t b)
    {
        for(; b != 0; )
        {
            std::tie(a, b) = std::tuple{b, a % b};
        }
        return a;
    }

    // An initTask represents the set of initializations that need to be done for a package.
    // Keep in sync with ../../test/noinit.go:initTask
    
    template<typename T> requires gocpp::GoStruct<T>
    initTask::operator T()
    {
        T result;
        result.state = this->state;
        result.nfns = this->nfns;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool initTask::operator==(const T& ref) const
    {
        if (state != ref.state) return false;
        if (nfns != ref.nfns) return false;
        return true;
    }

    std::ostream& initTask::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << state;
        os << " " << nfns;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct initTask& value)
    {
        return value.PrintTo(os);
    }

    // inittrace stores statistics for init functions which are
    // updated by malloc and newproc when active is true.
    tracestat inittrace;
    
    template<typename T> requires gocpp::GoStruct<T>
    tracestat::operator T()
    {
        T result;
        result.active = this->active;
        result.id = this->id;
        result.allocs = this->allocs;
        result.bytes = this->bytes;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool tracestat::operator==(const T& ref) const
    {
        if (active != ref.active) return false;
        if (id != ref.id) return false;
        if (allocs != ref.allocs) return false;
        if (bytes != ref.bytes) return false;
        return true;
    }

    std::ostream& tracestat::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << active;
        os << " " << id;
        os << " " << allocs;
        os << " " << bytes;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct tracestat& value)
    {
        return value.PrintTo(os);
    }

    void doInit(gocpp::slice<initTask*> ts)
    {
        for(auto [gocpp_ignored, t] : ts)
        {
            doInit1(t);
        }
    }

    void doInit1(struct initTask* t)
    {
        //Go switch emulation
        {
            auto condition = t->state;
            int conditionId = -1;
            if(condition == 2) { conditionId = 0; }
            else if(condition == 1) { conditionId = 1; }
            switch(conditionId)
            {
                case 0:
                    return;
                    break;
                case 1:
                    go_throw("recursive call during initialization - linker skew"s);
                    break;
                default:
                    t->state = 1;
                    int64_t start = {};
                    tracestat before = {};
                    if(inittrace.active)
                    {
                        start = nanotime();
                        before = inittrace;
                    }
                    if(t->nfns == 0)
                    {
                        go_throw("inittask with no functions"s);
                    }
                    auto firstFunc = add(unsafe::Pointer(t), 8);
                    for(auto i = uint32_t(0); i < t->nfns; i++)
                    {
                        auto p = add(firstFunc, uintptr_t(i) * goarch::PtrSize);
                        auto f = *(std::function<void ()>*)(unsafe::Pointer(& p));
                        f();
                    }
                    if(inittrace.active)
                    {
                        auto end = nanotime();
                        auto after = inittrace;
                        auto f = *(std::function<void ()>*)(unsafe::Pointer(& firstFunc));
                        auto pkg = funcpkgpath(findfunc(abi::FuncPCABIInternal(f)));
                        gocpp::array<unsigned char, 24> sbuf = {};
                        print("init "s, pkg, " @"s);
                        print(std::string(fmtNSAsMS(sbuf.make_slice(0), uint64_t(start - runtimeInitTime))), " ms, "s);
                        print(std::string(fmtNSAsMS(sbuf.make_slice(0), uint64_t(end - start))), " ms clock, "s);
                        print(std::string(itoa(sbuf.make_slice(0), after.bytes - before.bytes)), " bytes, "s);
                        print(std::string(itoa(sbuf.make_slice(0), after.allocs - before.allocs)), " allocs"s);
                        print("\n"s);
                    }
                    t->state = 2;
                    break;
            }
        }
    }

}

int main()
{
    try
    {
        std::cout << std::boolalpha << std::setprecision(5) << std::fixed;
        golang::runtime::main();
        return 0;
    }
    catch(const gocpp::GoPanic& ex)
    {
        std::cout << "Panic: " << ex.what() << std::endl;
        return -1;
    }
}
