// generated by GoCpp from file '$(ImportDir)/runtime/mprof.go'
#include <complex>
#include <functional>
#include <iostream>
#include <iomanip>
#include <map>
#include <string>
#include <tuple>
#include <vector>

#include "golang/runtime/mprof.h"
#include "gocpp/support.h"

#include "golang/internal/abi/funcpc.h"
#include "golang/internal/abi/symtab.h"
#include "golang/internal/abi/type.h"
#include "golang/internal/chacha8rand/chacha8.h"
#include "golang/runtime/asan0.h"
#include "golang/runtime/cgocall.h"
#include "golang/runtime/chan.h"
#include "golang/runtime/coro.h"
#include "golang/runtime/cputicks.h"
#include "golang/runtime/debuglog_off.h"
#include "golang/runtime/internal/atomic/atomic_amd64.h"
#include "golang/runtime/internal/atomic/types.h"
#include "golang/runtime/internal/sys/consts.h"
#include "golang/runtime/internal/sys/nih.h"
#include "golang/runtime/lock_sema.h"
#include "golang/runtime/lockrank.h"
#include "golang/runtime/lockrank_off.h"
#include "golang/runtime/malloc.h"
#include "golang/runtime/mcache.h"
#include "golang/runtime/mem.h"
#include "golang/runtime/mfinal.h"
#include "golang/runtime/mgc.h"
#include "golang/runtime/mgclimit.h"
#include "golang/runtime/mgcwork.h"
#include "golang/runtime/mheap.h"
#include "golang/runtime/mpagecache.h"
#include "golang/runtime/mranges.h"
#include "golang/runtime/msan0.h"
#include "golang/runtime/mstats.h"
#include "golang/runtime/mwbbuf.h"
#include "golang/runtime/os_windows.h"
#include "golang/runtime/pagetrace_off.h"
#include "golang/runtime/panic.h"
#include "golang/runtime/pinner.h"
#include "golang/runtime/plugin.h"
#include "golang/runtime/print.h"
#include "golang/runtime/proc.h"
#include "golang/runtime/race0.h"
#include "golang/runtime/rand.h"
#include "golang/runtime/runtime.h"
#include "golang/runtime/runtime1.h"
#include "golang/runtime/runtime2.h"
#include "golang/runtime/sema.h"
#include "golang/runtime/signal_windows.h"
#include "golang/runtime/stack.h"
#include "golang/runtime/stkframe.h"
#include "golang/runtime/stubs.h"
#include "golang/runtime/symtab.h"
#include "golang/runtime/time.h"
#include "golang/runtime/time_nofake.h"
#include "golang/runtime/trace2buf.h"
#include "golang/runtime/trace2runtime.h"
#include "golang/runtime/trace2status.h"
#include "golang/runtime/trace2time.h"
#include "golang/runtime/traceback.h"
#include "golang/runtime/type.h"
#include "golang/unsafe/unsafe.h"

namespace golang::runtime
{
    namespace rec
    {
        using namespace mocklib::rec;
        using atomic::rec::Add;
        using atomic::rec::CompareAndSwap;
        using atomic::rec::Load;
        using atomic::rec::Store;
        using atomic::rec::StoreNoWB;
        using atomic::rec::Swap;
    }

    // NOTE(rsc): Everything here could use cas if contention became an issue.
    // profInsertLock protects changes to the start of all *bucket linked lists
    // profBlockLock protects the contents of every blockRecord struct
    // profMemActiveLock protects the active field of every memRecord struct
    // profMemFutureLock is a set of locks that protect the respective elements
    // of the future array of every memRecord struct
    mutex profInsertLock;
    mutex profBlockLock;
    mutex profMemActiveLock;
    gocpp::array<mutex, len(memRecord {}.future)> profMemFutureLock;
    // profile types
    // size of bucket hash table
    // maxStack is the max depth of stack to record in bucket.
    // Note that it's only used internally as a guard against
    // wildly out-of-bounds slicing of the PCs that come after
    // a bucket struct, and it could increase in the future.
    // A bucket holds per-call-stack profiling information.
    // The representation is a bit sleazy, inherited from C.
    // This struct defines the bucket header. It is followed in
    // memory by the stack words and then the actual record
    // data, either a memRecord or a blockRecord.
    //
    // Per-call-stack profiling information.
    // Lookup by hashing call stack into a linked-list hash table.
    //
    // None of the fields in this bucket header are modified after
    // creation, including its next and allnext links.
    //
    // No heap pointers.
    
    template<typename T> requires gocpp::GoStruct<T>
    bucket::operator T()
    {
        T result;
        result._1 = this->_1;
        result.next = this->next;
        result.allnext = this->allnext;
        result.typ = this->typ;
        result.hash = this->hash;
        result.size = this->size;
        result.nstk = this->nstk;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool bucket::operator==(const T& ref) const
    {
        if (_1 != ref._1) return false;
        if (next != ref.next) return false;
        if (allnext != ref.allnext) return false;
        if (typ != ref.typ) return false;
        if (hash != ref.hash) return false;
        if (size != ref.size) return false;
        if (nstk != ref.nstk) return false;
        return true;
    }

    std::ostream& bucket::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << _1;
        os << " " << next;
        os << " " << allnext;
        os << " " << typ;
        os << " " << hash;
        os << " " << size;
        os << " " << nstk;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct bucket& value)
    {
        return value.PrintTo(os);
    }

    // A memRecord is the bucket data for a bucket of type memProfile,
    // part of the memory profile.
    
    template<typename T> requires gocpp::GoStruct<T>
    memRecord::operator T()
    {
        T result;
        result.active = this->active;
        result.future = this->future;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool memRecord::operator==(const T& ref) const
    {
        if (active != ref.active) return false;
        if (future != ref.future) return false;
        return true;
    }

    std::ostream& memRecord::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << active;
        os << " " << future;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct memRecord& value)
    {
        return value.PrintTo(os);
    }

    // memRecordCycle
    
    template<typename T> requires gocpp::GoStruct<T>
    memRecordCycle::operator T()
    {
        T result;
        result.allocs = this->allocs;
        result.frees = this->frees;
        result.alloc_bytes = this->alloc_bytes;
        result.free_bytes = this->free_bytes;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool memRecordCycle::operator==(const T& ref) const
    {
        if (allocs != ref.allocs) return false;
        if (frees != ref.frees) return false;
        if (alloc_bytes != ref.alloc_bytes) return false;
        if (free_bytes != ref.free_bytes) return false;
        return true;
    }

    std::ostream& memRecordCycle::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << allocs;
        os << " " << frees;
        os << " " << alloc_bytes;
        os << " " << free_bytes;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct memRecordCycle& value)
    {
        return value.PrintTo(os);
    }

    // add accumulates b into a. It does not zero b.
    void rec::add(struct memRecordCycle* a, struct memRecordCycle* b)
    {
        a->allocs += b->allocs;
        a->frees += b->frees;
        a->alloc_bytes += b->alloc_bytes;
        a->free_bytes += b->free_bytes;
    }

    // A blockRecord is the bucket data for a bucket of type blockProfile,
    // which is used in blocking and mutex profiles.
    
    template<typename T> requires gocpp::GoStruct<T>
    blockRecord::operator T()
    {
        T result;
        result.count = this->count;
        result.cycles = this->cycles;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool blockRecord::operator==(const T& ref) const
    {
        if (count != ref.count) return false;
        if (cycles != ref.cycles) return false;
        return true;
    }

    std::ostream& blockRecord::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << count;
        os << " " << cycles;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct blockRecord& value)
    {
        return value.PrintTo(os);
    }

    atomic::UnsafePointer mbuckets;
    atomic::UnsafePointer bbuckets;
    atomic::UnsafePointer xbuckets;
    atomic::UnsafePointer buckhash;
    mProfCycleHolder mProfCycle;
    // // *bucket
    // mProfCycleHolder holds the global heap profile cycle number (wrapped at
    // mProfCycleWrap, stored starting at bit 1), and a flag (stored at bit 0) to
    // indicate whether future[cycle] in all buckets has been queued to flush into
    // the active profile.
    
    template<typename T> requires gocpp::GoStruct<T>
    mProfCycleHolder::operator T()
    {
        T result;
        result.value = this->value;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool mProfCycleHolder::operator==(const T& ref) const
    {
        if (value != ref.value) return false;
        return true;
    }

    std::ostream& mProfCycleHolder::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << value;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct mProfCycleHolder& value)
    {
        return value.PrintTo(os);
    }

    // read returns the current cycle count.
    uint32_t rec::read(struct mProfCycleHolder* c)
    {
        uint32_t cycle;
        auto v = rec::Load(gocpp::recv(c->value));
        cycle = v >> 1;
        return cycle;
    }

    // setFlushed sets the flushed flag. It returns the current cycle count and the
    // previous value of the flushed flag.
    std::tuple<uint32_t, bool> rec::setFlushed(struct mProfCycleHolder* c)
    {
        uint32_t cycle;
        bool alreadyFlushed;
        for(; ; )
        {
            auto prev = rec::Load(gocpp::recv(c->value));
            cycle = prev >> 1;
            alreadyFlushed = (prev & 0x1) != 0;
            auto next = prev | 0x1;
            if(rec::CompareAndSwap(gocpp::recv(c->value), prev, next))
            {
                return {cycle, alreadyFlushed};
            }
        }
    }

    // increment increases the cycle count by one, wrapping the value at
    // mProfCycleWrap. It clears the flushed flag.
    void rec::increment(struct mProfCycleHolder* c)
    {
        for(; ; )
        {
            auto prev = rec::Load(gocpp::recv(c->value));
            auto cycle = prev >> 1;
            cycle = (cycle + 1) % mProfCycleWrap;
            auto next = cycle << 1;
            if(rec::CompareAndSwap(gocpp::recv(c->value), prev, next))
            {
                break;
            }
        }
    }

    // newBucket allocates a bucket with the given type and number of stack entries.
    struct bucket* newBucket(golang::runtime::bucketType typ, int nstk)
    {
        auto size = gocpp::Sizeof<bucket>() + uintptr_t(nstk) * gocpp::Sizeof<uintptr_t>();
        //Go switch emulation
        {
            auto condition = typ;
            int conditionId = -1;
            if(condition == memProfile) { conditionId = 0; }
            else if(condition == blockProfile) { conditionId = 1; }
            else if(condition == mutexProfile) { conditionId = 2; }
            switch(conditionId)
            {
                default:
                    go_throw("invalid profile bucket type"s);
                    break;
                case 0:
                    size += gocpp::Sizeof<memRecord>();
                    break;
                case 1:
                case 2:
                    size += gocpp::Sizeof<blockRecord>();
                    break;
            }
        }
        auto b = (bucket*)(persistentalloc(size, 0, & memstats.buckhash_sys));
        b->typ = typ;
        b->nstk = uintptr_t(nstk);
        return b;
    }

    // stk returns the slice in b holding the stack.
    gocpp::slice<uintptr_t> rec::stk(struct bucket* b)
    {
        auto stk = (gocpp::array<uintptr_t, maxStack>*)(add(unsafe::Pointer(b), gocpp::Sizeof<bucket>()));
        if(b->nstk > maxStack)
        {
            go_throw("bad profile stack count"s);
        }
        return stk.make_slice(0, b->nstk, b->nstk);
    }

    // mp returns the memRecord associated with the memProfile bucket b.
    struct memRecord* rec::mp(struct bucket* b)
    {
        if(b->typ != memProfile)
        {
            go_throw("bad use of bucket.mp"s);
        }
        auto data = add(unsafe::Pointer(b), gocpp::Sizeof<bucket>() + b->nstk * gocpp::Sizeof<uintptr_t>());
        return (memRecord*)(data);
    }

    // bp returns the blockRecord associated with the blockProfile bucket b.
    struct blockRecord* rec::bp(struct bucket* b)
    {
        if(b->typ != blockProfile && b->typ != mutexProfile)
        {
            go_throw("bad use of bucket.bp"s);
        }
        auto data = add(unsafe::Pointer(b), gocpp::Sizeof<bucket>() + b->nstk * gocpp::Sizeof<uintptr_t>());
        return (blockRecord*)(data);
    }

    // Return the bucket for stk[0:nstk], allocating new bucket if needed.
    struct bucket* stkbucket(golang::runtime::bucketType typ, uintptr_t size, gocpp::slice<uintptr_t> stk, bool alloc)
    {
        auto bh = (runtime::buckhashArray*)(rec::Load(gocpp::recv(buckhash)));
        if(bh == nullptr)
        {
            lock(& profInsertLock);
            bh = (runtime::buckhashArray*)(rec::Load(gocpp::recv(buckhash)));
            if(bh == nullptr)
            {
                bh = (runtime::buckhashArray*)(sysAlloc(gocpp::Sizeof<runtime::buckhashArray>(), & memstats.buckhash_sys));
                if(bh == nullptr)
                {
                    go_throw("runtime: cannot allocate memory"s);
                }
                rec::StoreNoWB(gocpp::recv(buckhash), unsafe::Pointer(bh));
            }
            unlock(& profInsertLock);
        }
        // Hash stack.
        uintptr_t h = {};
        for(auto [gocpp_ignored, pc] : stk)
        {
            h += pc;
            h += h << 10;
            h ^= h >> 6;
        }
        h += size;
        h += h << 10;
        h ^= h >> 6;
        h += h << 3;
        h ^= h >> 11;
        auto i = int(h % buckHashSize);
        for(auto b = (bucket*)(rec::Load(gocpp::recv(bh[i]))); b != nullptr; b = b->next)
        {
            if(b->typ == typ && b->hash == h && b->size == size && eqslice(rec::stk(gocpp::recv(b)), stk))
            {
                return b;
            }
        }
        if(! alloc)
        {
            return nullptr;
        }
        lock(& profInsertLock);
        for(auto b = (bucket*)(rec::Load(gocpp::recv(bh[i]))); b != nullptr; b = b->next)
        {
            if(b->typ == typ && b->hash == h && b->size == size && eqslice(rec::stk(gocpp::recv(b)), stk))
            {
                unlock(& profInsertLock);
                return b;
            }
        }
        auto b = newBucket(typ, len(stk));
        copy(rec::stk(gocpp::recv(b)), stk);
        b->hash = h;
        b->size = size;
        atomic::UnsafePointer* allnext = {};
        if(typ == memProfile)
        {
            allnext = & mbuckets;
        }
        else
        if(typ == mutexProfile)
        {
            allnext = & xbuckets;
        }
        else
        {
            allnext = & bbuckets;
        }
        b->next = (bucket*)(rec::Load(gocpp::recv(bh[i])));
        b->allnext = (bucket*)(rec::Load(gocpp::recv(allnext)));
        rec::StoreNoWB(gocpp::recv(bh[i]), unsafe::Pointer(b));
        rec::StoreNoWB(gocpp::recv(allnext), unsafe::Pointer(b));
        unlock(& profInsertLock);
        return b;
    }

    bool eqslice(gocpp::slice<uintptr_t> x, gocpp::slice<uintptr_t> y)
    {
        if(len(x) != len(y))
        {
            return false;
        }
        for(auto [i, xi] : x)
        {
            if(xi != y[i])
            {
                return false;
            }
        }
        return true;
    }

    // mProf_NextCycle publishes the next heap profile cycle and creates a
    // fresh heap profile cycle. This operation is fast and can be done
    // during STW. The caller must call mProf_Flush before calling
    // mProf_NextCycle again.
    //
    // This is called by mark termination during STW so allocations and
    // frees after the world is started again count towards a new heap
    // profiling cycle.
    void mProf_NextCycle()
    {
        rec::increment(gocpp::recv(mProfCycle));
    }

    // mProf_Flush flushes the events from the current heap profiling
    // cycle into the active profile. After this it is safe to start a new
    // heap profiling cycle with mProf_NextCycle.
    //
    // This is called by GC after mark termination starts the world. In
    // contrast with mProf_NextCycle, this is somewhat expensive, but safe
    // to do concurrently.
    void mProf_Flush()
    {
        auto [cycle, alreadyFlushed] = rec::setFlushed(gocpp::recv(mProfCycle));
        if(alreadyFlushed)
        {
            return;
        }
        auto index = cycle % uint32_t(len(memRecord {}.future));
        lock(& profMemActiveLock);
        lock(& profMemFutureLock[index]);
        mProf_FlushLocked(index);
        unlock(& profMemFutureLock[index]);
        unlock(& profMemActiveLock);
    }

    // mProf_FlushLocked flushes the events from the heap profiling cycle at index
    // into the active profile. The caller must hold the lock for the active profile
    // (profMemActiveLock) and for the profiling cycle at index
    // (profMemFutureLock[index]).
    void mProf_FlushLocked(uint32_t index)
    {
        assertLockHeld(& profMemActiveLock);
        assertLockHeld(& profMemFutureLock[index]);
        auto head = (bucket*)(rec::Load(gocpp::recv(mbuckets)));
        for(auto b = head; b != nullptr; b = b->allnext)
        {
            auto mp = rec::mp(gocpp::recv(b));
            auto mpc = & mp->future[index];
            rec::add(gocpp::recv(mp->active), mpc);
            *mpc = memRecordCycle {};
        }
    }

    // mProf_PostSweep records that all sweep frees for this GC cycle have
    // completed. This has the effect of publishing the heap profile
    // snapshot as of the last mark termination without advancing the heap
    // profile cycle.
    void mProf_PostSweep()
    {
        auto cycle = rec::read(gocpp::recv(mProfCycle)) + 1;
        auto index = cycle % uint32_t(len(memRecord {}.future));
        lock(& profMemActiveLock);
        lock(& profMemFutureLock[index]);
        mProf_FlushLocked(index);
        unlock(& profMemFutureLock[index]);
        unlock(& profMemActiveLock);
    }

    // Called by malloc to record a profiled block.
    void mProf_Malloc(unsafe::Pointer p, uintptr_t size)
    {
        gocpp::array<uintptr_t, maxStack> stk = {};
        auto nstk = callers(4, stk.make_slice(0));
        auto index = (rec::read(gocpp::recv(mProfCycle)) + 2) % uint32_t(len(memRecord {}.future));
        auto b = stkbucket(memProfile, size, stk.make_slice(0, nstk), true);
        auto mp = rec::mp(gocpp::recv(b));
        auto mpc = & mp->future[index];
        lock(& profMemFutureLock[index]);
        mpc->allocs++;
        mpc->alloc_bytes += size;
        unlock(& profMemFutureLock[index]);
        systemstack([=]() mutable -> void
        {
            setprofilebucket(p, b);
        });
    }

    // Called when freeing a profiled block.
    void mProf_Free(struct bucket* b, uintptr_t size)
    {
        auto index = (rec::read(gocpp::recv(mProfCycle)) + 1) % uint32_t(len(memRecord {}.future));
        auto mp = rec::mp(gocpp::recv(b));
        auto mpc = & mp->future[index];
        lock(& profMemFutureLock[index]);
        mpc->frees++;
        mpc->free_bytes += size;
        unlock(& profMemFutureLock[index]);
    }

    uint64_t blockprofilerate;
    // SetBlockProfileRate controls the fraction of goroutine blocking events
    // that are reported in the blocking profile. The profiler aims to sample
    // an average of one blocking event per rate nanoseconds spent blocked.
    //
    // To include every blocking event in the profile, pass rate = 1.
    // To turn off profiling entirely, pass rate <= 0.
    void SetBlockProfileRate(int rate)
    {
        int64_t r = {};
        if(rate <= 0)
        {
            r = 0;
        }
        else
        if(rate == 1)
        {
            r = 1;
        }
        else
        {
            r = int64_t(double(rate) * double(ticksPerSecond()) / (1000 * 1000 * 1000));
            if(r == 0)
            {
                r = 1;
            }
        }
        atomic::Store64(& blockprofilerate, uint64_t(r));
    }

    void blockevent(int64_t cycles, int skip)
    {
        if(cycles <= 0)
        {
            cycles = 1;
        }
        auto rate = int64_t(atomic::Load64(& blockprofilerate));
        if(blocksampled(cycles, rate))
        {
            saveblockevent(cycles, rate, skip + 1, blockProfile);
        }
    }

    // blocksampled returns true for all events where cycles >= rate. Shorter
    // events have a cycles/rate random chance of returning true.
    bool blocksampled(int64_t cycles, int64_t rate)
    {
        if(rate <= 0 || (rate > cycles && cheaprand64() % rate > cycles))
        {
            return false;
        }
        return true;
    }

    void saveblockevent(int64_t cycles, int64_t rate, int skip, golang::runtime::bucketType which)
    {
        auto gp = getg();
        int nstk = {};
        gocpp::array<uintptr_t, maxStack> stk = {};
        if(gp->m->curg == nullptr || gp->m->curg == gp)
        {
            nstk = callers(skip, stk.make_slice(0));
        }
        else
        {
            nstk = gcallers(gp->m->curg, skip, stk.make_slice(0));
        }
        saveBlockEventStack(cycles, rate, stk.make_slice(0, nstk), which);
    }

    // lockTimer assists with profiling contention on runtime-internal locks.
    //
    // There are several steps between the time that an M experiences contention and
    // when that contention may be added to the profile. This comes from our
    // constraints: We need to keep the critical section of each lock small,
    // especially when those locks are contended. The reporting code cannot acquire
    // new locks until the M has released all other locks, which means no memory
    // allocations and encourages use of (temporary) M-local storage.
    //
    // The M will have space for storing one call stack that caused contention, and
    // for the magnitude of that contention. It will also have space to store the
    // magnitude of additional contention the M caused, since it only has space to
    // remember one call stack and might encounter several contention events before
    // it releases all of its locks and is thus able to transfer the local buffer
    // into the profile.
    //
    // The M will collect the call stack when it unlocks the contended lock. That
    // minimizes the impact on the critical section of the contended lock, and
    // matches the mutex profile's behavior for contention in sync.Mutex: measured
    // at the Unlock method.
    //
    // The profile for contention on sync.Mutex blames the caller of Unlock for the
    // amount of contention experienced by the callers of Lock which had to wait.
    // When there are several critical sections, this allows identifying which of
    // them is responsible.
    //
    // Matching that behavior for runtime-internal locks will require identifying
    // which Ms are blocked on the mutex. The semaphore-based implementation is
    // ready to allow that, but the futex-based implementation will require a bit
    // more work. Until then, we report contention on runtime-internal locks with a
    // call stack taken from the unlock call (like the rest of the user-space
    // "mutex" profile), but assign it a duration value based on how long the
    // previous lock call took (like the user-space "block" profile).
    //
    // Thus, reporting the call stacks of runtime-internal lock contention is
    // guarded by GODEBUG for now. Set GODEBUG=runtimecontentionstacks=1 to enable.
    //
    // TODO(rhysh): plumb through the delay duration, remove GODEBUG, update comment
    //
    // The M will track this by storing a pointer to the lock; lock/unlock pairs for
    // runtime-internal locks are always on the same M.
    //
    // Together, that demands several steps for recording contention. First, when
    // finally acquiring a contended lock, the M decides whether it should plan to
    // profile that event by storing a pointer to the lock in its "to be profiled
    // upon unlock" field. If that field is already set, it uses the relative
    // magnitudes to weight a random choice between itself and the other lock, with
    // the loser's time being added to the "additional contention" field. Otherwise
    // if the M's call stack buffer is occupied, it does the comparison against that
    // sample's magnitude.
    //
    // Second, having unlocked a mutex the M checks to see if it should capture the
    // call stack into its local buffer. Finally, when the M unlocks its last mutex,
    // it transfers the local buffer into the profile. As part of that step, it also
    // transfers any "additional contention" time to the profile. Any lock
    // contention that it experiences while adding samples to the profile will be
    // recorded later as "additional contention" and not include a call stack, to
    // avoid an echo.
    
    template<typename T> requires gocpp::GoStruct<T>
    lockTimer::operator T()
    {
        T result;
        result.lock = this->lock;
        result.timeRate = this->timeRate;
        result.timeStart = this->timeStart;
        result.tickStart = this->tickStart;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool lockTimer::operator==(const T& ref) const
    {
        if (lock != ref.lock) return false;
        if (timeRate != ref.timeRate) return false;
        if (timeStart != ref.timeStart) return false;
        if (tickStart != ref.tickStart) return false;
        return true;
    }

    std::ostream& lockTimer::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << lock;
        os << " " << timeRate;
        os << " " << timeStart;
        os << " " << tickStart;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct lockTimer& value)
    {
        return value.PrintTo(os);
    }

    void rec::begin(struct lockTimer* lt)
    {
        auto rate = int64_t(atomic::Load64(& mutexprofilerate));
        lt->timeRate = gTrackingPeriod;
        if(rate != 0 && rate < lt->timeRate)
        {
            lt->timeRate = rate;
        }
        if(int64_t(cheaprand()) % lt->timeRate == 0)
        {
            lt->timeStart = nanotime();
        }
        if(rate > 0 && int64_t(cheaprand()) % rate == 0)
        {
            lt->tickStart = cputicks();
        }
    }

    void rec::end(struct lockTimer* lt)
    {
        auto gp = getg();
        if(lt->timeStart != 0)
        {
            auto nowTime = nanotime();
            rec::Add(gocpp::recv(gp->m->mLockProfile.waitTime), (nowTime - lt->timeStart) * lt->timeRate);
        }
        if(lt->tickStart != 0)
        {
            auto nowTick = cputicks();
            rec::recordLock(gocpp::recv(gp->m->mLockProfile), nowTick - lt->tickStart, lt->lock);
        }
    }

    
    template<typename T> requires gocpp::GoStruct<T>
    mLockProfile::operator T()
    {
        T result;
        result.waitTime = this->waitTime;
        result.stack = this->stack;
        result.pending = this->pending;
        result.cycles = this->cycles;
        result.cyclesLost = this->cyclesLost;
        result.disabled = this->disabled;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool mLockProfile::operator==(const T& ref) const
    {
        if (waitTime != ref.waitTime) return false;
        if (stack != ref.stack) return false;
        if (pending != ref.pending) return false;
        if (cycles != ref.cycles) return false;
        if (cyclesLost != ref.cyclesLost) return false;
        if (disabled != ref.disabled) return false;
        return true;
    }

    std::ostream& mLockProfile::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << waitTime;
        os << " " << stack;
        os << " " << pending;
        os << " " << cycles;
        os << " " << cyclesLost;
        os << " " << disabled;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct mLockProfile& value)
    {
        return value.PrintTo(os);
    }

    void rec::recordLock(struct mLockProfile* prof, int64_t cycles, struct mutex* l)
    {
        if(cycles <= 0)
        {
            return;
        }
        if(prof->disabled)
        {
            prof->cyclesLost += cycles;
            return;
        }
        if(uintptr_t(unsafe::Pointer(l)) == prof->pending)
        {
            prof->cycles += cycles;
            return;
        }
        if(auto prev = prof->cycles; prev > 0)
        {
            auto prevScore = uint64_t(cheaprand64()) % uint64_t(prev);
            auto thisScore = uint64_t(cheaprand64()) % uint64_t(cycles);
            if(prevScore > thisScore)
            {
                prof->cyclesLost += cycles;
                return;
            }
            else
            {
                prof->cyclesLost += prev;
            }
        }
        prof->pending = uintptr_t(unsafe::Pointer(l));
        prof->cycles = cycles;
    }

    // From unlock2, we might not be holding a p in this code.
    //
    //go:nowritebarrierrec
    void rec::recordUnlock(struct mLockProfile* prof, struct mutex* l)
    {
        if(uintptr_t(unsafe::Pointer(l)) == prof->pending)
        {
            rec::captureStack(gocpp::recv(prof));
        }
        if(auto gp = getg(); gp->m->locks == 1 && gp->m->mLockProfile.cycles != 0)
        {
            rec::store(gocpp::recv(prof));
        }
    }

    void rec::captureStack(struct mLockProfile* prof)
    {
        auto skip = 3;
        if(staticLockRanking)
        {
            skip += 1;
        }
        prof->pending = 0;
        if(rec::Load(gocpp::recv(debug.runtimeContentionStacks)) == 0)
        {
            prof->stack[0] = abi::FuncPCABIInternal(_LostContendedRuntimeLock) + sys::PCQuantum;
            prof->stack[1] = 0;
            return;
        }
        int nstk = {};
        auto gp = getg();
        auto sp = getcallersp();
        auto pc = getcallerpc();
        systemstack([=]() mutable -> void
        {
            unwinder u = {};
            rec::initAt(gocpp::recv(u), pc, sp, 0, gp, unwindSilentErrors | unwindJumpStack);
            nstk = tracebackPCs(& u, skip, prof->stack.make_slice(0));
        });
        if(nstk < len(prof->stack))
        {
            prof->stack[nstk] = 0;
        }
    }

    void rec::store(struct mLockProfile* prof)
    {
        auto mp = acquirem();
        prof->disabled = true;
        auto nstk = maxStack;
        for(auto i = 0; i < nstk; i++)
        {
            if(auto pc = prof->stack[i]; pc == 0)
            {
                nstk = i;
                break;
            }
        }
        auto [cycles, lost] = std::tuple{prof->cycles, prof->cyclesLost};
        std::tie(prof->cycles, prof->cyclesLost) = std::tuple{0, 0};
        auto rate = int64_t(atomic::Load64(& mutexprofilerate));
        saveBlockEventStack(cycles, rate, prof->stack.make_slice(0, nstk), mutexProfile);
        if(lost > 0)
        {
            auto lostStk = gocpp::array<uintptr_t, 1> {abi::FuncPCABIInternal(_LostContendedRuntimeLock) + sys::PCQuantum};
            saveBlockEventStack(lost, rate, lostStk.make_slice(0), mutexProfile);
        }
        prof->disabled = false;
        releasem(mp);
    }

    void saveBlockEventStack(int64_t cycles, int64_t rate, gocpp::slice<uintptr_t> stk, golang::runtime::bucketType which)
    {
        auto b = stkbucket(which, 0, stk, true);
        auto bp = rec::bp(gocpp::recv(b));
        lock(& profBlockLock);
        if(which == blockProfile && cycles < rate)
        {
            bp->count += double(rate) / double(cycles);
            bp->cycles += rate;
        }
        else
        if(which == mutexProfile)
        {
            bp->count += double(rate);
            bp->cycles += rate * cycles;
        }
        else
        {
            bp->count++;
            bp->cycles += cycles;
        }
        unlock(& profBlockLock);
    }

    uint64_t mutexprofilerate;
    // SetMutexProfileFraction controls the fraction of mutex contention events
    // that are reported in the mutex profile. On average 1/rate events are
    // reported. The previous rate is returned.
    //
    // To turn off profiling entirely, pass rate 0.
    // To just read the current rate, pass rate < 0.
    // (For n>1 the details of sampling may change.)
    int SetMutexProfileFraction(int rate)
    {
        if(rate < 0)
        {
            return int(mutexprofilerate);
        }
        auto old = mutexprofilerate;
        atomic::Store64(& mutexprofilerate, uint64_t(rate));
        return int(old);
    }

    //go:linkname mutexevent sync.event
    void mutexevent(int64_t cycles, int skip)
    {
        if(cycles < 0)
        {
            cycles = 0;
        }
        auto rate = int64_t(atomic::Load64(& mutexprofilerate));
        if(rate > 0 && cheaprand64() % rate == 0)
        {
            saveblockevent(cycles, rate, skip + 1, mutexProfile);
        }
    }

    // A StackRecord describes a single execution stack.
    
    template<typename T> requires gocpp::GoStruct<T>
    StackRecord::operator T()
    {
        T result;
        result.Stack0 = this->Stack0;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool StackRecord::operator==(const T& ref) const
    {
        if (Stack0 != ref.Stack0) return false;
        return true;
    }

    std::ostream& StackRecord::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << Stack0;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct StackRecord& value)
    {
        return value.PrintTo(os);
    }

    // Stack returns the stack trace associated with the record,
    // a prefix of r.Stack0.
    gocpp::slice<uintptr_t> rec::Stack(struct StackRecord* r)
    {
        for(auto [i, v] : r->Stack0)
        {
            if(v == 0)
            {
                return r->Stack0.make_slice(0, i);
            }
        }
        return r->Stack0.make_slice(0);
    }

    // MemProfileRate controls the fraction of memory allocations
    // that are recorded and reported in the memory profile.
    // The profiler aims to sample an average of
    // one allocation per MemProfileRate bytes allocated.
    //
    // To include every allocated block in the profile, set MemProfileRate to 1.
    // To turn off profiling entirely, set MemProfileRate to 0.
    //
    // The tools that process the memory profiles assume that the
    // profile rate is constant across the lifetime of the program
    // and equal to the current value. Programs that change the
    // memory profiling rate should do so just once, as early as
    // possible in the execution of the program (for example,
    // at the beginning of main).
    int MemProfileRate = 512 * 1024;
    // disableMemoryProfiling is set by the linker if runtime.MemProfile
    // is not used and the link type guarantees nobody else could use it
    // elsewhere.
    bool disableMemoryProfiling;
    // A MemProfileRecord describes the live objects allocated
    // by a particular call sequence (stack trace).
    
    template<typename T> requires gocpp::GoStruct<T>
    MemProfileRecord::operator T()
    {
        T result;
        result.AllocBytes = this->AllocBytes;
        result.FreeBytes = this->FreeBytes;
        result.AllocObjects = this->AllocObjects;
        result.FreeObjects = this->FreeObjects;
        result.Stack0 = this->Stack0;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool MemProfileRecord::operator==(const T& ref) const
    {
        if (AllocBytes != ref.AllocBytes) return false;
        if (FreeBytes != ref.FreeBytes) return false;
        if (AllocObjects != ref.AllocObjects) return false;
        if (FreeObjects != ref.FreeObjects) return false;
        if (Stack0 != ref.Stack0) return false;
        return true;
    }

    std::ostream& MemProfileRecord::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << AllocBytes;
        os << " " << FreeBytes;
        os << " " << AllocObjects;
        os << " " << FreeObjects;
        os << " " << Stack0;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct MemProfileRecord& value)
    {
        return value.PrintTo(os);
    }

    // InUseBytes returns the number of bytes in use (AllocBytes - FreeBytes).
    int64_t rec::InUseBytes(struct MemProfileRecord* r)
    {
        return r->AllocBytes - r->FreeBytes;
    }

    // InUseObjects returns the number of objects in use (AllocObjects - FreeObjects).
    int64_t rec::InUseObjects(struct MemProfileRecord* r)
    {
        return r->AllocObjects - r->FreeObjects;
    }

    // Stack returns the stack trace associated with the record,
    // a prefix of r.Stack0.
    gocpp::slice<uintptr_t> rec::Stack(struct MemProfileRecord* r)
    {
        for(auto [i, v] : r->Stack0)
        {
            if(v == 0)
            {
                return r->Stack0.make_slice(0, i);
            }
        }
        return r->Stack0.make_slice(0);
    }

    // MemProfile returns a profile of memory allocated and freed per allocation
    // site.
    //
    // MemProfile returns n, the number of records in the current memory profile.
    // If len(p) >= n, MemProfile copies the profile into p and returns n, true.
    // If len(p) < n, MemProfile does not change p and returns n, false.
    //
    // If inuseZero is true, the profile includes allocation records
    // where r.AllocBytes > 0 but r.AllocBytes == r.FreeBytes.
    // These are sites where memory was allocated, but it has all
    // been released back to the runtime.
    //
    // The returned profile may be up to two garbage collection cycles old.
    // This is to avoid skewing the profile toward allocations; because
    // allocations happen in real time but frees are delayed until the garbage
    // collector performs sweeping, the profile only accounts for allocations
    // that have had a chance to be freed by the garbage collector.
    //
    // Most clients should use the runtime/pprof package or
    // the testing package's -test.memprofile flag instead
    // of calling MemProfile directly.
    std::tuple<int, bool> MemProfile(gocpp::slice<MemProfileRecord> p, bool inuseZero)
    {
        int n;
        bool ok;
        auto cycle = rec::read(gocpp::recv(mProfCycle));
        auto index = cycle % uint32_t(len(memRecord {}.future));
        lock(& profMemActiveLock);
        lock(& profMemFutureLock[index]);
        mProf_FlushLocked(index);
        unlock(& profMemFutureLock[index]);
        auto clear = true;
        auto head = (bucket*)(rec::Load(gocpp::recv(mbuckets)));
        for(auto b = head; b != nullptr; b = b->allnext)
        {
            auto mp = rec::mp(gocpp::recv(b));
            if(inuseZero || mp->active.alloc_bytes != mp->active.free_bytes)
            {
                n++;
            }
            if(mp->active.allocs != 0 || mp->active.frees != 0)
            {
                clear = false;
            }
        }
        if(clear)
        {
            n = 0;
            for(auto b = head; b != nullptr; b = b->allnext)
            {
                auto mp = rec::mp(gocpp::recv(b));
                for(auto [c, gocpp_ignored] : mp->future)
                {
                    lock(& profMemFutureLock[c]);
                    rec::add(gocpp::recv(mp->active), & mp->future[c]);
                    mp->future[c] = memRecordCycle {};
                    unlock(& profMemFutureLock[c]);
                }
                if(inuseZero || mp->active.alloc_bytes != mp->active.free_bytes)
                {
                    n++;
                }
            }
        }
        if(n <= len(p))
        {
            ok = true;
            auto idx = 0;
            for(auto b = head; b != nullptr; b = b->allnext)
            {
                auto mp = rec::mp(gocpp::recv(b));
                if(inuseZero || mp->active.alloc_bytes != mp->active.free_bytes)
                {
                    record(& p[idx], b);
                    idx++;
                }
            }
        }
        unlock(& profMemActiveLock);
        return {n, ok};
    }

    // Write b's data to r.
    void record(struct MemProfileRecord* r, struct bucket* b)
    {
        auto mp = rec::mp(gocpp::recv(b));
        r->AllocBytes = int64_t(mp->active.alloc_bytes);
        r->FreeBytes = int64_t(mp->active.free_bytes);
        r->AllocObjects = int64_t(mp->active.allocs);
        r->FreeObjects = int64_t(mp->active.frees);
        if(raceenabled)
        {
            racewriterangepc(unsafe::Pointer(& r->Stack0[0]), gocpp::Sizeof<gocpp::array<uintptr_t, 32>>(), getcallerpc(), abi::FuncPCABIInternal(MemProfile));
        }
        if(msanenabled)
        {
            msanwrite(unsafe::Pointer(& r->Stack0[0]), gocpp::Sizeof<gocpp::array<uintptr_t, 32>>());
        }
        if(asanenabled)
        {
            asanwrite(unsafe::Pointer(& r->Stack0[0]), gocpp::Sizeof<gocpp::array<uintptr_t, 32>>());
        }
        copy(r->Stack0.make_slice(0), rec::stk(gocpp::recv(b)));
        for(auto i = int(b->nstk); i < len(r->Stack0); i++)
        {
            r->Stack0[i] = 0;
        }
    }

    void iterate_memprof(std::function<void (struct bucket* _1, uintptr_t _2, uintptr_t* _3, uintptr_t _4, uintptr_t _5, uintptr_t _6)> fn)
    {
        lock(& profMemActiveLock);
        auto head = (bucket*)(rec::Load(gocpp::recv(mbuckets)));
        for(auto b = head; b != nullptr; b = b->allnext)
        {
            auto mp = rec::mp(gocpp::recv(b));
            fn(b, b->nstk, & rec::stk(gocpp::recv(b))[0], b->size, mp->active.allocs, mp->active.frees);
        }
        unlock(& profMemActiveLock);
    }

    // BlockProfileRecord describes blocking events originated
    // at a particular call sequence (stack trace).
    
    template<typename T> requires gocpp::GoStruct<T>
    BlockProfileRecord::operator T()
    {
        T result;
        result.Count = this->Count;
        result.Cycles = this->Cycles;
        result.StackRecord = this->StackRecord;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool BlockProfileRecord::operator==(const T& ref) const
    {
        if (Count != ref.Count) return false;
        if (Cycles != ref.Cycles) return false;
        if (StackRecord != ref.StackRecord) return false;
        return true;
    }

    std::ostream& BlockProfileRecord::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << Count;
        os << " " << Cycles;
        os << " " << StackRecord;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct BlockProfileRecord& value)
    {
        return value.PrintTo(os);
    }

    // BlockProfile returns n, the number of records in the current blocking profile.
    // If len(p) >= n, BlockProfile copies the profile into p and returns n, true.
    // If len(p) < n, BlockProfile does not change p and returns n, false.
    //
    // Most clients should use the [runtime/pprof] package or
    // the [testing] package's -test.blockprofile flag instead
    // of calling BlockProfile directly.
    std::tuple<int, bool> BlockProfile(gocpp::slice<BlockProfileRecord> p)
    {
        int n;
        bool ok;
        lock(& profBlockLock);
        auto head = (bucket*)(rec::Load(gocpp::recv(bbuckets)));
        for(auto b = head; b != nullptr; b = b->allnext)
        {
            n++;
        }
        if(n <= len(p))
        {
            ok = true;
            for(auto b = head; b != nullptr; b = b->allnext)
            {
                auto bp = rec::bp(gocpp::recv(b));
                auto r = & p[0];
                r->Count = int64_t(bp->count);
                if(r->Count == 0)
                {
                    r->Count = 1;
                }
                r->Cycles = bp->cycles;
                if(raceenabled)
                {
                    racewriterangepc(unsafe::Pointer(& r->Stack0[0]), gocpp::Sizeof<gocpp::array<uintptr_t, 32>>(), getcallerpc(), abi::FuncPCABIInternal(BlockProfile));
                }
                if(msanenabled)
                {
                    msanwrite(unsafe::Pointer(& r->Stack0[0]), gocpp::Sizeof<gocpp::array<uintptr_t, 32>>());
                }
                if(asanenabled)
                {
                    asanwrite(unsafe::Pointer(& r->Stack0[0]), gocpp::Sizeof<gocpp::array<uintptr_t, 32>>());
                }
                auto i = copy(r->Stack0.make_slice(0), rec::stk(gocpp::recv(b)));
                for(; i < len(r->Stack0); i++)
                {
                    r->Stack0[i] = 0;
                }
                p = p.make_slice(1);
            }
        }
        unlock(& profBlockLock);
        return {n, ok};
    }

    // MutexProfile returns n, the number of records in the current mutex profile.
    // If len(p) >= n, MutexProfile copies the profile into p and returns n, true.
    // Otherwise, MutexProfile does not change p, and returns n, false.
    //
    // Most clients should use the [runtime/pprof] package
    // instead of calling MutexProfile directly.
    std::tuple<int, bool> MutexProfile(gocpp::slice<BlockProfileRecord> p)
    {
        int n;
        bool ok;
        lock(& profBlockLock);
        auto head = (bucket*)(rec::Load(gocpp::recv(xbuckets)));
        for(auto b = head; b != nullptr; b = b->allnext)
        {
            n++;
        }
        if(n <= len(p))
        {
            ok = true;
            for(auto b = head; b != nullptr; b = b->allnext)
            {
                auto bp = rec::bp(gocpp::recv(b));
                auto r = & p[0];
                r->Count = int64_t(bp->count);
                r->Cycles = bp->cycles;
                auto i = copy(r->Stack0.make_slice(0), rec::stk(gocpp::recv(b)));
                for(; i < len(r->Stack0); i++)
                {
                    r->Stack0[i] = 0;
                }
                p = p.make_slice(1);
            }
        }
        unlock(& profBlockLock);
        return {n, ok};
    }

    // ThreadCreateProfile returns n, the number of records in the thread creation profile.
    // If len(p) >= n, ThreadCreateProfile copies the profile into p and returns n, true.
    // If len(p) < n, ThreadCreateProfile does not change p and returns n, false.
    //
    // Most clients should use the runtime/pprof package instead
    // of calling ThreadCreateProfile directly.
    std::tuple<int, bool> ThreadCreateProfile(gocpp::slice<StackRecord> p)
    {
        int n;
        bool ok;
        auto first = (m*)(atomic::Loadp(unsafe::Pointer(& allm)));
        for(auto mp = first; mp != nullptr; mp = mp->alllink)
        {
            n++;
        }
        if(n <= len(p))
        {
            ok = true;
            auto i = 0;
            for(auto mp = first; mp != nullptr; mp = mp->alllink)
            {
                p[i].Stack0 = mp->createstack;
                i++;
            }
        }
        return {n, ok};
    }

    //go:linkname runtime_goroutineProfileWithLabels runtime/pprof.runtime_goroutineProfileWithLabels
    std::tuple<int, bool> runtime_goroutineProfileWithLabels(gocpp::slice<StackRecord> p, gocpp::slice<unsafe::Pointer> labels)
    {
        int n;
        bool ok;
        return goroutineProfileWithLabels(p, labels);
    }

    // labels may be nil. If labels is non-nil, it must have the same length as p.
    std::tuple<int, bool> goroutineProfileWithLabels(gocpp::slice<StackRecord> p, gocpp::slice<unsafe::Pointer> labels)
    {
        int n;
        bool ok;
        if(labels != nullptr && len(labels) != len(p))
        {
            labels = nullptr;
        }
        return goroutineProfileWithLabelsConcurrent(p, labels);
    }

    struct gocpp_id_0
    {
        uint32_t sema;
        bool active;
        atomic::Int64 offset;
        gocpp::slice<StackRecord> records;
        gocpp::slice<unsafe::Pointer> labels;

        using isGoStruct = void;

        template<typename T> requires gocpp::GoStruct<T>
        operator T()
        {
            T result;
            result.sema = this->sema;
            result.active = this->active;
            result.offset = this->offset;
            result.records = this->records;
            result.labels = this->labels;
            return result;
        }

        template<typename T> requires gocpp::GoStruct<T>
        bool operator==(const T& ref) const
        {
            if (sema != ref.sema) return false;
            if (active != ref.active) return false;
            if (offset != ref.offset) return false;
            if (records != ref.records) return false;
            if (labels != ref.labels) return false;
            return true;
        }

        std::ostream& PrintTo(std::ostream& os) const
        {
            os << '{';
            os << "" << sema;
            os << " " << active;
            os << " " << offset;
            os << " " << records;
            os << " " << labels;
            os << '}';
            return os;
        }
    };

    std::ostream& operator<<(std::ostream& os, const struct gocpp_id_0& value)
    {
        return value.PrintTo(os);
    }


    gocpp_id_0 goroutineProfile = gocpp::Init<gocpp_id_0>([](auto& x) {
        x.sema = 1;
    });
    // goroutineProfileState indicates the status of a goroutine's stack for the
    // current in-progress goroutine profile. Goroutines' stacks are initially
    // "Absent" from the profile, and end up "Satisfied" by the time the profile is
    // complete. While a goroutine's stack is being captured, its
    // goroutineProfileState will be "InProgress" and it will not be able to run
    // until the capture completes and the state moves to "Satisfied".
    //
    // Some goroutines (the finalizer goroutine, which at various times can be
    // either a "system" or a "user" goroutine, and the goroutine that is
    // coordinating the profile, any goroutines created during the profile) move
    // directly to the "Satisfied" state.
    runtime::goroutineProfileState rec::Load(golang::runtime::goroutineProfileStateHolder* p)
    {
        return goroutineProfileState(rec::Load(gocpp::recv((atomic::Uint32*)(p))));
    }

    void rec::Store(golang::runtime::goroutineProfileStateHolder* p, golang::runtime::goroutineProfileState value)
    {
        rec::Store(gocpp::recv((atomic::Uint32*)(p)), uint32_t(value));
    }

    bool rec::CompareAndSwap(golang::runtime::goroutineProfileStateHolder* p, golang::runtime::goroutineProfileState old, golang::runtime::goroutineProfileState go_new)
    {
        return rec::CompareAndSwap(gocpp::recv((atomic::Uint32*)(p)), uint32_t(old), uint32_t(go_new));
    }

    std::tuple<int, bool> goroutineProfileWithLabelsConcurrent(gocpp::slice<StackRecord> p, gocpp::slice<unsafe::Pointer> labels)
    {
        int n;
        bool ok;
        semacquire(& goroutineProfile.sema);
        auto ourg = getg();
        auto stw = stopTheWorld(stwGoroutineProfile);
        n = int(gcount());
        if(rec::Load(gocpp::recv(fingStatus)) & fingRunningFinalizer != 0)
        {
            n++;
        }
        if(n > len(p))
        {
            startTheWorld(stw);
            semrelease(& goroutineProfile.sema);
            return {n, false};
        }
        auto sp = getcallersp();
        auto pc = getcallerpc();
        systemstack([=]() mutable -> void
        {
            saveg(pc, sp, ourg, & p[0]);
        });
        if(labels != nullptr)
        {
            labels[0] = ourg->labels;
        }
        rec::Store(gocpp::recv(ourg->goroutineProfiled), goroutineProfileSatisfied);
        rec::Store(gocpp::recv(goroutineProfile.offset), 1);
        goroutineProfile.active = true;
        goroutineProfile.records = p;
        goroutineProfile.labels = labels;
        if(fing != nullptr)
        {
            rec::Store(gocpp::recv(fing->goroutineProfiled), goroutineProfileSatisfied);
            if(readgstatus(fing) != _Gdead && ! isSystemGoroutine(fing, false))
            {
                doRecordGoroutineProfile(fing);
            }
        }
        startTheWorld(stw);
        forEachGRace([=](struct g* gp1) mutable -> void
        {
            tryRecordGoroutineProfile(gp1, Gosched);
        });
        stw = stopTheWorld(stwGoroutineProfileCleanup);
        auto endOffset = rec::Swap(gocpp::recv(goroutineProfile.offset), 0);
        goroutineProfile.active = false;
        goroutineProfile.records = nullptr;
        goroutineProfile.labels = nullptr;
        startTheWorld(stw);
        forEachGRace([=](struct g* gp1) mutable -> void
        {
            rec::Store(gocpp::recv(gp1->goroutineProfiled), goroutineProfileAbsent);
        });
        if(raceenabled)
        {
            raceacquire(unsafe::Pointer(& labelSync));
        }
        if(n != int(endOffset))
        {
        }
        semrelease(& goroutineProfile.sema);
        return {n, true};
    }

    // tryRecordGoroutineProfileWB asserts that write barriers are allowed and calls
    // tryRecordGoroutineProfile.
    //
    //go:yeswritebarrierrec
    void tryRecordGoroutineProfileWB(struct g* gp1)
    {
        if(rec::ptr(gocpp::recv(getg()->m->p)) == nullptr)
        {
            go_throw("no P available, write barriers are forbidden"s);
        }
        tryRecordGoroutineProfile(gp1, osyield);
    }

    // tryRecordGoroutineProfile ensures that gp1 has the appropriate representation
    // in the current goroutine profile: either that it should not be profiled, or
    // that a snapshot of its call stack and labels are now in the profile.
    void tryRecordGoroutineProfile(struct g* gp1, std::function<void ()> yield)
    {
        if(readgstatus(gp1) == _Gdead)
        {
            return;
        }
        if(isSystemGoroutine(gp1, true))
        {
            return;
        }
        for(; ; )
        {
            auto prev = rec::Load(gocpp::recv(gp1->goroutineProfiled));
            if(prev == goroutineProfileSatisfied)
            {
                break;
            }
            if(prev == goroutineProfileInProgress)
            {
                yield();
                continue;
            }
            auto mp = acquirem();
            if(rec::CompareAndSwap(gocpp::recv(gp1->goroutineProfiled), goroutineProfileAbsent, goroutineProfileInProgress))
            {
                doRecordGoroutineProfile(gp1);
                rec::Store(gocpp::recv(gp1->goroutineProfiled), goroutineProfileSatisfied);
            }
            releasem(mp);
        }
    }

    // doRecordGoroutineProfile writes gp1's call stack and labels to an in-progress
    // goroutine profile. Preemption is disabled.
    //
    // This may be called via tryRecordGoroutineProfile in two ways: by the
    // goroutine that is coordinating the goroutine profile (running on its own
    // stack), or from the scheduler in preparation to execute gp1 (running on the
    // system stack).
    void doRecordGoroutineProfile(struct g* gp1)
    {
        if(readgstatus(gp1) == _Grunning)
        {
            print("doRecordGoroutineProfile gp1="s, gp1->goid, "\n"s);
            go_throw("cannot read stack of running goroutine"s);
        }
        auto offset = int(rec::Add(gocpp::recv(goroutineProfile.offset), 1)) - 1;
        if(offset >= len(goroutineProfile.records))
        {
            return;
        }
        systemstack([=]() mutable -> void
        {
            saveg(~ uintptr_t(0), ~ uintptr_t(0), gp1, & goroutineProfile.records[offset]);
        });
        if(goroutineProfile.labels != nullptr)
        {
            goroutineProfile.labels[offset] = gp1->labels;
        }
    }

    std::tuple<int, bool> goroutineProfileWithLabelsSync(gocpp::slice<StackRecord> p, gocpp::slice<unsafe::Pointer> labels)
    {
        int n;
        bool ok;
        auto gp = getg();
        auto isOK = [=](struct g* gp1) mutable -> bool
        {
            return gp1 != gp && readgstatus(gp1) != _Gdead && ! isSystemGoroutine(gp1, false);
        };
        auto stw = stopTheWorld(stwGoroutineProfile);
        n = 1;
        forEachGRace([=](struct g* gp1) mutable -> void
        {
            if(isOK(gp1))
            {
                n++;
            }
        });
        if(n <= len(p))
        {
            ok = true;
            auto [r, lbl] = std::tuple{p, labels};
            auto sp = getcallersp();
            auto pc = getcallerpc();
            systemstack([=]() mutable -> void
            {
                saveg(pc, sp, gp, & r[0]);
            });
            r = r.make_slice(1);
            if(labels != nullptr)
            {
                lbl[0] = gp->labels;
                lbl = lbl.make_slice(1);
            }
            forEachGRace([=](struct g* gp1) mutable -> void
            {
                if(! isOK(gp1))
                {
                    return;
                }
                if(len(r) == 0)
                {
                    return;
                }
                systemstack([=]() mutable -> void
                {
                    saveg(~ uintptr_t(0), ~ uintptr_t(0), gp1, & r[0]);
                });
                if(labels != nullptr)
                {
                    lbl[0] = gp1->labels;
                    lbl = lbl.make_slice(1);
                }
                r = r.make_slice(1);
            });
        }
        if(raceenabled)
        {
            raceacquire(unsafe::Pointer(& labelSync));
        }
        startTheWorld(stw);
        return {n, ok};
    }

    // GoroutineProfile returns n, the number of records in the active goroutine stack profile.
    // If len(p) >= n, GoroutineProfile copies the profile into p and returns n, true.
    // If len(p) < n, GoroutineProfile does not change p and returns n, false.
    //
    // Most clients should use the [runtime/pprof] package instead
    // of calling GoroutineProfile directly.
    std::tuple<int, bool> GoroutineProfile(gocpp::slice<StackRecord> p)
    {
        int n;
        bool ok;
        return goroutineProfileWithLabels(p, nullptr);
    }

    void saveg(uintptr_t pc, uintptr_t sp, struct g* gp, struct StackRecord* r)
    {
        unwinder u = {};
        rec::initAt(gocpp::recv(u), pc, sp, 0, gp, unwindSilentErrors);
        auto n = tracebackPCs(& u, 0, r->Stack0.make_slice(0));
        if(n < len(r->Stack0))
        {
            r->Stack0[n] = 0;
        }
    }

    // Stack formats a stack trace of the calling goroutine into buf
    // and returns the number of bytes written to buf.
    // If all is true, Stack formats stack traces of all other goroutines
    // into buf after the trace for the current goroutine.
    int Stack(gocpp::slice<unsigned char> buf, bool all)
    {
        worldStop stw = {};
        if(all)
        {
            stw = stopTheWorld(stwAllGoroutinesStack);
        }
        auto n = 0;
        if(len(buf) > 0)
        {
            auto gp = getg();
            auto sp = getcallersp();
            auto pc = getcallerpc();
            systemstack([=]() mutable -> void
            {
                auto g0 = getg();
                g0->m->traceback = 1;
                g0->writebuf = buf.make_slice(0, 0, len(buf));
                goroutineheader(gp);
                traceback(pc, sp, 0, gp);
                if(all)
                {
                    tracebackothers(gp);
                }
                g0->m->traceback = 0;
                n = len(g0->writebuf);
                g0->writebuf = nullptr;
            });
        }
        if(all)
        {
            startTheWorld(stw);
        }
        return n;
    }

    mutex tracelock;
    void tracealloc(unsafe::Pointer p, uintptr_t size, golang::runtime::_type* typ)
    {
        lock(& tracelock);
        auto gp = getg();
        gp->m->traceback = 2;
        if(typ == nullptr)
        {
            print("tracealloc("s, p, ", "s, hex(size), ")\n"s);
        }
        else
        {
            print("tracealloc("s, p, ", "s, hex(size), ", "s, rec::string(gocpp::recv(toRType(typ))), ")\n"s);
        }
        if(gp->m->curg == nullptr || gp == gp->m->curg)
        {
            goroutineheader(gp);
            auto pc = getcallerpc();
            auto sp = getcallersp();
            systemstack([=]() mutable -> void
            {
                traceback(pc, sp, 0, gp);
            });
        }
        else
        {
            goroutineheader(gp->m->curg);
            traceback(~ uintptr_t(0), ~ uintptr_t(0), 0, gp->m->curg);
        }
        print("\n"s);
        gp->m->traceback = 0;
        unlock(& tracelock);
    }

    void tracefree(unsafe::Pointer p, uintptr_t size)
    {
        lock(& tracelock);
        auto gp = getg();
        gp->m->traceback = 2;
        print("tracefree("s, p, ", "s, hex(size), ")\n"s);
        goroutineheader(gp);
        auto pc = getcallerpc();
        auto sp = getcallersp();
        systemstack([=]() mutable -> void
        {
            traceback(pc, sp, 0, gp);
        });
        print("\n"s);
        gp->m->traceback = 0;
        unlock(& tracelock);
    }

    void tracegc()
    {
        lock(& tracelock);
        auto gp = getg();
        gp->m->traceback = 2;
        print("tracegc()\n"s);
        tracebackothers(gp);
        print("end tracegc\n"s);
        print("\n"s);
        gp->m->traceback = 0;
        unlock(& tracelock);
    }

}

