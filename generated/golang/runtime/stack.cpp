// generated by GoCpp from file '$(ImportDir)/runtime/stack.go'
#include <complex>
#include <functional>
#include <iostream>
#include <iomanip>
#include <map>
#include <string>
#include <tuple>
#include <vector>

#include "golang/runtime/stack.h"
#include "gocpp/support.h"

#include "golang/internal/abi/funcpc.h"
#include "golang/internal/abi/stack.h"
#include "golang/internal/abi/symtab.h"
#include "golang/internal/abi/type.h"
#include "golang/internal/chacha8rand/chacha8.h"
#include "golang/internal/cpu/cpu.h"
#include "golang/internal/cpu/cpu_x86.h"
#include "golang/internal/goarch/goarch.h"
#include "golang/internal/goarch/zgoarch_amd64.h"
#include "golang/internal/goos/zgoos_windows.h"
#include "golang/runtime/asan0.h"
#include "golang/runtime/cgocall.h"
#include "golang/runtime/chan.h"
#include "golang/runtime/coro.h"
#include "golang/runtime/debuglog_off.h"
#include "golang/runtime/extern.h"
#include "golang/runtime/internal/atomic/stubs.h"
#include "golang/runtime/internal/atomic/types.h"
#include "golang/runtime/internal/sys/consts.h"
#include "golang/runtime/internal/sys/intrinsics.h"
#include "golang/runtime/internal/sys/nih.h"
#include "golang/runtime/lock_sema.h"
#include "golang/runtime/lockrank.h"
#include "golang/runtime/lockrank_off.h"
#include "golang/runtime/malloc.h"
#include "golang/runtime/mbitmap.h"
#include "golang/runtime/mbitmap_allocheaders.h"
#include "golang/runtime/mcache.h"
#include "golang/runtime/mcentral.h"
#include "golang/runtime/mcheckmark.h"
#include "golang/runtime/mem.h"
#include "golang/runtime/mfixalloc.h"
#include "golang/runtime/mgc.h"
#include "golang/runtime/mgclimit.h"
#include "golang/runtime/mgcpacer.h"
#include "golang/runtime/mgcscavenge.h"
#include "golang/runtime/mgcwork.h"
#include "golang/runtime/mheap.h"
#include "golang/runtime/mpagealloc.h"
#include "golang/runtime/mpagecache.h"
#include "golang/runtime/mpallocbits.h"
#include "golang/runtime/mprof.h"
#include "golang/runtime/mranges.h"
#include "golang/runtime/msan0.h"
#include "golang/runtime/mspanset.h"
#include "golang/runtime/mstats.h"
#include "golang/runtime/mwbbuf.h"
#include "golang/runtime/os_nonopenbsd.h"
#include "golang/runtime/os_windows.h"
#include "golang/runtime/pagetrace_off.h"
#include "golang/runtime/panic.h"
#include "golang/runtime/pinner.h"
#include "golang/runtime/plugin.h"
#include "golang/runtime/preempt.h"
#include "golang/runtime/print.h"
#include "golang/runtime/proc.h"
#include "golang/runtime/race0.h"
#include "golang/runtime/runtime1.h"
#include "golang/runtime/runtime2.h"
#include "golang/runtime/signal_windows.h"
#include "golang/runtime/sizeclasses.h"
#include "golang/runtime/stkframe.h"
#include "golang/runtime/stubs.h"
#include "golang/runtime/symtab.h"
#include "golang/runtime/sys_x86.h"
#include "golang/runtime/time.h"
#include "golang/runtime/trace2buf.h"
#include "golang/runtime/trace2runtime.h"
#include "golang/runtime/trace2status.h"
#include "golang/runtime/trace2time.h"
#include "golang/runtime/traceback.h"
#include "golang/unsafe/unsafe.h"

namespace golang::runtime
{
    namespace rec
    {
        using namespace mocklib::rec;
        using atomic::rec::Load;
    }

    // stackSystem is a number of additional bytes to add
    // to each stack below the usual guard area for OS-specific
    // purposes like signal handling. Used on Windows, Plan 9,
    // and iOS because they do not use a separate stack.
    // The minimum size of stack used by Go code
    // The minimum stack size to allocate.
    // The hackery here rounds fixedStack0 up to a power of 2.
    // stackNosplit is the maximum number of bytes that a chain of NOSPLIT
    // functions can use.
    // This arithmetic must match that in cmd/internal/objabi/stack.go:StackNosplit.
    // The stack guard is a pointer this many bytes above the
    // bottom of the stack.
    //
    // The guard leaves enough room for a stackNosplit chain of NOSPLIT calls
    // plus one stackSmall frame plus stackSystem bytes for the OS.
    // This arithmetic must match that in cmd/internal/objabi/stack.go:StackLimit.
    // stackDebug == 0: no logging
    //            == 1: logging of per-stack operations
    //            == 2: logging of per-frame operations
    //            == 3: logging of per-word updates
    //            == 4: logging of per-word reads
    // check the BP links during traceback.
    long stackPoisonCopy = 0;
    // Goroutine preemption request.
    // 0xfffffade in hex.
    // Thread is forking. Causes a split stack check failure.
    // 0xfffffb2e in hex.
    // Force a stack movement. Used for debugging.
    // 0xfffffeed in hex.
    // stackPoisonMin is the lowest allowed stack poison value.
    struct gocpp_id_0
    {
        stackpoolItem item;
        gocpp::array<unsigned char, (cpu::CacheLinePadSize - gocpp::Sizeof<stackpoolItem>() % cpu::CacheLinePadSize) % cpu::CacheLinePadSize> _1;

        using isGoStruct = void;

        template<typename T> requires gocpp::GoStruct<T>
        operator T()
        {
            T result;
            result.item = this->item;
            result._1 = this->_1;
            return result;
        }

        template<typename T> requires gocpp::GoStruct<T>
        bool operator==(const T& ref) const
        {
            if (item != ref.item) return false;
            if (_1 != ref._1) return false;
            return true;
        }

        std::ostream& PrintTo(std::ostream& os) const
        {
            os << '{';
            os << "" << item;
            os << " " << _1;
            os << '}';
            return os;
        }
    };

    std::ostream& operator<<(std::ostream& os, const struct gocpp_id_0& value)
    {
        return value.PrintTo(os);
    }


    // Global pool of spans that have free stacks.
    // Stacks are assigned an order according to size.
    //
    //	order = log_2(size/FixedStack)
    //
    // There is a free list for each order.
    gocpp::array<gocpp_id_0, _NumStackOrders> stackpool;
    
    template<typename T> requires gocpp::GoStruct<T>
    stackpoolItem::operator T()
    {
        T result;
        result._1 = this->_1;
        result.mu = this->mu;
        result.span = this->span;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool stackpoolItem::operator==(const T& ref) const
    {
        if (_1 != ref._1) return false;
        if (mu != ref.mu) return false;
        if (span != ref.span) return false;
        return true;
    }

    std::ostream& stackpoolItem::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << _1;
        os << " " << mu;
        os << " " << span;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct stackpoolItem& value)
    {
        return value.PrintTo(os);
    }

    struct gocpp_id_1
    {
        mutex lock;
        gocpp::array<mSpanList, heapAddrBits - pageShift> free;

        using isGoStruct = void;

        template<typename T> requires gocpp::GoStruct<T>
        operator T()
        {
            T result;
            result.lock = this->lock;
            result.free = this->free;
            return result;
        }

        template<typename T> requires gocpp::GoStruct<T>
        bool operator==(const T& ref) const
        {
            if (lock != ref.lock) return false;
            if (free != ref.free) return false;
            return true;
        }

        std::ostream& PrintTo(std::ostream& os) const
        {
            os << '{';
            os << "" << lock;
            os << " " << free;
            os << '}';
            return os;
        }
    };

    std::ostream& operator<<(std::ostream& os, const struct gocpp_id_1& value)
    {
        return value.PrintTo(os);
    }


    // Global pool of large stack spans.
    gocpp_id_1 stackLarge;
    void stackinit()
    {
        if(_StackCacheSize & _PageMask != 0)
        {
            go_throw("cache size must be a multiple of page size"_s);
        }
        for(auto [i, gocpp_ignored] : stackpool)
        {
            rec::init(gocpp::recv(stackpool[i].item.span));
            lockInit(& stackpool[i].item.mu, lockRankStackpool);
        }
        for(auto [i, gocpp_ignored] : stackLarge.free)
        {
            rec::init(gocpp::recv(stackLarge.free[i]));
            lockInit(& stackLarge.lock, lockRankStackLarge);
        }
    }

    // stacklog2 returns ⌊log_2(n)⌋.
    int stacklog2(uintptr_t n)
    {
        auto log2 = 0;
        for(; n > 1; )
        {
            n >>= 1;
            log2++;
        }
        return log2;
    }

    // Allocates a stack from the free pool. Must be called with
    // stackpool[order].item.mu held.
    runtime::gclinkptr stackpoolalloc(uint8_t order)
    {
        auto list = & stackpool[order].item.span;
        auto s = list->first;
        lockWithRankMayAcquire(& mheap_.lock, lockRankMheap);
        if(s == nullptr)
        {
            s = rec::allocManual(gocpp::recv(mheap_), _StackCacheSize >> _PageShift, spanAllocStack);
            if(s == nullptr)
            {
                go_throw("out of memory"_s);
            }
            if(s->allocCount != 0)
            {
                go_throw("bad allocCount"_s);
            }
            if(rec::ptr(gocpp::recv(s->manualFreeList)) != nullptr)
            {
                go_throw("bad manualFreeList"_s);
            }
            osStackAlloc(s);
            s->elemsize = fixedStack << order;
            for(auto i = uintptr_t(0); i < _StackCacheSize; i += s->elemsize)
            {
                auto x = gclinkptr(rec::base(gocpp::recv(s)) + i);
                rec::ptr(gocpp::recv(x))->next = s->manualFreeList;
                s->manualFreeList = x;
            }
            rec::insert(gocpp::recv(list), s);
        }
        auto x = s->manualFreeList;
        if(rec::ptr(gocpp::recv(x)) == nullptr)
        {
            go_throw("span has no free stacks"_s);
        }
        s->manualFreeList = rec::ptr(gocpp::recv(x))->next;
        s->allocCount++;
        if(rec::ptr(gocpp::recv(s->manualFreeList)) == nullptr)
        {
            rec::remove(gocpp::recv(list), s);
        }
        return x;
    }

    // Adds stack x to the free pool. Must be called with stackpool[order].item.mu held.
    void stackpoolfree(golang::runtime::gclinkptr x, uint8_t order)
    {
        auto s = spanOfUnchecked(uintptr_t(x));
        if(rec::get(gocpp::recv(s->state)) != mSpanManual)
        {
            go_throw("freeing stack not in a stack span"_s);
        }
        if(rec::ptr(gocpp::recv(s->manualFreeList)) == nullptr)
        {
            rec::insert(gocpp::recv(stackpool[order].item.span), s);
        }
        rec::ptr(gocpp::recv(x))->next = s->manualFreeList;
        s->manualFreeList = x;
        s->allocCount--;
        if(gcphase == _GCoff && s->allocCount == 0)
        {
            rec::remove(gocpp::recv(stackpool[order].item.span), s);
            s->manualFreeList = 0;
            osStackFree(s);
            rec::freeManual(gocpp::recv(mheap_), s, spanAllocStack);
        }
    }

    // stackcacherefill/stackcacherelease implement a global pool of stack segments.
    // The pool is required to prevent unlimited growth of per-thread caches.
    //
    //go:systemstack
    void stackcacherefill(struct mcache* c, uint8_t order)
    {
        if(stackDebug >= 1)
        {
            print("stackcacherefill order="_s, order, "\n"_s);
        }
        // Grab some stacks from the global cache.
        // Grab half of the allowed capacity (to prevent thrashing).
        runtime::gclinkptr list = {};
        uintptr_t size = {};
        lock(& stackpool[order].item.mu);
        for(; size < _StackCacheSize / 2; )
        {
            auto x = stackpoolalloc(order);
            rec::ptr(gocpp::recv(x))->next = list;
            list = x;
            size += fixedStack << order;
        }
        unlock(& stackpool[order].item.mu);
        c->stackcache[order].list = list;
        c->stackcache[order].size = size;
    }

    //go:systemstack
    void stackcacherelease(struct mcache* c, uint8_t order)
    {
        if(stackDebug >= 1)
        {
            print("stackcacherelease order="_s, order, "\n"_s);
        }
        auto x = c->stackcache[order].list;
        auto size = c->stackcache[order].size;
        lock(& stackpool[order].item.mu);
        for(; size > _StackCacheSize / 2; )
        {
            auto y = rec::ptr(gocpp::recv(x))->next;
            stackpoolfree(x, order);
            x = y;
            size -= fixedStack << order;
        }
        unlock(& stackpool[order].item.mu);
        c->stackcache[order].list = x;
        c->stackcache[order].size = size;
    }

    //go:systemstack
    void stackcache_clear(struct mcache* c)
    {
        if(stackDebug >= 1)
        {
            print("stackcache clear\n"_s);
        }
        for(auto order = uint8_t(0); order < _NumStackOrders; order++)
        {
            lock(& stackpool[order].item.mu);
            auto x = c->stackcache[order].list;
            for(; rec::ptr(gocpp::recv(x)) != nullptr; )
            {
                auto y = rec::ptr(gocpp::recv(x))->next;
                stackpoolfree(x, order);
                x = y;
            }
            c->stackcache[order].list = 0;
            c->stackcache[order].size = 0;
            unlock(& stackpool[order].item.mu);
        }
    }

    // stackalloc allocates an n byte stack.
    //
    // stackalloc must run on the system stack because it uses per-P
    // resources and must not split the stack.
    //
    //go:systemstack
    struct stack stackalloc(uint32_t n)
    {
        auto thisg = getg();
        if(thisg != thisg->m->g0)
        {
            go_throw("stackalloc not on scheduler stack"_s);
        }
        if(n & (n - 1) != 0)
        {
            go_throw("stack size not a power of 2"_s);
        }
        if(stackDebug >= 1)
        {
            print("stackalloc "_s, n, "\n"_s);
        }
        if(debug.efence != 0 || stackFromSystem != 0)
        {
            n = uint32_t(alignUp(uintptr_t(n), physPageSize));
            auto v = sysAlloc(uintptr_t(n), & memstats.stacks_sys);
            if(v == nullptr)
            {
                go_throw("out of memory (stackalloc)"_s);
            }
            return stack {uintptr_t(v), uintptr_t(v) + uintptr_t(n)};
        }
        // Small stacks are allocated with a fixed-size free-list allocator.
        // If we need a stack of a bigger size, we fall back on allocating
        // a dedicated span.
        unsafe::Pointer v = {};
        if(n < (fixedStack << _NumStackOrders) && n < _StackCacheSize)
        {
            auto order = uint8_t(0);
            auto n2 = n;
            for(; n2 > fixedStack; )
            {
                order++;
                n2 >>= 1;
            }
            runtime::gclinkptr x = {};
            if(stackNoCache != 0 || thisg->m->p == 0 || thisg->m->preemptoff != ""_s)
            {
                lock(& stackpool[order].item.mu);
                x = stackpoolalloc(order);
                unlock(& stackpool[order].item.mu);
            }
            else
            {
                auto c = rec::ptr(gocpp::recv(thisg->m->p))->mcache;
                x = c->stackcache[order].list;
                if(rec::ptr(gocpp::recv(x)) == nullptr)
                {
                    stackcacherefill(c, order);
                    x = c->stackcache[order].list;
                }
                c->stackcache[order].list = rec::ptr(gocpp::recv(x))->next;
                c->stackcache[order].size -= uintptr_t(n);
            }
            v = unsafe::Pointer(x);
        }
        else
        {
            mspan* s = {};
            auto npage = uintptr_t(n) >> _PageShift;
            auto log2npage = stacklog2(npage);
            lock(& stackLarge.lock);
            if(! rec::isEmpty(gocpp::recv(stackLarge.free[log2npage])))
            {
                s = stackLarge.free[log2npage].first;
                rec::remove(gocpp::recv(stackLarge.free[log2npage]), s);
            }
            unlock(& stackLarge.lock);
            lockWithRankMayAcquire(& mheap_.lock, lockRankMheap);
            if(s == nullptr)
            {
                s = rec::allocManual(gocpp::recv(mheap_), npage, spanAllocStack);
                if(s == nullptr)
                {
                    go_throw("out of memory"_s);
                }
                osStackAlloc(s);
                s->elemsize = uintptr_t(n);
            }
            v = unsafe::Pointer(rec::base(gocpp::recv(s)));
        }
        if(raceenabled)
        {
            racemalloc(v, uintptr_t(n));
        }
        if(msanenabled)
        {
            msanmalloc(v, uintptr_t(n));
        }
        if(asanenabled)
        {
            asanunpoison(v, uintptr_t(n));
        }
        if(stackDebug >= 1)
        {
            print("  allocated "_s, v, "\n"_s);
        }
        return stack {uintptr_t(v), uintptr_t(v) + uintptr_t(n)};
    }

    // stackfree frees an n byte stack allocation at stk.
    //
    // stackfree must run on the system stack because it uses per-P
    // resources and must not split the stack.
    //
    //go:systemstack
    void stackfree(struct stack stk)
    {
        auto gp = getg();
        auto v = unsafe::Pointer(stk.lo);
        auto n = stk.hi - stk.lo;
        if(n & (n - 1) != 0)
        {
            go_throw("stack not a power of 2"_s);
        }
        if(stk.lo + n < stk.hi)
        {
            go_throw("bad stack size"_s);
        }
        if(stackDebug >= 1)
        {
            println("stackfree"_s, v, n);
            memclrNoHeapPointers(v, n);
        }
        if(debug.efence != 0 || stackFromSystem != 0)
        {
            if(debug.efence != 0 || stackFaultOnFree != 0)
            {
                sysFault(v, n);
            }
            else
            {
                sysFree(v, n, & memstats.stacks_sys);
            }
            return;
        }
        if(msanenabled)
        {
            msanfree(v, n);
        }
        if(asanenabled)
        {
            asanpoison(v, n);
        }
        if(n < (fixedStack << _NumStackOrders) && n < _StackCacheSize)
        {
            auto order = uint8_t(0);
            auto n2 = n;
            for(; n2 > fixedStack; )
            {
                order++;
                n2 >>= 1;
            }
            auto x = gclinkptr(v);
            if(stackNoCache != 0 || gp->m->p == 0 || gp->m->preemptoff != ""_s)
            {
                lock(& stackpool[order].item.mu);
                stackpoolfree(x, order);
                unlock(& stackpool[order].item.mu);
            }
            else
            {
                auto c = rec::ptr(gocpp::recv(gp->m->p))->mcache;
                if(c->stackcache[order].size >= _StackCacheSize)
                {
                    stackcacherelease(c, order);
                }
                rec::ptr(gocpp::recv(x))->next = c->stackcache[order].list;
                c->stackcache[order].list = x;
                c->stackcache[order].size += n;
            }
        }
        else
        {
            auto s = spanOfUnchecked(uintptr_t(v));
            if(rec::get(gocpp::recv(s->state)) != mSpanManual)
            {
                println(hex(rec::base(gocpp::recv(s))), v);
                go_throw("bad span state"_s);
            }
            if(gcphase == _GCoff)
            {
                osStackFree(s);
                rec::freeManual(gocpp::recv(mheap_), s, spanAllocStack);
            }
            else
            {
                auto log2npage = stacklog2(s->npages);
                lock(& stackLarge.lock);
                rec::insert(gocpp::recv(stackLarge.free[log2npage]), s);
                unlock(& stackLarge.lock);
            }
        }
    }

    uintptr_t maxstacksize = 1 << 20;
    uintptr_t maxstackceiling = maxstacksize;
    gocpp::slice<gocpp::string> ptrnames = gocpp::Init<gocpp::slice<gocpp::string>>([](auto& x) {
        x[0] = "scalar"_s;
        x[1] = "ptr"_s;
    });
    
    template<typename T> requires gocpp::GoStruct<T>
    adjustinfo::operator T()
    {
        T result;
        result.old = this->old;
        result.delta = this->delta;
        result.sghi = this->sghi;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool adjustinfo::operator==(const T& ref) const
    {
        if (old != ref.old) return false;
        if (delta != ref.delta) return false;
        if (sghi != ref.sghi) return false;
        return true;
    }

    std::ostream& adjustinfo::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << old;
        os << " " << delta;
        os << " " << sghi;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct adjustinfo& value)
    {
        return value.PrintTo(os);
    }

    // adjustpointer checks whether *vpp is in the old stack described by adjinfo.
    // If so, it rewrites *vpp to point into the new stack.
    void adjustpointer(struct adjustinfo* adjinfo, unsafe::Pointer vpp)
    {
        auto pp = (uintptr_t*)(vpp);
        auto p = *pp;
        if(stackDebug >= 4)
        {
            print("        "_s, pp, ":"_s, hex(p), "\n"_s);
        }
        if(adjinfo->old.lo <= p && p < adjinfo->old.hi)
        {
            *pp = p + adjinfo->delta;
            if(stackDebug >= 3)
            {
                print("        adjust ptr "_s, pp, ":"_s, hex(p), " -> "_s, hex(*pp), "\n"_s);
            }
        }
    }

    // Information from the compiler about the layout of stack frames.
    // Note: this type must agree with reflect.bitVector.
    
    template<typename T> requires gocpp::GoStruct<T>
    bitvector::operator T()
    {
        T result;
        result.n = this->n;
        result.bytedata = this->bytedata;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool bitvector::operator==(const T& ref) const
    {
        if (n != ref.n) return false;
        if (bytedata != ref.bytedata) return false;
        return true;
    }

    std::ostream& bitvector::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << n;
        os << " " << bytedata;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct bitvector& value)
    {
        return value.PrintTo(os);
    }

    // ptrbit returns the i'th bit in bv.
    // ptrbit is less efficient than iterating directly over bitvector bits,
    // and should only be used in non-performance-critical code.
    // See adjustpointers for an example of a high-efficiency walk of a bitvector.
    uint8_t rec::ptrbit(golang::runtime::bitvector* bv, uintptr_t i)
    {
        auto b = *(addb(bv->bytedata, i / 8));
        return (b >> (i % 8)) & 1;
    }

    // bv describes the memory starting at address scanp.
    // Adjust any pointers contained therein.
    void adjustpointers(unsafe::Pointer scanp, struct bitvector* bv, struct adjustinfo* adjinfo, struct funcInfo f)
    {
        auto minp = adjinfo->old.lo;
        auto maxp = adjinfo->old.hi;
        auto delta = adjinfo->delta;
        auto num = uintptr_t(bv->n);
        auto useCAS = uintptr_t(scanp) < adjinfo->sghi;
        for(auto i = uintptr_t(0); i < num; i += 8)
        {
            if(stackDebug >= 4)
            {
                for(auto j = uintptr_t(0); j < 8; j++)
                {
                    print("        "_s, add(scanp, (i + j) * goarch::PtrSize), ":"_s, ptrnames[rec::ptrbit(gocpp::recv(bv), i + j)], ":"_s, hex(*(uintptr_t*)(add(scanp, (i + j) * goarch::PtrSize))), " # "_s, i, " "_s, *addb(bv->bytedata, i / 8), "\n"_s);
                }
            }
            auto b = *(addb(bv->bytedata, i / 8));
            for(; b != 0; )
            {
                auto j = uintptr_t(sys::TrailingZeros8(b));
                b &= b - 1;
                auto pp = (uintptr_t*)(add(scanp, (i + j) * goarch::PtrSize));
                retry:
                auto p = *pp;
                if(rec::valid(gocpp::recv(f)) && 0 < p && p < minLegalPointer && debug.invalidptr != 0)
                {
                    getg()->m->traceback = 2;
                    print("runtime: bad pointer in frame "_s, funcname(f), " at "_s, pp, ": "_s, hex(p), "\n"_s);
                    go_throw("invalid pointer found on stack"_s);
                }
                if(minp <= p && p < maxp)
                {
                    if(stackDebug >= 3)
                    {
                        print("adjust ptr "_s, hex(p), " "_s, funcname(f), "\n"_s);
                    }
                    if(useCAS)
                    {
                        auto ppu = (unsafe::Pointer*)(unsafe::Pointer(pp));
                        if(! atomic::Casp1(ppu, unsafe::Pointer(p), unsafe::Pointer(p + delta)))
                        {
                            goto retry;
                        }
                    }
                    else
                    {
                        *pp = p + delta;
                    }
                }
            }
        }
    }

    // Note: the argument/return area is adjusted by the callee.
    void adjustframe(struct stkframe* frame, struct adjustinfo* adjinfo)
    {
        if(frame->continpc == 0)
        {
            return;
        }
        auto f = frame->fn;
        if(stackDebug >= 2)
        {
            print("    adjusting "_s, funcname(f), " frame=["_s, hex(frame->sp), ","_s, hex(frame->fp), "] pc="_s, hex(frame->pc), " continpc="_s, hex(frame->continpc), "\n"_s);
        }
        if((goarch::ArchFamily == goarch::AMD64 || goarch::ArchFamily == goarch::ARM64) && frame->argp - frame->varp == 2 * goarch::PtrSize)
        {
            if(stackDebug >= 3)
            {
                print("      saved bp\n"_s);
            }
            if(debugCheckBP)
            {
                auto bp = *(uintptr_t*)(unsafe::Pointer(frame->varp));
                if(bp != 0 && (bp < adjinfo->old.lo || bp >= adjinfo->old.hi))
                {
                    println("runtime: found invalid frame pointer"_s);
                    print("bp="_s, hex(bp), " min="_s, hex(adjinfo->old.lo), " max="_s, hex(adjinfo->old.hi), "\n"_s);
                    go_throw("bad frame pointer"_s);
                }
            }
            adjustpointer(adjinfo, unsafe::Pointer(frame->varp));
        }
        auto [locals, args, objs] = rec::getStackMap(gocpp::recv(frame), true);
        if(locals.n > 0)
        {
            auto size = uintptr_t(locals.n) * goarch::PtrSize;
            adjustpointers(unsafe::Pointer(frame->varp - size), & locals, adjinfo, f);
        }
        if(args.n > 0)
        {
            if(stackDebug >= 3)
            {
                print("      args\n"_s);
            }
            adjustpointers(unsafe::Pointer(frame->argp), & args, adjinfo, funcInfo {});
        }
        if(frame->varp != 0)
        {
            for(auto [i, gocpp_ignored] : objs)
            {
                auto obj = & objs[i];
                auto off = obj->off;
                auto base = frame->varp;
                if(off >= 0)
                {
                    base = frame->argp;
                }
                auto p = base + uintptr_t(off);
                if(p < frame->sp)
                {
                    continue;
                }
                auto ptrdata = rec::ptrdata(gocpp::recv(obj));
                auto gcdata = rec::gcdata(gocpp::recv(obj));
                mspan* s = {};
                if(rec::useGCProg(gocpp::recv(obj)))
                {
                    s = materializeGCProg(ptrdata, gcdata);
                    gcdata = (unsigned char*)(unsafe::Pointer(s->startAddr));
                }
                for(auto i = uintptr_t(0); i < ptrdata; i += goarch::PtrSize)
                {
                    if((*addb(gcdata, i / (8 * goarch::PtrSize)) >> (i / goarch::PtrSize & 7)) & 1 != 0)
                    {
                        adjustpointer(adjinfo, unsafe::Pointer(p + i));
                    }
                }
                if(s != nullptr)
                {
                    dematerializeGCProg(s);
                }
            }
        }
    }

    void adjustctxt(struct g* gp, struct adjustinfo* adjinfo)
    {
        adjustpointer(adjinfo, unsafe::Pointer(& gp->sched.ctxt));
        if(! framepointer_enabled)
        {
            return;
        }
        if(debugCheckBP)
        {
            auto bp = gp->sched.bp;
            if(bp != 0 && (bp < adjinfo->old.lo || bp >= adjinfo->old.hi))
            {
                println("runtime: found invalid top frame pointer"_s);
                print("bp="_s, hex(bp), " min="_s, hex(adjinfo->old.lo), " max="_s, hex(adjinfo->old.hi), "\n"_s);
                go_throw("bad top frame pointer"_s);
            }
        }
        auto oldfp = gp->sched.bp;
        adjustpointer(adjinfo, unsafe::Pointer(& gp->sched.bp));
        if(GOARCH == "arm64"_s)
        {
            if(oldfp == gp->sched.sp - goarch::PtrSize)
            {
                memmove(unsafe::Pointer(gp->sched.bp), unsafe::Pointer(oldfp), goarch::PtrSize);
                adjustpointer(adjinfo, unsafe::Pointer(gp->sched.bp));
            }
        }
    }

    void adjustdefers(struct g* gp, struct adjustinfo* adjinfo)
    {
        adjustpointer(adjinfo, unsafe::Pointer(& gp->_defer));
        for(auto d = gp->_defer; d != nullptr; d = d->link)
        {
            adjustpointer(adjinfo, unsafe::Pointer(& d->fn));
            adjustpointer(adjinfo, unsafe::Pointer(& d->sp));
            adjustpointer(adjinfo, unsafe::Pointer(& d->link));
        }
    }

    void adjustpanics(struct g* gp, struct adjustinfo* adjinfo)
    {
        adjustpointer(adjinfo, unsafe::Pointer(& gp->_panic));
    }

    void adjustsudogs(struct g* gp, struct adjustinfo* adjinfo)
    {
        for(auto s = gp->waiting; s != nullptr; s = s->waitlink)
        {
            adjustpointer(adjinfo, unsafe::Pointer(& s->elem));
        }
    }

    void fillstack(struct stack stk, unsigned char b)
    {
        for(auto p = stk.lo; p < stk.hi; p++)
        {
            *(unsigned char*)(unsafe::Pointer(p)) = b;
        }
    }

    uintptr_t findsghi(struct g* gp, struct stack stk)
    {
        uintptr_t sghi = {};
        for(auto sg = gp->waiting; sg != nullptr; sg = sg->waitlink)
        {
            auto p = uintptr_t(sg->elem) + uintptr_t(sg->c->elemsize);
            if(stk.lo <= p && p < stk.hi && p > sghi)
            {
                sghi = p;
            }
        }
        return sghi;
    }

    // syncadjustsudogs adjusts gp's sudogs and copies the part of gp's
    // stack they refer to while synchronizing with concurrent channel
    // operations. It returns the number of bytes of stack copied.
    uintptr_t syncadjustsudogs(struct g* gp, uintptr_t used, struct adjustinfo* adjinfo)
    {
        if(gp->waiting == nullptr)
        {
            return 0;
        }
        // Lock channels to prevent concurrent send/receive.
        hchan* lastc = {};
        for(auto sg = gp->waiting; sg != nullptr; sg = sg->waitlink)
        {
            if(sg->c != lastc)
            {
                lockWithRank(& sg->c->lock, lockRankHchanLeaf);
            }
            lastc = sg->c;
        }
        adjustsudogs(gp, adjinfo);
        // Copy the part of the stack the sudogs point in to
        // while holding the lock to prevent races on
        // send/receive slots.
        uintptr_t sgsize = {};
        if(adjinfo->sghi != 0)
        {
            auto oldBot = adjinfo->old.hi - used;
            auto newBot = oldBot + adjinfo->delta;
            sgsize = adjinfo->sghi - oldBot;
            memmove(unsafe::Pointer(newBot), unsafe::Pointer(oldBot), sgsize);
        }
        lastc = nullptr;
        for(auto sg = gp->waiting; sg != nullptr; sg = sg->waitlink)
        {
            if(sg->c != lastc)
            {
                unlock(& sg->c->lock);
            }
            lastc = sg->c;
        }
        return sgsize;
    }

    // Copies gp's stack to a new stack of a different size.
    // Caller must have changed gp status to Gcopystack.
    void copystack(struct g* gp, uintptr_t newsize)
    {
        if(gp->syscallsp != 0)
        {
            go_throw("stack growth not allowed in system call"_s);
        }
        auto old = gp->stack;
        if(old.lo == 0)
        {
            go_throw("nil stackbase"_s);
        }
        auto used = old.hi - gp->sched.sp;
        rec::addScannableStack(gocpp::recv(gcController), rec::ptr(gocpp::recv(getg()->m->p)), int64_t(newsize) - int64_t(old.hi - old.lo));
        auto go_new = stackalloc(uint32_t(newsize));
        if(stackPoisonCopy != 0)
        {
            fillstack(go_new, 0xfd);
        }
        if(stackDebug >= 1)
        {
            print("copystack gp="_s, gp, " ["_s, hex(old.lo), " "_s, hex(old.hi - used), " "_s, hex(old.hi), "]"_s, " -> ["_s, hex(go_new.lo), " "_s, hex(go_new.hi - used), " "_s, hex(go_new.hi), "]/"_s, newsize, "\n"_s);
        }
        // Compute adjustment.
        adjustinfo adjinfo = {};
        adjinfo.old = old;
        adjinfo.delta = go_new.hi - old.hi;
        auto ncopy = used;
        if(! gp->activeStackChans)
        {
            if(newsize < old.hi - old.lo && rec::Load(gocpp::recv(gp->parkingOnChan)))
            {
                go_throw("racy sudog adjustment due to parking on channel"_s);
            }
            adjustsudogs(gp, & adjinfo);
        }
        else
        {
            adjinfo.sghi = findsghi(gp, old);
            ncopy -= syncadjustsudogs(gp, used, & adjinfo);
        }
        memmove(unsafe::Pointer(go_new.hi - ncopy), unsafe::Pointer(old.hi - ncopy), ncopy);
        adjustctxt(gp, & adjinfo);
        adjustdefers(gp, & adjinfo);
        adjustpanics(gp, & adjinfo);
        if(adjinfo.sghi != 0)
        {
            adjinfo.sghi += adjinfo.delta;
        }
        gp->stack = go_new;
        gp->stackguard0 = go_new.lo + stackGuard;
        gp->sched.sp = go_new.hi - used;
        gp->stktopsp += adjinfo.delta;
        // Adjust pointers in the new stack.
        unwinder u = {};
        for(rec::init(gocpp::recv(u), gp, 0); rec::valid(gocpp::recv(u)); rec::next(gocpp::recv(u)))
        {
            adjustframe(& u.frame, & adjinfo);
        }
        if(stackPoisonCopy != 0)
        {
            fillstack(old, 0xfc);
        }
        stackfree(old);
    }

    // round x up to a power of 2.
    int32_t round2(int32_t x)
    {
        auto s = (unsigned int)(0);
        for(; (1 << s) < x; )
        {
            s++;
        }
        return 1 << s;
    }

    // Called from runtime·morestack when more stack is needed.
    // Allocate larger stack and relocate to new stack.
    // Stack growth is multiplicative, for constant amortized cost.
    //
    // g->atomicstatus will be Grunning or Gscanrunning upon entry.
    // If the scheduler is trying to stop this g, then it will set preemptStop.
    //
    // This must be nowritebarrierrec because it can be called as part of
    // stack growth from other nowritebarrierrec functions, but the
    // compiler doesn't check this.
    //
    //go:nowritebarrierrec
    void newstack()
    {
        auto thisg = getg();
        if(rec::ptr(gocpp::recv(thisg->m->morebuf.g))->stackguard0 == stackFork)
        {
            go_throw("stack growth after fork"_s);
        }
        if(rec::ptr(gocpp::recv(thisg->m->morebuf.g)) != thisg->m->curg)
        {
            print("runtime: newstack called from g="_s, hex(thisg->m->morebuf.g), "\n"_s + "\tm="_s, thisg->m, " m->curg="_s, thisg->m->curg, " m->g0="_s, thisg->m->g0, " m->gsignal="_s, thisg->m->gsignal, "\n"_s);
            auto morebuf = thisg->m->morebuf;
            traceback(morebuf.pc, morebuf.sp, morebuf.lr, rec::ptr(gocpp::recv(morebuf.g)));
            go_throw("runtime: wrong goroutine in newstack"_s);
        }
        auto gp = thisg->m->curg;
        if(thisg->m->curg->throwsplit)
        {
            auto morebuf = thisg->m->morebuf;
            gp->syscallsp = morebuf.sp;
            gp->syscallpc = morebuf.pc;
            auto [pcname, pcoff] = std::tuple{"(unknown)"_s, uintptr_t(0)};
            auto f = findfunc(gp->sched.pc);
            if(rec::valid(gocpp::recv(f)))
            {
                pcname = funcname(f);
                pcoff = gp->sched.pc - rec::entry(gocpp::recv(f));
            }
            print("runtime: newstack at "_s, pcname, "+"_s, hex(pcoff), " sp="_s, hex(gp->sched.sp), " stack=["_s, hex(gp->stack.lo), ", "_s, hex(gp->stack.hi), "]\n"_s, "\tmorebuf={pc:"_s, hex(morebuf.pc), " sp:"_s, hex(morebuf.sp), " lr:"_s, hex(morebuf.lr), "}\n"_s, "\tsched={pc:"_s, hex(gp->sched.pc), " sp:"_s, hex(gp->sched.sp), " lr:"_s, hex(gp->sched.lr), " ctxt:"_s, gp->sched.ctxt, "}\n"_s);
            thisg->m->traceback = 2;
            traceback(morebuf.pc, morebuf.sp, morebuf.lr, gp);
            go_throw("runtime: stack split at bad time"_s);
        }
        auto morebuf = thisg->m->morebuf;
        thisg->m->morebuf.pc = 0;
        thisg->m->morebuf.lr = 0;
        thisg->m->morebuf.sp = 0;
        thisg->m->morebuf.g = 0;
        auto stackguard0 = atomic::Loaduintptr(& gp->stackguard0);
        auto preempt = stackguard0 == stackPreempt;
        if(preempt)
        {
            if(! canPreemptM(thisg->m))
            {
                gp->stackguard0 = gp->stack.lo + stackGuard;
                gogo(& gp->sched);
            }
        }
        if(gp->stack.lo == 0)
        {
            go_throw("missing stack in newstack"_s);
        }
        auto sp = gp->sched.sp;
        if(goarch::ArchFamily == goarch::AMD64 || goarch::ArchFamily == goarch::I386 || goarch::ArchFamily == goarch::WASM)
        {
            sp -= goarch::PtrSize;
        }
        if(stackDebug >= 1 || sp < gp->stack.lo)
        {
            print("runtime: newstack sp="_s, hex(sp), " stack=["_s, hex(gp->stack.lo), ", "_s, hex(gp->stack.hi), "]\n"_s, "\tmorebuf={pc:"_s, hex(morebuf.pc), " sp:"_s, hex(morebuf.sp), " lr:"_s, hex(morebuf.lr), "}\n"_s, "\tsched={pc:"_s, hex(gp->sched.pc), " sp:"_s, hex(gp->sched.sp), " lr:"_s, hex(gp->sched.lr), " ctxt:"_s, gp->sched.ctxt, "}\n"_s);
        }
        if(sp < gp->stack.lo)
        {
            print("runtime: gp="_s, gp, ", goid="_s, gp->goid, ", gp->status="_s, hex(readgstatus(gp)), "\n "_s);
            print("runtime: split stack overflow: "_s, hex(sp), " < "_s, hex(gp->stack.lo), "\n"_s);
            go_throw("runtime: split stack overflow"_s);
        }
        if(preempt)
        {
            if(gp == thisg->m->g0)
            {
                go_throw("runtime: preempt g0"_s);
            }
            if(thisg->m->p == 0 && thisg->m->locks == 0)
            {
                go_throw("runtime: g is running but p is not"_s);
            }
            if(gp->preemptShrink)
            {
                gp->preemptShrink = false;
                shrinkstack(gp);
            }
            if(gp->preemptStop)
            {
                preemptPark(gp);
            }
            gopreempt_m(gp);
        }
        auto oldsize = gp->stack.hi - gp->stack.lo;
        auto newsize = oldsize * 2;
        if(auto f = findfunc(gp->sched.pc); rec::valid(gocpp::recv(f)))
        {
            auto max = uintptr_t(funcMaxSPDelta(f));
            auto needed = max + stackGuard;
            auto used = gp->stack.hi - gp->sched.sp;
            for(; newsize - used < needed; )
            {
                newsize *= 2;
            }
        }
        if(stackguard0 == stackForceMove)
        {
            newsize = oldsize;
        }
        if(newsize > maxstacksize || newsize > maxstackceiling)
        {
            if(maxstacksize < maxstackceiling)
            {
                print("runtime: goroutine stack exceeds "_s, maxstacksize, "-byte limit\n"_s);
            }
            else
            {
                print("runtime: goroutine stack exceeds "_s, maxstackceiling, "-byte limit\n"_s);
            }
            print("runtime: sp="_s, hex(sp), " stack=["_s, hex(gp->stack.lo), ", "_s, hex(gp->stack.hi), "]\n"_s);
            go_throw("stack overflow"_s);
        }
        casgstatus(gp, _Grunning, _Gcopystack);
        copystack(gp, newsize);
        if(stackDebug >= 1)
        {
            print("stack grow done\n"_s);
        }
        casgstatus(gp, _Gcopystack, _Grunning);
        gogo(& gp->sched);
    }

    //go:nosplit
    void nilfunc()
    {
        *(uint8_t*)(nullptr) = 0;
    }

    // adjust Gobuf as if it executed a call to fn
    // and then stopped before the first instruction in fn.
    void gostartcallfn(struct gobuf* gobuf, struct funcval* fv)
    {
        unsafe::Pointer fn = {};
        if(fv != nullptr)
        {
            fn = unsafe::Pointer(fv->fn);
        }
        else
        {
            fn = unsafe::Pointer(abi::FuncPCABIInternal(nilfunc));
        }
        gostartcall(gobuf, fn, unsafe::Pointer(fv));
    }

    // isShrinkStackSafe returns whether it's safe to attempt to shrink
    // gp's stack. Shrinking the stack is only safe when we have precise
    // pointer maps for all frames on the stack.
    bool isShrinkStackSafe(struct g* gp)
    {
        return gp->syscallsp == 0 && ! gp->asyncSafePoint && ! rec::Load(gocpp::recv(gp->parkingOnChan));
    }

    // Maybe shrink the stack being used by gp.
    //
    // gp must be stopped and we must own its stack. It may be in
    // _Grunning, but only if this is our own user G.
    void shrinkstack(struct g* gp)
    {
        if(gp->stack.lo == 0)
        {
            go_throw("missing stack in shrinkstack"_s);
        }
        if(auto s = readgstatus(gp); s & _Gscan == 0)
        {
            if(! (gp == getg()->m->curg && getg() != getg()->m->curg && s == _Grunning))
            {
                go_throw("bad status in shrinkstack"_s);
            }
        }
        if(! isShrinkStackSafe(gp))
        {
            go_throw("shrinkstack at bad time"_s);
        }
        if(gp == getg()->m->curg && gp->m->libcallsp != 0)
        {
            go_throw("shrinking stack in libcall"_s);
        }
        if(debug.gcshrinkstackoff > 0)
        {
            return;
        }
        auto f = findfunc(gp->startpc);
        if(rec::valid(gocpp::recv(f)) && f.funcID == abi::FuncID_gcBgMarkWorker)
        {
            return;
        }
        auto oldsize = gp->stack.hi - gp->stack.lo;
        auto newsize = oldsize / 2;
        if(newsize < fixedStack)
        {
            return;
        }
        auto avail = gp->stack.hi - gp->stack.lo;
        if(auto used = gp->stack.hi - gp->sched.sp + stackNosplit; used >= avail / 4)
        {
            return;
        }
        if(stackDebug > 0)
        {
            print("shrinking stack "_s, oldsize, "->"_s, newsize, "\n"_s);
        }
        copystack(gp, newsize);
    }

    // freeStackSpans frees unused stack spans at the end of GC.
    void freeStackSpans()
    {
        for(auto [order, gocpp_ignored] : stackpool)
        {
            lock(& stackpool[order].item.mu);
            auto list = & stackpool[order].item.span;
            for(auto s = list->first; s != nullptr; )
            {
                auto next = s->next;
                if(s->allocCount == 0)
                {
                    rec::remove(gocpp::recv(list), s);
                    s->manualFreeList = 0;
                    osStackFree(s);
                    rec::freeManual(gocpp::recv(mheap_), s, spanAllocStack);
                }
                s = next;
            }
            unlock(& stackpool[order].item.mu);
        }
        lock(& stackLarge.lock);
        for(auto [i, gocpp_ignored] : stackLarge.free)
        {
            for(auto s = stackLarge.free[i].first; s != nullptr; )
            {
                auto next = s->next;
                rec::remove(gocpp::recv(stackLarge.free[i]), s);
                osStackFree(s);
                rec::freeManual(gocpp::recv(mheap_), s, spanAllocStack);
                s = next;
            }
        }
        unlock(& stackLarge.lock);
    }

    // A stackObjectRecord is generated by the compiler for each stack object in a stack frame.
    // This record must match the generator code in cmd/compile/internal/liveness/plive.go:emitStackObjects.
    
    template<typename T> requires gocpp::GoStruct<T>
    stackObjectRecord::operator T()
    {
        T result;
        result.off = this->off;
        result.size = this->size;
        result._ptrdata = this->_ptrdata;
        result.gcdataoff = this->gcdataoff;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool stackObjectRecord::operator==(const T& ref) const
    {
        if (off != ref.off) return false;
        if (size != ref.size) return false;
        if (_ptrdata != ref._ptrdata) return false;
        if (gcdataoff != ref.gcdataoff) return false;
        return true;
    }

    std::ostream& stackObjectRecord::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << off;
        os << " " << size;
        os << " " << _ptrdata;
        os << " " << gcdataoff;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct stackObjectRecord& value)
    {
        return value.PrintTo(os);
    }

    bool rec::useGCProg(golang::runtime::stackObjectRecord* r)
    {
        return r->_ptrdata < 0;
    }

    uintptr_t rec::ptrdata(golang::runtime::stackObjectRecord* r)
    {
        auto x = r->_ptrdata;
        if(x < 0)
        {
            return uintptr_t(- x);
        }
        return uintptr_t(x);
    }

    // gcdata returns pointer map or GC prog of the type.
    unsigned char* rec::gcdata(golang::runtime::stackObjectRecord* r)
    {
        auto ptr = uintptr_t(unsafe::Pointer(r));
        moduledata* mod = {};
        for(auto datap = & firstmoduledata; datap != nullptr; datap = datap->next)
        {
            if(datap->gofunc <= ptr && ptr < datap->end)
            {
                mod = datap;
                break;
            }
        }
        auto res = mod->rodata + uintptr_t(r->gcdataoff);
        return (unsigned char*)(unsafe::Pointer(res));
    }

    // This is exported as ABI0 via linkname so obj can call it.
    //
    //go:nosplit
    //go:linkname morestackc
    void morestackc()
    {
        go_throw("attempt to execute system stack code on user stack"_s);
    }

    // startingStackSize is the amount of stack that new goroutines start with.
    // It is a power of 2, and between _FixedStack and maxstacksize, inclusive.
    // startingStackSize is updated every GC by tracking the average size of
    // stacks scanned during the GC.
    uint32_t startingStackSize = fixedStack;
    void gcComputeStartingStackSize()
    {
        if(debug.adaptivestackstart == 0)
        {
            return;
        }
        // For details, see the design doc at
        // https://docs.google.com/document/d/1YDlGIdVTPnmUiTAavlZxBI1d9pwGQgZT7IKFKlIXohQ/edit?usp=sharing
        // The basic algorithm is to track the average size of stacks
        // and start goroutines with stack equal to that average size.
        // Starting at the average size uses at most 2x the space that
        // an ideal algorithm would have used.
        // This is just a heuristic to avoid excessive stack growth work
        // early in a goroutine's lifetime. See issue 18138. Stacks that
        // are allocated too small can still grow, and stacks allocated
        // too large can still shrink.
        uint64_t scannedStackSize = {};
        uint64_t scannedStacks = {};
        for(auto [gocpp_ignored, p] : allp)
        {
            scannedStackSize += p->scannedStackSize;
            scannedStacks += p->scannedStacks;
            p->scannedStackSize = 0;
            p->scannedStacks = 0;
        }
        if(scannedStacks == 0)
        {
            startingStackSize = fixedStack;
            return;
        }
        auto avg = scannedStackSize / scannedStacks + stackGuard;
        if(avg > uint64_t(maxstacksize))
        {
            avg = uint64_t(maxstacksize);
        }
        if(avg < fixedStack)
        {
            avg = fixedStack;
        }
        startingStackSize = uint32_t(round2(int32_t(avg)));
    }

}

