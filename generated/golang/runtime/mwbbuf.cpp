// generated by GoCpp from file '$(ImportDir)/runtime/mwbbuf.go'
#include <complex>
#include <functional>
#include <iostream>
#include <iomanip>
#include <map>
#include <string>
#include <tuple>
#include <vector>

#include "golang/runtime/mwbbuf.h"
#include "gocpp/support.h"

#include "golang/internal/abi/type.h"
#include "golang/internal/chacha8rand/chacha8.h"
#include "golang/internal/goarch/goarch.h"
#include "golang/runtime/cgocall.h"
#include "golang/runtime/chan.h"
#include "golang/runtime/coro.h"
#include "golang/runtime/debuglog_off.h"
#include "golang/runtime/internal/atomic/atomic_amd64.h"
#include "golang/runtime/internal/atomic/types.h"
#include "golang/runtime/internal/sys/nih.h"
#include "golang/runtime/lockrank.h"
#include "golang/runtime/lockrank_off.h"
#include "golang/runtime/malloc.h"
#include "golang/runtime/mbitmap.h"
#include "golang/runtime/mbitmap_allocheaders.h"
#include "golang/runtime/mcache.h"
#include "golang/runtime/mcheckmark.h"
#include "golang/runtime/mgc.h"
#include "golang/runtime/mgclimit.h"
#include "golang/runtime/mgcmark.h"
#include "golang/runtime/mgcwork.h"
#include "golang/runtime/mheap.h"
#include "golang/runtime/mpagecache.h"
#include "golang/runtime/mprof.h"
#include "golang/runtime/mranges.h"
#include "golang/runtime/os_windows.h"
#include "golang/runtime/pagetrace_off.h"
#include "golang/runtime/panic.h"
#include "golang/runtime/pinner.h"
#include "golang/runtime/proc.h"
#include "golang/runtime/runtime2.h"
#include "golang/runtime/signal_windows.h"
#include "golang/runtime/stubs.h"
#include "golang/runtime/symtab.h"
#include "golang/runtime/time.h"
#include "golang/runtime/trace2buf.h"
#include "golang/runtime/trace2runtime.h"
#include "golang/runtime/trace2status.h"
#include "golang/runtime/trace2time.h"
#include "golang/unsafe/unsafe.h"

namespace golang::runtime
{
    namespace rec
    {
        using namespace mocklib::rec;
    }

    // testSmallBuf forces a small write barrier buffer to stress write
    // barrier flushing.
    // wbBuf is a per-P buffer of pointers queued by the write barrier.
    // This buffer is flushed to the GC workbufs when it fills up and on
    // various GC transitions.
    //
    // This is closely related to a "sequential store buffer" (SSB),
    // except that SSBs are usually used for maintaining remembered sets,
    // while this is used for marking.
    
    template<typename T> requires gocpp::GoStruct<T>
    wbBuf::operator T()
    {
        T result;
        result.next = this->next;
        result.end = this->end;
        result.buf = this->buf;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool wbBuf::operator==(const T& ref) const
    {
        if (next != ref.next) return false;
        if (end != ref.end) return false;
        if (buf != ref.buf) return false;
        return true;
    }

    std::ostream& wbBuf::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << next;
        os << " " << end;
        os << " " << buf;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct wbBuf& value)
    {
        return value.PrintTo(os);
    }

    // wbBufEntries is the maximum number of pointers that can be
    // stored in the write barrier buffer.
    //
    // This trades latency for throughput amortization. Higher
    // values amortize flushing overhead more, but increase the
    // latency of flushing. Higher values also increase the cache
    // footprint of the buffer.
    //
    // TODO: What is the latency cost of this? Tune this value.
    // Maximum number of entries that we need to ask from the
    // buffer in a single call.
    // reset empties b by resetting its next and end pointers.
    void rec::reset(golang::runtime::wbBuf* b)
    {
        auto start = uintptr_t(unsafe::Pointer(& b->buf[0]));
        b->next = start;
        if(testSmallBuf)
        {
            b->end = uintptr_t(unsafe::Pointer(& b->buf[wbMaxEntriesPerCall + 1]));
        }
        else
        {
            b->end = start + uintptr_t(len(b->buf)) * gocpp::Sizeof<uintptr_t>();
        }
        if((b->end - b->next) % gocpp::Sizeof<uintptr_t>() != 0)
        {
            go_throw("bad write barrier buffer bounds"_s);
        }
    }

    // discard resets b's next pointer, but not its end pointer.
    //
    // This must be nosplit because it's called by wbBufFlush.
    //
    //go:nosplit
    void rec::discard(golang::runtime::wbBuf* b)
    {
        b->next = uintptr_t(unsafe::Pointer(& b->buf[0]));
    }

    // empty reports whether b contains no pointers.
    bool rec::empty(golang::runtime::wbBuf* b)
    {
        return b->next == uintptr_t(unsafe::Pointer(& b->buf[0]));
    }

    // getX returns space in the write barrier buffer to store X pointers.
    // getX will flush the buffer if necessary. Callers should use this as:
    //
    //	buf := &getg().m.p.ptr().wbBuf
    //	p := buf.get2()
    //	p[0], p[1] = old, new
    //	... actual memory write ...
    //
    // The caller must ensure there are no preemption points during the
    // above sequence. There must be no preemption points while buf is in
    // use because it is a per-P resource. There must be no preemption
    // points between the buffer put and the write to memory because this
    // could allow a GC phase change, which could result in missed write
    // barriers.
    //
    // getX must be nowritebarrierrec to because write barriers here would
    // corrupt the write barrier buffer. It (and everything it calls, if
    // it called anything) has to be nosplit to avoid scheduling on to a
    // different P and a different buffer.
    //
    //go:nowritebarrierrec
    //go:nosplit
    gocpp::array<uintptr_t, 1>* rec::get1(golang::runtime::wbBuf* b)
    {
        if(b->next + goarch::PtrSize > b->end)
        {
            wbBufFlush();
        }
        auto p = (gocpp::array<uintptr_t, 1>*)(unsafe::Pointer(b->next));
        b->next += goarch::PtrSize;
        return p;
    }

    //go:nowritebarrierrec
    //go:nosplit
    gocpp::array<uintptr_t, 2>* rec::get2(golang::runtime::wbBuf* b)
    {
        if(b->next + 2 * goarch::PtrSize > b->end)
        {
            wbBufFlush();
        }
        auto p = (gocpp::array<uintptr_t, 2>*)(unsafe::Pointer(b->next));
        b->next += 2 * goarch::PtrSize;
        return p;
    }

    // wbBufFlush flushes the current P's write barrier buffer to the GC
    // workbufs.
    //
    // This must not have write barriers because it is part of the write
    // barrier implementation.
    //
    // This and everything it calls must be nosplit because 1) the stack
    // contains untyped slots from gcWriteBarrier and 2) there must not be
    // a GC safe point between the write barrier test in the caller and
    // flushing the buffer.
    //
    // TODO: A "go:nosplitrec" annotation would be perfect for this.
    //
    //go:nowritebarrierrec
    //go:nosplit
    void wbBufFlush()
    {
        if(getg()->m->dying > 0)
        {
            rec::discard(gocpp::recv(rec::ptr(gocpp::recv(getg()->m->p))->wbBuf));
            return;
        }
        systemstack([=]() mutable -> void
        {
            wbBufFlush1(rec::ptr(gocpp::recv(getg()->m->p)));
        });
    }

    // wbBufFlush1 flushes p's write barrier buffer to the GC work queue.
    //
    // This must not have write barriers because it is part of the write
    // barrier implementation, so this may lead to infinite loops or
    // buffer corruption.
    //
    // This must be non-preemptible because it uses the P's workbuf.
    //
    //go:nowritebarrierrec
    //go:systemstack
    void wbBufFlush1(struct p* pp)
    {
        auto start = uintptr_t(unsafe::Pointer(& pp->wbBuf.buf[0]));
        auto n = (pp->wbBuf.next - start) / gocpp::Sizeof<uintptr_t>();
        auto ptrs = pp->wbBuf.buf.make_slice(0, n);
        pp->wbBuf.next = 0;
        if(useCheckmark)
        {
            for(auto [gocpp_ignored, ptr] : ptrs)
            {
                shade(ptr);
            }
            rec::reset(gocpp::recv(pp->wbBuf));
            return;
        }
        auto gcw = & pp->gcw;
        auto pos = 0;
        for(auto [gocpp_ignored, ptr] : ptrs)
        {
            if(ptr < minLegalPointer)
            {
                continue;
            }
            auto [obj, span, objIndex] = findObject(ptr, 0, 0);
            if(obj == 0)
            {
                continue;
            }
            auto mbits = rec::markBitsForIndex(gocpp::recv(span), objIndex);
            if(rec::isMarked(gocpp::recv(mbits)))
            {
                continue;
            }
            rec::setMarked(gocpp::recv(mbits));
            auto [arena, pageIdx, pageMask] = pageIndexOf(rec::base(gocpp::recv(span)));
            if(arena->pageMarks[pageIdx] & pageMask == 0)
            {
                atomic::Or8(& arena->pageMarks[pageIdx], pageMask);
            }
            if(rec::noscan(gocpp::recv(span->spanclass)))
            {
                gcw->bytesMarked += uint64_t(span->elemsize);
                continue;
            }
            ptrs[pos] = obj;
            pos++;
        }
        rec::putBatch(gocpp::recv(gcw), ptrs.make_slice(0, pos));
        rec::reset(gocpp::recv(pp->wbBuf));
    }

}

