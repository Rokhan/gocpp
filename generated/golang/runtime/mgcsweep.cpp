// generated by GoCpp from file '$(ImportDir)/runtime/mgcsweep.go'
#include <complex>
#include <functional>
#include <iostream>
#include <iomanip>
#include <map>
#include <string>
#include <tuple>
#include <vector>

#include "golang/runtime/mgcsweep.h"
#include "gocpp/support.h"

#include "golang/internal/abi/type.h"
#include "golang/internal/chacha8rand/chacha8.h"
#include "golang/internal/cpu/cpu.h"
#include "golang/internal/goexperiment/exp_allocheaders_on.h"
#include "golang/runtime/asan0.h"
#include "golang/runtime/cgocall.h"
#include "golang/runtime/chan.h"
#include "golang/runtime/coro.h"
#include "golang/runtime/debuglog_off.h"
#include "golang/runtime/internal/atomic/atomic_amd64.h"
#include "golang/runtime/internal/atomic/stubs.h"
#include "golang/runtime/internal/atomic/types.h"
#include "golang/runtime/internal/sys/nih.h"
#include "golang/runtime/lock_sema.h"
#include "golang/runtime/lockrank.h"
#include "golang/runtime/lockrank_off.h"
#include "golang/runtime/malloc.h"
#include "golang/runtime/mbitmap.h"
#include "golang/runtime/mbitmap_allocheaders.h"
#include "golang/runtime/mcache.h"
#include "golang/runtime/mcentral.h"
#include "golang/runtime/mcheckmark.h"
#include "golang/runtime/mem.h"
#include "golang/runtime/mfixalloc.h"
#include "golang/runtime/mgc.h"
#include "golang/runtime/mgclimit.h"
#include "golang/runtime/mgcpacer.h"
#include "golang/runtime/mgcscavenge.h"
#include "golang/runtime/mgcwork.h"
#include "golang/runtime/mheap.h"
#include "golang/runtime/mpagealloc.h"
#include "golang/runtime/mpagecache.h"
#include "golang/runtime/mpallocbits.h"
#include "golang/runtime/mprof.h"
#include "golang/runtime/mranges.h"
#include "golang/runtime/msan0.h"
#include "golang/runtime/mspanset.h"
#include "golang/runtime/mstats.h"
#include "golang/runtime/mwbbuf.h"
#include "golang/runtime/os_windows.h"
#include "golang/runtime/pagetrace_off.h"
#include "golang/runtime/panic.h"
#include "golang/runtime/pinner.h"
#include "golang/runtime/print.h"
#include "golang/runtime/proc.h"
#include "golang/runtime/race0.h"
#include "golang/runtime/runtime1.h"
#include "golang/runtime/runtime2.h"
#include "golang/runtime/signal_windows.h"
#include "golang/runtime/stubs.h"
#include "golang/runtime/symtab.h"
#include "golang/runtime/time.h"
#include "golang/runtime/trace2buf.h"
#include "golang/runtime/trace2runtime.h"
#include "golang/runtime/trace2status.h"
#include "golang/runtime/trace2time.h"
#include "golang/unsafe/unsafe.h"

namespace golang::runtime
{
    namespace rec
    {
        using namespace mocklib::rec;
        using atomic::rec::Add;
        using atomic::rec::CompareAndSwap;
        using atomic::rec::Load;
        using atomic::rec::Store;
    }

    sweepdata sweep;
    // State of background sweep.
    
    template<typename T> requires gocpp::GoStruct<T>
    sweepdata::operator T()
    {
        T result;
        result.lock = this->lock;
        result.g = this->g;
        result.parked = this->parked;
        result.active = this->active;
        result.centralIndex = this->centralIndex;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool sweepdata::operator==(const T& ref) const
    {
        if (lock != ref.lock) return false;
        if (g != ref.g) return false;
        if (parked != ref.parked) return false;
        if (active != ref.active) return false;
        if (centralIndex != ref.centralIndex) return false;
        return true;
    }

    std::ostream& sweepdata::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << lock;
        os << " " << g;
        os << " " << parked;
        os << " " << active;
        os << " " << centralIndex;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct sweepdata& value)
    {
        return value.PrintTo(os);
    }

    // sweepClass is a spanClass and one bit to represent whether we're currently
    // sweeping partial or full spans.
    runtime::sweepClass rec::load(golang::runtime::sweepClass* s)
    {
        return sweepClass(atomic::Load((uint32_t*)(s)));
    }

    void rec::update(golang::runtime::sweepClass* s, golang::runtime::sweepClass sNew)
    {
        auto sOld = rec::load(gocpp::recv(s));
        for(; sOld < sNew && ! atomic::Cas((uint32_t*)(s), uint32_t(sOld), uint32_t(sNew)); )
        {
            sOld = rec::load(gocpp::recv(s));
        }
    }

    void rec::clear(golang::runtime::sweepClass* s)
    {
        atomic::Store((uint32_t*)(s), 0);
    }

    // split returns the underlying span class as well as
    // whether we're interested in the full or partial
    // unswept lists for that class, indicated as a boolean
    // (true means "full").
    std::tuple<runtime::spanClass, bool> rec::split(golang::runtime::sweepClass s)
    {
        runtime::spanClass spc;
        bool full;
        return {spanClass(s >> 1), s & 1 == 0};
    }

    // nextSpanForSweep finds and pops the next span for sweeping from the
    // central sweep buffers. It returns ownership of the span to the caller.
    // Returns nil if no such span exists.
    struct mspan* rec::nextSpanForSweep(golang::runtime::mheap* h)
    {
        auto sg = h->sweepgen;
        for(auto sc = rec::load(gocpp::recv(sweep.centralIndex)); sc < numSweepClasses; sc++)
        {
            auto [spc, full] = rec::split(gocpp::recv(sc));
            auto c = & h->central[spc].mcentral;
            mspan* s = {};
            if(full)
            {
                s = rec::pop(gocpp::recv(rec::fullUnswept(gocpp::recv(c), sg)));
            }
            else
            {
                s = rec::pop(gocpp::recv(rec::partialUnswept(gocpp::recv(c), sg)));
            }
            if(s != nullptr)
            {
                rec::update(gocpp::recv(sweep.centralIndex), sc);
                return s;
            }
        }
        rec::update(gocpp::recv(sweep.centralIndex), sweepClassDone);
        return nullptr;
    }

    // activeSweep is a type that captures whether sweeping
    // is done, and whether there are any outstanding sweepers.
    //
    // Every potential sweeper must call begin() before they look
    // for work, and end() after they've finished sweeping.
    
    template<typename T> requires gocpp::GoStruct<T>
    activeSweep::operator T()
    {
        T result;
        result.state = this->state;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool activeSweep::operator==(const T& ref) const
    {
        if (state != ref.state) return false;
        return true;
    }

    std::ostream& activeSweep::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << state;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct activeSweep& value)
    {
        return value.PrintTo(os);
    }

    // begin registers a new sweeper. Returns a sweepLocker
    // for acquiring spans for sweeping. Any outstanding sweeper blocks
    // sweep termination.
    //
    // If the sweepLocker is invalid, the caller can be sure that all
    // outstanding sweep work has been drained, so there is nothing left
    // to sweep. Note that there may be sweepers currently running, so
    // this does not indicate that all sweeping has completed.
    //
    // Even if the sweepLocker is invalid, its sweepGen is always valid.
    struct sweepLocker rec::begin(golang::runtime::activeSweep* a)
    {
        for(; ; )
        {
            auto state = rec::Load(gocpp::recv(a->state));
            if(state & sweepDrainedMask != 0)
            {
                return sweepLocker {mheap_.sweepgen, false};
            }
            if(rec::CompareAndSwap(gocpp::recv(a->state), state, state + 1))
            {
                return sweepLocker {mheap_.sweepgen, true};
            }
        }
    }

    // end deregisters a sweeper. Must be called once for each time
    // begin is called if the sweepLocker is valid.
    void rec::end(golang::runtime::activeSweep* a, struct sweepLocker sl)
    {
        if(sl.sweepGen != mheap_.sweepgen)
        {
            go_throw("sweeper left outstanding across sweep generations"_s);
        }
        for(; ; )
        {
            auto state = rec::Load(gocpp::recv(a->state));
            if((state &^ sweepDrainedMask) - 1 >= sweepDrainedMask)
            {
                go_throw("mismatched begin/end of activeSweep"_s);
            }
            if(rec::CompareAndSwap(gocpp::recv(a->state), state, state - 1))
            {
                if(state != sweepDrainedMask)
                {
                    return;
                }
                if(debug.gcpacertrace > 0)
                {
                    auto live = rec::Load(gocpp::recv(gcController.heapLive));
                    print("pacer: sweep done at heap size "_s, live >> 20, "MB; allocated "_s, (live - mheap_.sweepHeapLiveBasis) >> 20, "MB during sweep; swept "_s, rec::Load(gocpp::recv(mheap_.pagesSwept)), " pages at "_s, mheap_.sweepPagesPerByte, " pages/byte\n"_s);
                }
                return;
            }
        }
    }

    // markDrained marks the active sweep cycle as having drained
    // all remaining work. This is safe to be called concurrently
    // with all other methods of activeSweep, though may race.
    //
    // Returns true if this call was the one that actually performed
    // the mark.
    bool rec::markDrained(golang::runtime::activeSweep* a)
    {
        for(; ; )
        {
            auto state = rec::Load(gocpp::recv(a->state));
            if(state & sweepDrainedMask != 0)
            {
                return false;
            }
            if(rec::CompareAndSwap(gocpp::recv(a->state), state, state | sweepDrainedMask))
            {
                return true;
            }
        }
    }

    // sweepers returns the current number of active sweepers.
    uint32_t rec::sweepers(golang::runtime::activeSweep* a)
    {
        return rec::Load(gocpp::recv(a->state)) &^ sweepDrainedMask;
    }

    // isDone returns true if all sweep work has been drained and no more
    // outstanding sweepers exist. That is, when the sweep phase is
    // completely done.
    bool rec::isDone(golang::runtime::activeSweep* a)
    {
        return rec::Load(gocpp::recv(a->state)) == sweepDrainedMask;
    }

    // reset sets up the activeSweep for the next sweep cycle.
    //
    // The world must be stopped.
    void rec::reset(golang::runtime::activeSweep* a)
    {
        assertWorldStopped();
        rec::Store(gocpp::recv(a->state), 0);
    }

    // finishsweep_m ensures that all spans are swept.
    //
    // The world must be stopped. This ensures there are no sweeps in
    // progress.
    //
    //go:nowritebarrier
    void finishsweep_m()
    {
        assertWorldStopped();
        for(; sweepone() != ~ uintptr_t(0); )
        {
        }
        if(rec::sweepers(gocpp::recv(sweep.active)) != 0)
        {
            go_throw("active sweepers found at start of mark phase"_s);
        }
        auto sg = mheap_.sweepgen;
        for(auto [i, gocpp_ignored] : mheap_.central)
        {
            auto c = & mheap_.central[i].mcentral;
            rec::reset(gocpp::recv(rec::partialUnswept(gocpp::recv(c), sg)));
            rec::reset(gocpp::recv(rec::fullUnswept(gocpp::recv(c), sg)));
        }
        rec::wake(gocpp::recv(scavenger));
        nextMarkBitArenaEpoch();
    }

    void bgsweep(gocpp::channel<int> c)
    {
        sweep.g = getg();
        lockInit(& sweep.lock, lockRankSweep);
        lock(& sweep.lock);
        sweep.parked = true;
        c.send(1);
        goparkunlock(& sweep.lock, waitReasonGCSweepWait, traceBlockGCSweep, 1);
        for(; ; )
        {
            // bgsweep attempts to be a "low priority" goroutine by intentionally
            // yielding time. It's OK if it doesn't run, because goroutines allocating
            // memory will sweep and ensure that all spans are swept before the next
            // GC cycle. We really only want to run when we're idle.
            //
            // However, calling Gosched after each span swept produces a tremendous
            // amount of tracing events, sometimes up to 50% of events in a trace. It's
            // also inefficient to call into the scheduler so much because sweeping a
            // single span is in general a very fast operation, taking as little as 30 ns
            // on modern hardware. (See #54767.)
            //
            // As a result, bgsweep sweeps in batches, and only calls into the scheduler
            // at the end of every batch. Furthermore, it only yields its time if there
            // isn't spare idle time available on other cores. If there's available idle
            // time, helping to sweep can reduce allocation latencies by getting ahead of
            // the proportional sweeper and having spans ready to go for allocation.
            auto sweepBatchSize = 10;
            auto nSwept = 0;
            for(; sweepone() != ~ uintptr_t(0); )
            {
                nSwept++;
                if(nSwept % sweepBatchSize == 0)
                {
                    goschedIfBusy();
                }
            }
            for(; freeSomeWbufs(true); )
            {
                goschedIfBusy();
            }
            lock(& sweep.lock);
            if(! isSweepDone())
            {
                unlock(& sweep.lock);
                continue;
            }
            sweep.parked = true;
            goparkunlock(& sweep.lock, waitReasonGCSweepWait, traceBlockGCSweep, 1);
        }
    }

    // sweepLocker acquires sweep ownership of spans.
    
    template<typename T> requires gocpp::GoStruct<T>
    sweepLocker::operator T()
    {
        T result;
        result.sweepGen = this->sweepGen;
        result.valid = this->valid;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool sweepLocker::operator==(const T& ref) const
    {
        if (sweepGen != ref.sweepGen) return false;
        if (valid != ref.valid) return false;
        return true;
    }

    std::ostream& sweepLocker::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << sweepGen;
        os << " " << valid;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct sweepLocker& value)
    {
        return value.PrintTo(os);
    }

    // sweepLocked represents sweep ownership of a span.
    
    template<typename T> requires gocpp::GoStruct<T>
    sweepLocked::operator T()
    {
        T result;
        result.mspan = this->mspan;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool sweepLocked::operator==(const T& ref) const
    {
        if (mspan != ref.mspan) return false;
        return true;
    }

    std::ostream& sweepLocked::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << mspan;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct sweepLocked& value)
    {
        return value.PrintTo(os);
    }

    // tryAcquire attempts to acquire sweep ownership of span s. If it
    // successfully acquires ownership, it blocks sweep completion.
    std::tuple<struct sweepLocked, bool> rec::tryAcquire(golang::runtime::sweepLocker* l, struct mspan* s)
    {
        if(! l->valid)
        {
            go_throw("use of invalid sweepLocker"_s);
        }
        if(atomic::Load(& s->sweepgen) != l->sweepGen - 2)
        {
            return {sweepLocked {}, false};
        }
        if(! atomic::Cas(& s->sweepgen, l->sweepGen - 2, l->sweepGen - 1))
        {
            return {sweepLocked {}, false};
        }
        return {sweepLocked {s}, true};
    }

    // sweepone sweeps some unswept heap span and returns the number of pages returned
    // to the heap, or ^uintptr(0) if there was nothing to sweep.
    uintptr_t sweepone()
    {
        auto gp = getg();
        gp->m->locks++;
        auto sl = rec::begin(gocpp::recv(sweep.active));
        if(! sl.valid)
        {
            gp->m->locks--;
            return ~ uintptr_t(0);
        }
        auto npages = ~ uintptr_t(0);
        bool noMoreWork = {};
        for(; ; )
        {
            auto s = rec::nextSpanForSweep(gocpp::recv(mheap_));
            if(s == nullptr)
            {
                noMoreWork = rec::markDrained(gocpp::recv(sweep.active));
                break;
            }
            if(auto state = rec::get(gocpp::recv(s->state)); state != mSpanInUse)
            {
                if(! (s->sweepgen == sl.sweepGen || s->sweepgen == sl.sweepGen + 3))
                {
                    print("runtime: bad span s.state="_s, state, " s.sweepgen="_s, s->sweepgen, " sweepgen="_s, sl.sweepGen, "\n"_s);
                    go_throw("non in-use span in unswept list"_s);
                }
                continue;
            }
            {
                auto [s_tmp, ok] = rec::tryAcquire(gocpp::recv(sl), s);
                if(auto& s = s_tmp; ok)
                {
                    npages = s.npages;
                    if(rec::sweep(gocpp::recv(s), false))
                    {
                        rec::Add(gocpp::recv(mheap_.reclaimCredit), npages);
                    }
                    else
                    {
                        npages = 0;
                    }
                    break;
                }
            }
        }
        rec::end(gocpp::recv(sweep.active), sl);
        if(noMoreWork)
        {
            if(debug.scavtrace > 0)
            {
                systemstack([=]() mutable -> void
                {
                    lock(& mheap_.lock);
                    auto releasedBg = rec::Load(gocpp::recv(mheap_.pages.scav.releasedBg));
                    auto releasedEager = rec::Load(gocpp::recv(mheap_.pages.scav.releasedEager));
                    printScavTrace(releasedBg, releasedEager, false);
                    rec::Add(gocpp::recv(mheap_.pages.scav.releasedBg), - releasedBg);
                    rec::Add(gocpp::recv(mheap_.pages.scav.releasedEager), - releasedEager);
                    unlock(& mheap_.lock);
                });
            }
            rec::ready(gocpp::recv(scavenger));
        }
        gp->m->locks--;
        return npages;
    }

    // isSweepDone reports whether all spans are swept.
    //
    // Note that this condition may transition from false to true at any
    // time as the sweeper runs. It may transition from true to false if a
    // GC runs; to prevent that the caller must be non-preemptible or must
    // somehow block GC progress.
    bool isSweepDone()
    {
        return rec::isDone(gocpp::recv(sweep.active));
    }

    // Returns only when span s has been swept.
    //
    //go:nowritebarrier
    void rec::ensureSwept(golang::runtime::mspan* s)
    {
        auto gp = getg();
        if(gp->m->locks == 0 && gp->m->mallocing == 0 && gp != gp->m->g0)
        {
            go_throw("mspan.ensureSwept: m is not locked"_s);
        }
        auto sl = rec::begin(gocpp::recv(sweep.active));
        if(sl.valid)
        {
            {
                auto [s_tmp, ok] = rec::tryAcquire(gocpp::recv(sl), s);
                if(auto& s = s_tmp; ok)
                {
                    rec::sweep(gocpp::recv(s), false);
                    rec::end(gocpp::recv(sweep.active), sl);
                    return;
                }
            }
            rec::end(gocpp::recv(sweep.active), sl);
        }
        for(; ; )
        {
            auto spangen = atomic::Load(& s->sweepgen);
            if(spangen == sl.sweepGen || spangen == sl.sweepGen + 3)
            {
                break;
            }
            osyield();
        }
    }

    // sweep frees or collects finalizers for blocks not marked in the mark phase.
    // It clears the mark bits in preparation for the next GC round.
    // Returns true if the span was returned to heap.
    // If preserve=true, don't return it to heap nor relink in mcentral lists;
    // caller takes care of it.
    bool rec::sweep(golang::runtime::sweepLocked* sl, bool preserve)
    {
        auto gp = getg();
        if(gp->m->locks == 0 && gp->m->mallocing == 0 && gp != gp->m->g0)
        {
            go_throw("mspan.sweep: m is not locked"_s);
        }
        auto s = sl->mspan;
        if(! preserve)
        {
            sl->mspan = nullptr;
        }
        auto sweepgen = mheap_.sweepgen;
        if(auto state = rec::get(gocpp::recv(s->state)); state != mSpanInUse || s->sweepgen != sweepgen - 1)
        {
            print("mspan.sweep: state="_s, state, " sweepgen="_s, s->sweepgen, " mheap.sweepgen="_s, sweepgen, "\n"_s);
            go_throw("mspan.sweep: bad span state"_s);
        }
        auto trace = traceAcquire();
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::GCSweepSpan(gocpp::recv(trace), s->npages * _PageSize);
            traceRelease(trace);
        }
        rec::Add(gocpp::recv(mheap_.pagesSwept), int64_t(s->npages));
        auto spc = s->spanclass;
        auto size = s->elemsize;
        auto hadSpecials = s->specials != nullptr;
        auto siter = newSpecialsIter(s);
        for(; rec::valid(gocpp::recv(siter)); )
        {
            auto objIndex = uintptr_t(siter.s->offset) / size;
            auto p = rec::base(gocpp::recv(s)) + objIndex * size;
            auto mbits = rec::markBitsForIndex(gocpp::recv(s), objIndex);
            if(! rec::isMarked(gocpp::recv(mbits)))
            {
                auto hasFin = false;
                auto endOffset = p - rec::base(gocpp::recv(s)) + size;
                for(auto tmp = siter.s; tmp != nullptr && uintptr_t(tmp->offset) < endOffset; tmp = tmp->next)
                {
                    if(tmp->kind == _KindSpecialFinalizer)
                    {
                        rec::setMarkedNonAtomic(gocpp::recv(mbits));
                        hasFin = true;
                        break;
                    }
                }
                for(; rec::valid(gocpp::recv(siter)) && uintptr_t(siter.s->offset) < endOffset; )
                {
                    auto special = siter.s;
                    auto p = rec::base(gocpp::recv(s)) + uintptr_t(special->offset);
                    if(special->kind == _KindSpecialFinalizer || ! hasFin)
                    {
                        rec::unlinkAndNext(gocpp::recv(siter));
                        freeSpecial(special, unsafe::Pointer(p), size);
                    }
                    else
                    {
                        rec::next(gocpp::recv(siter));
                    }
                }
            }
            else
            {
                if(siter.s->kind == _KindSpecialReachable)
                {
                    auto special = rec::unlinkAndNext(gocpp::recv(siter));
                    (specialReachable*)(unsafe::Pointer(special))->reachable = true;
                    freeSpecial(special, unsafe::Pointer(p), size);
                }
                else
                {
                    rec::next(gocpp::recv(siter));
                }
            }
        }
        if(hadSpecials && s->specials == nullptr)
        {
            spanHasNoSpecials(s);
        }
        if(debug.allocfreetrace != 0 || debug.clobberfree != 0 || raceenabled || msanenabled || asanenabled)
        {
            auto mbits = rec::markBitsForBase(gocpp::recv(s));
            auto abits = rec::allocBitsForIndex(gocpp::recv(s), 0);
            for(auto i = uintptr_t(0); i < uintptr_t(s->nelems); i++)
            {
                if(! rec::isMarked(gocpp::recv(mbits)) && (abits.index < uintptr_t(s->freeindex) || rec::isMarked(gocpp::recv(abits))))
                {
                    auto x = rec::base(gocpp::recv(s)) + i * s->elemsize;
                    if(debug.allocfreetrace != 0)
                    {
                        tracefree(unsafe::Pointer(x), size);
                    }
                    if(debug.clobberfree != 0)
                    {
                        clobberfree(unsafe::Pointer(x), size);
                    }
                    if(raceenabled && ! s->isUserArenaChunk)
                    {
                        racefree(unsafe::Pointer(x), size);
                    }
                    if(msanenabled && ! s->isUserArenaChunk)
                    {
                        msanfree(unsafe::Pointer(x), size);
                    }
                    if(asanenabled && ! s->isUserArenaChunk)
                    {
                        asanpoison(unsafe::Pointer(x), size);
                    }
                }
                rec::advance(gocpp::recv(mbits));
                rec::advance(gocpp::recv(abits));
            }
        }
        if(s->freeindex < s->nelems)
        {
            auto obj = uintptr_t(s->freeindex);
            if(((*rec::bytep(gocpp::recv(s->gcmarkBits), obj / 8) &^ *rec::bytep(gocpp::recv(s->allocBits), obj / 8)) >> (obj % 8)) != 0)
            {
                rec::reportZombies(gocpp::recv(s));
            }
            for(auto i = obj / 8 + 1; i < divRoundUp(uintptr_t(s->nelems), 8); i++)
            {
                if(*rec::bytep(gocpp::recv(s->gcmarkBits), i) &^ *rec::bytep(gocpp::recv(s->allocBits), i) != 0)
                {
                    rec::reportZombies(gocpp::recv(s));
                }
            }
        }
        auto nalloc = uint16_t(rec::countAlloc(gocpp::recv(s)));
        auto nfreed = s->allocCount - nalloc;
        if(nalloc > s->allocCount)
        {
            print("runtime: nelems="_s, s->nelems, " nalloc="_s, nalloc, " previous allocCount="_s, s->allocCount, " nfreed="_s, nfreed, "\n"_s);
            go_throw("sweep increased allocation count"_s);
        }
        s->allocCount = nalloc;
        s->freeindex = 0;
        s->freeIndexForScan = 0;
        if(traceEnabled())
        {
            rec::ptr(gocpp::recv(getg()->m->p))->trace.reclaimed += uintptr_t(nfreed) * s->elemsize;
        }
        s->allocBits = s->gcmarkBits;
        s->gcmarkBits = newMarkBits(uintptr_t(s->nelems));
        if(s->pinnerBits != nullptr)
        {
            rec::refreshPinnerBits(gocpp::recv(s));
        }
        rec::refillAllocCache(gocpp::recv(s), 0);
        if(auto state = rec::get(gocpp::recv(s->state)); state != mSpanInUse || s->sweepgen != sweepgen - 1)
        {
            print("mspan.sweep: state="_s, state, " sweepgen="_s, s->sweepgen, " mheap.sweepgen="_s, sweepgen, "\n"_s);
            go_throw("mspan.sweep: bad span state after sweep"_s);
        }
        if(s->sweepgen == sweepgen + 1 || s->sweepgen == sweepgen + 3)
        {
            go_throw("swept cached span"_s);
        }
        atomic::Store(& s->sweepgen, sweepgen);
        if(s->isUserArenaChunk)
        {
            if(preserve)
            {
                go_throw("sweep: tried to preserve a user arena span"_s);
            }
            if(nalloc > 0)
            {
                rec::push(gocpp::recv(rec::fullSwept(gocpp::recv(mheap_.central[spc].mcentral), sweepgen)), s);
                return false;
            }
            rec::Add(gocpp::recv(mheap_.pagesInUse), - s->npages);
            rec::set(gocpp::recv(s->state), mSpanDead);
            systemstack([=]() mutable -> void
            {
                if(s->list != & mheap_.userArena.quarantineList)
                {
                    go_throw("user arena span is on the wrong list"_s);
                }
                lock(& mheap_.lock);
                rec::remove(gocpp::recv(mheap_.userArena.quarantineList), s);
                rec::insert(gocpp::recv(mheap_.userArena.readyList), s);
                unlock(& mheap_.lock);
            });
            return false;
        }
        if(rec::sizeclass(gocpp::recv(spc)) != 0)
        {
            if(nfreed > 0)
            {
                s->needzero = 1;
                auto stats = rec::acquire(gocpp::recv(memstats.heapStats));
                atomic::Xadd64(& stats->smallFreeCount[rec::sizeclass(gocpp::recv(spc))], int64_t(nfreed));
                rec::release(gocpp::recv(memstats.heapStats));
                rec::Add(gocpp::recv(gcController.totalFree), int64_t(nfreed) * int64_t(s->elemsize));
            }
            if(! preserve)
            {
                if(nalloc == 0)
                {
                    rec::freeSpan(gocpp::recv(mheap_), s);
                    return true;
                }
                if(nalloc == s->nelems)
                {
                    rec::push(gocpp::recv(rec::fullSwept(gocpp::recv(mheap_.central[spc].mcentral), sweepgen)), s);
                }
                else
                {
                    rec::push(gocpp::recv(rec::partialSwept(gocpp::recv(mheap_.central[spc].mcentral), sweepgen)), s);
                }
            }
        }
        else
        if(! preserve)
        {
            if(nfreed != 0)
            {
                if(debug.efence > 0)
                {
                    s->limit = 0;
                    sysFault(unsafe::Pointer(rec::base(gocpp::recv(s))), size);
                }
                else
                {
                    rec::freeSpan(gocpp::recv(mheap_), s);
                }
                if(goexperiment::AllocHeaders && s->largeType != nullptr && s->largeType->TFlag & abi::TFlagUnrolledBitmap != 0)
                {
                    systemstack([=]() mutable -> void
                    {
                        auto s_tmp = spanOf(uintptr_t(unsafe::Pointer(s->largeType)));
                        auto& s = s_tmp;
                        rec::freeManual(gocpp::recv(mheap_), s, spanAllocPtrScalarBits);
                    });
                    *(uintptr_t*)(unsafe::Pointer(& s->largeType)) = 0;
                }
                auto stats = rec::acquire(gocpp::recv(memstats.heapStats));
                atomic::Xadd64(& stats->largeFreeCount, 1);
                atomic::Xadd64(& stats->largeFree, int64_t(size));
                rec::release(gocpp::recv(memstats.heapStats));
                rec::Add(gocpp::recv(gcController.totalFree), int64_t(size));
                return true;
            }
            rec::push(gocpp::recv(rec::fullSwept(gocpp::recv(mheap_.central[spc].mcentral), sweepgen)), s);
        }
        return false;
    }

    // reportZombies reports any marked but free objects in s and throws.
    //
    // This generally means one of the following:
    //
    // 1. User code converted a pointer to a uintptr and then back
    // unsafely, and a GC ran while the uintptr was the only reference to
    // an object.
    //
    // 2. User code (or a compiler bug) constructed a bad pointer that
    // points to a free slot, often a past-the-end pointer.
    //
    // 3. The GC two cycles ago missed a pointer and freed a live object,
    // but it was still live in the last cycle, so this GC cycle found a
    // pointer to that object and marked it.
    void rec::reportZombies(golang::runtime::mspan* s)
    {
        printlock();
        print("runtime: marked free object in span "_s, s, ", elemsize="_s, s->elemsize, " freeindex="_s, s->freeindex, " (bad use of unsafe.Pointer? try -d=checkptr)\n"_s);
        auto mbits = rec::markBitsForBase(gocpp::recv(s));
        auto abits = rec::allocBitsForIndex(gocpp::recv(s), 0);
        for(auto i = uintptr_t(0); i < uintptr_t(s->nelems); i++)
        {
            auto addr = rec::base(gocpp::recv(s)) + i * s->elemsize;
            print(hex(addr));
            auto alloc = i < uintptr_t(s->freeindex) || rec::isMarked(gocpp::recv(abits));
            if(alloc)
            {
                print(" alloc"_s);
            }
            else
            {
                print(" free "_s);
            }
            if(rec::isMarked(gocpp::recv(mbits)))
            {
                print(" marked  "_s);
            }
            else
            {
                print(" unmarked"_s);
            }
            auto zombie = rec::isMarked(gocpp::recv(mbits)) && ! alloc;
            if(zombie)
            {
                print(" zombie"_s);
            }
            print("\n"_s);
            if(zombie)
            {
                auto length = s->elemsize;
                if(length > 1024)
                {
                    length = 1024;
                }
                hexdumpWords(addr, addr + length, nullptr);
            }
            rec::advance(gocpp::recv(mbits));
            rec::advance(gocpp::recv(abits));
        }
        go_throw("found pointer to free object"_s);
    }

    // deductSweepCredit deducts sweep credit for allocating a span of
    // size spanBytes. This must be performed *before* the span is
    // allocated to ensure the system has enough credit. If necessary, it
    // performs sweeping to prevent going in to debt. If the caller will
    // also sweep pages (e.g., for a large allocation), it can pass a
    // non-zero callerSweepPages to leave that many pages unswept.
    //
    // deductSweepCredit makes a worst-case assumption that all spanBytes
    // bytes of the ultimately allocated span will be available for object
    // allocation.
    //
    // deductSweepCredit is the core of the "proportional sweep" system.
    // It uses statistics gathered by the garbage collector to perform
    // enough sweeping so that all pages are swept during the concurrent
    // sweep phase between GC cycles.
    //
    // mheap_ must NOT be locked.
    void deductSweepCredit(uintptr_t spanBytes, uintptr_t callerSweepPages)
    {
        if(mheap_.sweepPagesPerByte == 0)
        {
            return;
        }
        auto trace = traceAcquire();
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::GCSweepStart(gocpp::recv(trace));
            traceRelease(trace);
        }
        retry:
        auto sweptBasis = rec::Load(gocpp::recv(mheap_.pagesSweptBasis));
        auto live = rec::Load(gocpp::recv(gcController.heapLive));
        auto liveBasis = mheap_.sweepHeapLiveBasis;
        auto newHeapLive = spanBytes;
        if(liveBasis < live)
        {
            newHeapLive += uintptr_t(live - liveBasis);
        }
        auto pagesTarget = int64_t(mheap_.sweepPagesPerByte * double(newHeapLive)) - int64_t(callerSweepPages);
        for(; pagesTarget > int64_t(rec::Load(gocpp::recv(mheap_.pagesSwept)) - sweptBasis); )
        {
            if(sweepone() == ~ uintptr_t(0))
            {
                mheap_.sweepPagesPerByte = 0;
                break;
            }
            if(rec::Load(gocpp::recv(mheap_.pagesSweptBasis)) != sweptBasis)
            {
                goto retry;
            }
        }
        trace = traceAcquire();
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::GCSweepDone(gocpp::recv(trace));
            traceRelease(trace);
        }
    }

    // clobberfree sets the memory content at x to bad content, for debugging
    // purposes.
    void clobberfree(unsafe::Pointer x, uintptr_t size)
    {
        for(auto i = uintptr_t(0); i < size; i += 4)
        {
            *(uint32_t*)(add(x, i)) = 0xdeadbeef;
        }
    }

    // gcPaceSweeper updates the sweeper's pacing parameters.
    //
    // Must be called whenever the GC's pacing is updated.
    //
    // The world must be stopped, or mheap_.lock must be held.
    void gcPaceSweeper(uint64_t trigger)
    {
        assertWorldStoppedOrLockHeld(& mheap_.lock);
        if(isSweepDone())
        {
            mheap_.sweepPagesPerByte = 0;
        }
        else
        {
            auto heapLiveBasis = rec::Load(gocpp::recv(gcController.heapLive));
            auto heapDistance = int64_t(trigger) - int64_t(heapLiveBasis);
            heapDistance -= 1024 * 1024;
            if(heapDistance < _PageSize)
            {
                heapDistance = _PageSize;
            }
            auto pagesSwept = rec::Load(gocpp::recv(mheap_.pagesSwept));
            auto pagesInUse = rec::Load(gocpp::recv(mheap_.pagesInUse));
            auto sweepDistancePages = int64_t(pagesInUse) - int64_t(pagesSwept);
            if(sweepDistancePages <= 0)
            {
                mheap_.sweepPagesPerByte = 0;
            }
            else
            {
                mheap_.sweepPagesPerByte = double(sweepDistancePages) / double(heapDistance);
                mheap_.sweepHeapLiveBasis = heapLiveBasis;
                rec::Store(gocpp::recv(mheap_.pagesSweptBasis), pagesSwept);
            }
        }
    }

}

