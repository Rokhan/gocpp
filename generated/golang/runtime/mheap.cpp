// generated by GoCpp from file '$(ImportDir)/runtime/mheap.go'
#include <complex>
#include <functional>
#include <iostream>
#include <iomanip>
#include <map>
#include <string>
#include <tuple>
#include <vector>

#include "golang/runtime/mheap.h"
#include "gocpp/support.h"

#include "golang/internal/abi/type.h"
#include "golang/internal/chacha8rand/chacha8.h"
#include "golang/internal/cpu/cpu.h"
#include "golang/internal/cpu/cpu_x86.h"
#include "golang/internal/goarch/goarch.h"
#include "golang/internal/goexperiment/exp_allocheaders_on.h"
#include "golang/runtime/asan0.h"
#include "golang/runtime/cgocall.h"
#include "golang/runtime/chan.h"
#include "golang/runtime/coro.h"
#include "golang/runtime/debuglog_off.h"
#include "golang/runtime/extern.h"
#include "golang/runtime/internal/atomic/atomic_amd64.h"
#include "golang/runtime/internal/atomic/stubs.h"
#include "golang/runtime/internal/atomic/types.h"
#include "golang/runtime/internal/sys/nih.h"
#include "golang/runtime/lock_sema.h"
#include "golang/runtime/lockrank.h"
#include "golang/runtime/lockrank_off.h"
#include "golang/runtime/malloc.h"
#include "golang/runtime/mbitmap.h"
#include "golang/runtime/mbitmap_allocheaders.h"
#include "golang/runtime/mcache.h"
#include "golang/runtime/mcentral.h"
#include "golang/runtime/mcheckmark.h"
#include "golang/runtime/mem.h"
#include "golang/runtime/mfinal.h"
#include "golang/runtime/mfixalloc.h"
#include "golang/runtime/mgc.h"
#include "golang/runtime/mgclimit.h"
#include "golang/runtime/mgcmark.h"
#include "golang/runtime/mgcpacer.h"
#include "golang/runtime/mgcscavenge.h"
#include "golang/runtime/mgcstack.h"
#include "golang/runtime/mgcsweep.h"
#include "golang/runtime/mgcwork.h"
#include "golang/runtime/mpagealloc.h"
#include "golang/runtime/mpagecache.h"
#include "golang/runtime/mpallocbits.h"
#include "golang/runtime/mprof.h"
#include "golang/runtime/mranges.h"
#include "golang/runtime/msan0.h"
#include "golang/runtime/mspanset.h"
#include "golang/runtime/mstats.h"
#include "golang/runtime/mwbbuf.h"
#include "golang/runtime/os_windows.h"
#include "golang/runtime/pagetrace_off.h"
#include "golang/runtime/panic.h"
#include "golang/runtime/pinner.h"
#include "golang/runtime/print.h"
#include "golang/runtime/proc.h"
#include "golang/runtime/runtime1.h"
#include "golang/runtime/runtime2.h"
#include "golang/runtime/signal_windows.h"
#include "golang/runtime/sizeclasses.h"
#include "golang/runtime/slice.h"
#include "golang/runtime/stack.h"
#include "golang/runtime/stubs.h"
#include "golang/runtime/symtab.h"
#include "golang/runtime/time.h"
#include "golang/runtime/time_nofake.h"
#include "golang/runtime/trace2buf.h"
#include "golang/runtime/trace2runtime.h"
#include "golang/runtime/trace2status.h"
#include "golang/runtime/trace2time.h"
#include "golang/runtime/type.h"
#include "golang/unsafe/unsafe.h"

namespace golang::runtime
{
    namespace rec
    {
        using namespace mocklib::rec;
        using atomic::rec::Add;
        using atomic::rec::CompareAndSwap;
        using atomic::rec::Load;
        using atomic::rec::Store;
    }

    // minPhysPageSize is a lower-bound on the physical page size. The
    // true physical page size may be larger than this. In contrast,
    // sys.PhysPageSize is an upper-bound on the physical page size.
    // maxPhysPageSize is the maximum page size the runtime supports.
    // maxPhysHugePageSize sets an upper-bound on the maximum huge page size
    // that the runtime supports.
    // pagesPerReclaimerChunk indicates how many pages to scan from the
    // pageInUse bitmap at a time. Used by the page reclaimer.
    //
    // Higher values reduce contention on scanning indexes (such as
    // h.reclaimIndex), but increase the minimum latency of the
    // operation.
    //
    // The time required to scan this many pages can vary a lot depending
    // on how many spans are actually freed. Experimentally, it can
    // scan for pages at ~300 GB/ms on a 2.6GHz Core i7, but can only
    // free spans at ~32 MB/ms. Using 512 pages bounds this at
    // roughly 100Âµs.
    //
    // Must be a multiple of the pageInUse bitmap element size and
    // must also evenly divide pagesPerArena.
    // physPageAlignedStacks indicates whether stack allocations must be
    // physical page aligned. This is a requirement for MAP_STACK on
    // OpenBSD.
    bool physPageAlignedStacks = GOOS == "openbsd"s;
    
    template<typename T> requires gocpp::GoStruct<T>
    gocpp_id_0::operator T()
    {
        T result;
        result.base = this->base;
        result.end = this->end;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool gocpp_id_0::operator==(const T& ref) const
    {
        if (base != ref.base) return false;
        if (end != ref.end) return false;
        return true;
    }

    std::ostream& gocpp_id_0::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << base;
        os << " " << end;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct gocpp_id_0& value)
    {
        return value.PrintTo(os);
    }


    
    template<typename T> requires gocpp::GoStruct<T>
    gocpp_id_1::operator T()
    {
        T result;
        result.mcentral = this->mcentral;
        result.pad = this->pad;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool gocpp_id_1::operator==(const T& ref) const
    {
        if (mcentral != ref.mcentral) return false;
        if (pad != ref.pad) return false;
        return true;
    }

    std::ostream& gocpp_id_1::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << mcentral;
        os << " " << pad;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct gocpp_id_1& value)
    {
        return value.PrintTo(os);
    }


    
    template<typename T> requires gocpp::GoStruct<T>
    gocpp_id_2::operator T()
    {
        T result;
        result.arenaHints = this->arenaHints;
        result.quarantineList = this->quarantineList;
        result.readyList = this->readyList;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool gocpp_id_2::operator==(const T& ref) const
    {
        if (arenaHints != ref.arenaHints) return false;
        if (quarantineList != ref.quarantineList) return false;
        if (readyList != ref.readyList) return false;
        return true;
    }

    std::ostream& gocpp_id_2::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << arenaHints;
        os << " " << quarantineList;
        os << " " << readyList;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct gocpp_id_2& value)
    {
        return value.PrintTo(os);
    }


    // Main malloc heap.
    // The heap itself is the "free" and "scav" treaps,
    // but all the other global data is here too.
    //
    // mheap must not be heap-allocated because it contains mSpanLists,
    // which must not be heap-allocated.
    
    template<typename T> requires gocpp::GoStruct<T>
    mheap::operator T()
    {
        T result;
        result._1 = this->_1;
        result.lock = this->lock;
        result.pages = this->pages;
        result.sweepgen = this->sweepgen;
        result.allspans = this->allspans;
        result.pagesInUse = this->pagesInUse;
        result.pagesSwept = this->pagesSwept;
        result.pagesSweptBasis = this->pagesSweptBasis;
        result.sweepHeapLiveBasis = this->sweepHeapLiveBasis;
        result.sweepPagesPerByte = this->sweepPagesPerByte;
        result.reclaimIndex = this->reclaimIndex;
        result.reclaimCredit = this->reclaimCredit;
        result._2 = this->_2;
        result.arenas = this->arenas;
        result.arenasHugePages = this->arenasHugePages;
        result.heapArenaAlloc = this->heapArenaAlloc;
        result.arenaHints = this->arenaHints;
        result.arena = this->arena;
        result.allArenas = this->allArenas;
        result.sweepArenas = this->sweepArenas;
        result.markArenas = this->markArenas;
        result.curArena = this->curArena;
        result.central = this->central;
        result.spanalloc = this->spanalloc;
        result.cachealloc = this->cachealloc;
        result.specialfinalizeralloc = this->specialfinalizeralloc;
        result.specialprofilealloc = this->specialprofilealloc;
        result.specialReachableAlloc = this->specialReachableAlloc;
        result.specialPinCounterAlloc = this->specialPinCounterAlloc;
        result.speciallock = this->speciallock;
        result.arenaHintAlloc = this->arenaHintAlloc;
        result.userArena = this->userArena;
        result.unused = this->unused;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool mheap::operator==(const T& ref) const
    {
        if (_1 != ref._1) return false;
        if (lock != ref.lock) return false;
        if (pages != ref.pages) return false;
        if (sweepgen != ref.sweepgen) return false;
        if (allspans != ref.allspans) return false;
        if (pagesInUse != ref.pagesInUse) return false;
        if (pagesSwept != ref.pagesSwept) return false;
        if (pagesSweptBasis != ref.pagesSweptBasis) return false;
        if (sweepHeapLiveBasis != ref.sweepHeapLiveBasis) return false;
        if (sweepPagesPerByte != ref.sweepPagesPerByte) return false;
        if (reclaimIndex != ref.reclaimIndex) return false;
        if (reclaimCredit != ref.reclaimCredit) return false;
        if (_2 != ref._2) return false;
        if (arenas != ref.arenas) return false;
        if (arenasHugePages != ref.arenasHugePages) return false;
        if (heapArenaAlloc != ref.heapArenaAlloc) return false;
        if (arenaHints != ref.arenaHints) return false;
        if (arena != ref.arena) return false;
        if (allArenas != ref.allArenas) return false;
        if (sweepArenas != ref.sweepArenas) return false;
        if (markArenas != ref.markArenas) return false;
        if (curArena != ref.curArena) return false;
        if (central != ref.central) return false;
        if (spanalloc != ref.spanalloc) return false;
        if (cachealloc != ref.cachealloc) return false;
        if (specialfinalizeralloc != ref.specialfinalizeralloc) return false;
        if (specialprofilealloc != ref.specialprofilealloc) return false;
        if (specialReachableAlloc != ref.specialReachableAlloc) return false;
        if (specialPinCounterAlloc != ref.specialPinCounterAlloc) return false;
        if (speciallock != ref.speciallock) return false;
        if (arenaHintAlloc != ref.arenaHintAlloc) return false;
        if (userArena != ref.userArena) return false;
        if (unused != ref.unused) return false;
        return true;
    }

    std::ostream& mheap::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << _1;
        os << " " << lock;
        os << " " << pages;
        os << " " << sweepgen;
        os << " " << allspans;
        os << " " << pagesInUse;
        os << " " << pagesSwept;
        os << " " << pagesSweptBasis;
        os << " " << sweepHeapLiveBasis;
        os << " " << sweepPagesPerByte;
        os << " " << reclaimIndex;
        os << " " << reclaimCredit;
        os << " " << _2;
        os << " " << arenas;
        os << " " << arenasHugePages;
        os << " " << heapArenaAlloc;
        os << " " << arenaHints;
        os << " " << arena;
        os << " " << allArenas;
        os << " " << sweepArenas;
        os << " " << markArenas;
        os << " " << curArena;
        os << " " << central;
        os << " " << spanalloc;
        os << " " << cachealloc;
        os << " " << specialfinalizeralloc;
        os << " " << specialprofilealloc;
        os << " " << specialReachableAlloc;
        os << " " << specialPinCounterAlloc;
        os << " " << speciallock;
        os << " " << arenaHintAlloc;
        os << " " << userArena;
        os << " " << unused;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct mheap& value)
    {
        return value.PrintTo(os);
    }

    mheap mheap_;
    // A heapArena stores metadata for a heap arena. heapArenas are stored
    // outside of the Go heap and accessed via the mheap_.arenas index.
    
    template<typename T> requires gocpp::GoStruct<T>
    heapArena::operator T()
    {
        T result;
        result._1 = this->_1;
        result.heapArenaPtrScalar = this->heapArenaPtrScalar;
        result.spans = this->spans;
        result.pageInUse = this->pageInUse;
        result.pageMarks = this->pageMarks;
        result.pageSpecials = this->pageSpecials;
        result.checkmarks = this->checkmarks;
        result.zeroedBase = this->zeroedBase;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool heapArena::operator==(const T& ref) const
    {
        if (_1 != ref._1) return false;
        if (heapArenaPtrScalar != ref.heapArenaPtrScalar) return false;
        if (spans != ref.spans) return false;
        if (pageInUse != ref.pageInUse) return false;
        if (pageMarks != ref.pageMarks) return false;
        if (pageSpecials != ref.pageSpecials) return false;
        if (checkmarks != ref.checkmarks) return false;
        if (zeroedBase != ref.zeroedBase) return false;
        return true;
    }

    std::ostream& heapArena::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << _1;
        os << " " << heapArenaPtrScalar;
        os << " " << spans;
        os << " " << pageInUse;
        os << " " << pageMarks;
        os << " " << pageSpecials;
        os << " " << checkmarks;
        os << " " << zeroedBase;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct heapArena& value)
    {
        return value.PrintTo(os);
    }

    // arenaHint is a hint for where to grow the heap arenas. See
    // mheap_.arenaHints.
    
    template<typename T> requires gocpp::GoStruct<T>
    arenaHint::operator T()
    {
        T result;
        result._1 = this->_1;
        result.addr = this->addr;
        result.down = this->down;
        result.next = this->next;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool arenaHint::operator==(const T& ref) const
    {
        if (_1 != ref._1) return false;
        if (addr != ref.addr) return false;
        if (down != ref.down) return false;
        if (next != ref.next) return false;
        return true;
    }

    std::ostream& arenaHint::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << _1;
        os << " " << addr;
        os << " " << down;
        os << " " << next;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct arenaHint& value)
    {
        return value.PrintTo(os);
    }

    // An mspan representing actual memory has state mSpanInUse,
    // mSpanManual, or mSpanFree. Transitions between these states are
    // constrained as follows:
    //
    //   - A span may transition from free to in-use or manual during any GC
    //     phase.
    //
    //   - During sweeping (gcphase == _GCoff), a span may transition from
    //     in-use to free (as a result of sweeping) or manual to free (as a
    //     result of stacks being freed).
    //
    //   - During GC (gcphase != _GCoff), a span *must not* transition from
    //     manual or in-use to free. Because concurrent GC may read a pointer
    //     and then look up its span, the span state must be monotonic.
    //
    // Setting mspan.state to mSpanInUse or mSpanManual must be done
    // atomically and only after all other span fields are valid.
    // Likewise, if inspecting a span is contingent on it being
    // mSpanInUse, the state should be loaded atomically and checked
    // before depending on other fields. This allows the garbage collector
    // to safely deal with potentially invalid pointers, since resolving
    // such pointers may race with a span being allocated.
    // mSpanStateNames are the names of the span states, indexed by
    // mSpanState.
    gocpp::slice<std::string> mSpanStateNames = gocpp::slice<std::string> {"mSpanDead"s, "mSpanInUse"s, "mSpanManual"s};
    // mSpanStateBox holds an atomic.Uint8 to provide atomic operations on
    // an mSpanState. This is a separate type to disallow accidental comparison
    // or assignment with mSpanState.
    
    template<typename T> requires gocpp::GoStruct<T>
    mSpanStateBox::operator T()
    {
        T result;
        result.s = this->s;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool mSpanStateBox::operator==(const T& ref) const
    {
        if (s != ref.s) return false;
        return true;
    }

    std::ostream& mSpanStateBox::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << s;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct mSpanStateBox& value)
    {
        return value.PrintTo(os);
    }

    //go:nosplit
    void rec::set(struct mSpanStateBox* b, golang::runtime::mSpanState s)
    {
        rec::Store(gocpp::recv(b->s), uint8_t(s));
    }

    //go:nosplit
    runtime::mSpanState rec::get(struct mSpanStateBox* b)
    {
        return mSpanState(rec::Load(gocpp::recv(b->s)));
    }

    // mSpanList heads a linked list of spans.
    
    template<typename T> requires gocpp::GoStruct<T>
    mSpanList::operator T()
    {
        T result;
        result._1 = this->_1;
        result.first = this->first;
        result.last = this->last;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool mSpanList::operator==(const T& ref) const
    {
        if (_1 != ref._1) return false;
        if (first != ref.first) return false;
        if (last != ref.last) return false;
        return true;
    }

    std::ostream& mSpanList::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << _1;
        os << " " << first;
        os << " " << last;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct mSpanList& value)
    {
        return value.PrintTo(os);
    }

    
    template<typename T> requires gocpp::GoStruct<T>
    mspan::operator T()
    {
        T result;
        result._1 = this->_1;
        result.next = this->next;
        result.prev = this->prev;
        result.list = this->list;
        result.startAddr = this->startAddr;
        result.npages = this->npages;
        result.manualFreeList = this->manualFreeList;
        result.freeindex = this->freeindex;
        result.nelems = this->nelems;
        result.freeIndexForScan = this->freeIndexForScan;
        result.allocCache = this->allocCache;
        result.allocBits = this->allocBits;
        result.gcmarkBits = this->gcmarkBits;
        result.pinnerBits = this->pinnerBits;
        result.sweepgen = this->sweepgen;
        result.divMul = this->divMul;
        result.allocCount = this->allocCount;
        result.spanclass = this->spanclass;
        result.state = this->state;
        result.needzero = this->needzero;
        result.isUserArenaChunk = this->isUserArenaChunk;
        result.allocCountBeforeCache = this->allocCountBeforeCache;
        result.elemsize = this->elemsize;
        result.limit = this->limit;
        result.speciallock = this->speciallock;
        result.specials = this->specials;
        result.userArenaChunkFree = this->userArenaChunkFree;
        result.largeType = this->largeType;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool mspan::operator==(const T& ref) const
    {
        if (_1 != ref._1) return false;
        if (next != ref.next) return false;
        if (prev != ref.prev) return false;
        if (list != ref.list) return false;
        if (startAddr != ref.startAddr) return false;
        if (npages != ref.npages) return false;
        if (manualFreeList != ref.manualFreeList) return false;
        if (freeindex != ref.freeindex) return false;
        if (nelems != ref.nelems) return false;
        if (freeIndexForScan != ref.freeIndexForScan) return false;
        if (allocCache != ref.allocCache) return false;
        if (allocBits != ref.allocBits) return false;
        if (gcmarkBits != ref.gcmarkBits) return false;
        if (pinnerBits != ref.pinnerBits) return false;
        if (sweepgen != ref.sweepgen) return false;
        if (divMul != ref.divMul) return false;
        if (allocCount != ref.allocCount) return false;
        if (spanclass != ref.spanclass) return false;
        if (state != ref.state) return false;
        if (needzero != ref.needzero) return false;
        if (isUserArenaChunk != ref.isUserArenaChunk) return false;
        if (allocCountBeforeCache != ref.allocCountBeforeCache) return false;
        if (elemsize != ref.elemsize) return false;
        if (limit != ref.limit) return false;
        if (speciallock != ref.speciallock) return false;
        if (specials != ref.specials) return false;
        if (userArenaChunkFree != ref.userArenaChunkFree) return false;
        if (largeType != ref.largeType) return false;
        return true;
    }

    std::ostream& mspan::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << _1;
        os << " " << next;
        os << " " << prev;
        os << " " << list;
        os << " " << startAddr;
        os << " " << npages;
        os << " " << manualFreeList;
        os << " " << freeindex;
        os << " " << nelems;
        os << " " << freeIndexForScan;
        os << " " << allocCache;
        os << " " << allocBits;
        os << " " << gcmarkBits;
        os << " " << pinnerBits;
        os << " " << sweepgen;
        os << " " << divMul;
        os << " " << allocCount;
        os << " " << spanclass;
        os << " " << state;
        os << " " << needzero;
        os << " " << isUserArenaChunk;
        os << " " << allocCountBeforeCache;
        os << " " << elemsize;
        os << " " << limit;
        os << " " << speciallock;
        os << " " << specials;
        os << " " << userArenaChunkFree;
        os << " " << largeType;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct mspan& value)
    {
        return value.PrintTo(os);
    }

    uintptr_t rec::base(struct mspan* s)
    {
        return s->startAddr;
    }

    std::tuple<uintptr_t, uintptr_t, uintptr_t> rec::layout(struct mspan* s)
    {
        uintptr_t size;
        uintptr_t n;
        uintptr_t total;
        total = s->npages << _PageShift;
        size = s->elemsize;
        if(size > 0)
        {
            n = total / size;
        }
        return {size, n, total};
    }

    // recordspan adds a newly allocated span to h.allspans.
    //
    // This only happens the first time a span is allocated from
    // mheap.spanalloc (it is not called when a span is reused).
    //
    // Write barriers are disallowed here because it can be called from
    // gcWork when allocating new workbufs. However, because it's an
    // indirect call from the fixalloc initializer, the compiler can't see
    // this.
    //
    // The heap lock must be held.
    //
    //go:nowritebarrierrec
    void recordspan(unsafe::Pointer vh, unsafe::Pointer p)
    {
        auto h = (mheap*)(vh);
        auto s = (mspan*)(p);
        assertLockHeld(& h->lock);
        if(len(h->allspans) >= cap(h->allspans))
        {
            auto n = 64 * 1024 / goarch::PtrSize;
            if(n < cap(h->allspans) * 3 / 2)
            {
                n = cap(h->allspans) * 3 / 2;
            }
            gocpp::slice<mspan*> go_new = {};
            auto sp = (slice*)(unsafe::Pointer(& go_new));
            sp->array = sysAlloc(uintptr_t(n) * goarch::PtrSize, & memstats.other_sys);
            if(sp->array == nullptr)
            {
                go_throw("runtime: cannot allocate memory"s);
            }
            sp->len = len(h->allspans);
            sp->cap = n;
            if(len(h->allspans) > 0)
            {
                copy(go_new, h->allspans);
            }
            auto oldAllspans = h->allspans;
            *(notInHeapSlice*)(unsafe::Pointer(& h->allspans)) = *(notInHeapSlice*)(unsafe::Pointer(& go_new));
            if(len(oldAllspans) != 0)
            {
                sysFree(unsafe::Pointer(& oldAllspans[0]), uintptr_t(cap(oldAllspans)) * gocpp::Sizeof<mspan*>(), & memstats.other_sys);
            }
        }
        h->allspans = h->allspans.make_slice(0, len(h->allspans) + 1);
        h->allspans[len(h->allspans) - 1] = s;
    }

    // A spanClass represents the size class and noscan-ness of a span.
    //
    // Each size class has a noscan spanClass and a scan spanClass. The
    // noscan spanClass contains only noscan objects, which do not contain
    // pointers and thus do not need to be scanned by the garbage
    // collector.
    runtime::spanClass makeSpanClass(uint8_t sizeclass, bool noscan)
    {
        return spanClass(sizeclass << 1) | spanClass(bool2int(noscan));
    }

    //go:nosplit
    int8_t rec::sizeclass(golang::runtime::spanClass sc)
    {
        return int8_t(sc >> 1);
    }

    //go:nosplit
    bool rec::noscan(golang::runtime::spanClass sc)
    {
        return sc & 1 != 0;
    }

    // arenaIndex returns the index into mheap_.arenas of the arena
    // containing metadata for p. This index combines of an index into the
    // L1 map and an index into the L2 map and should be used as
    // mheap_.arenas[ai.l1()][ai.l2()].
    //
    // If p is outside the range of valid heap addresses, either l1() or
    // l2() will be out of bounds.
    //
    // It is nosplit because it's called by spanOf and several other
    // nosplit functions.
    //
    //go:nosplit
    runtime::arenaIdx arenaIndex(uintptr_t p)
    {
        return arenaIdx((p - arenaBaseOffset) / heapArenaBytes);
    }

    // arenaBase returns the low address of the region covered by heap
    // arena i.
    uintptr_t arenaBase(golang::runtime::arenaIdx i)
    {
        return uintptr_t(i) * heapArenaBytes + arenaBaseOffset;
    }

    // l1 returns the "l1" portion of an arenaIdx.
    //
    // Marked nosplit because it's called by spanOf and other nosplit
    // functions.
    //
    //go:nosplit
    unsigned int rec::l1(golang::runtime::arenaIdx i)
    {
        if(arenaL1Bits == 0)
        {
            return 0;
        }
        else
        {
            return (unsigned int)(i) >> arenaL1Shift;
        }
    }

    // l2 returns the "l2" portion of an arenaIdx.
    //
    // Marked nosplit because it's called by spanOf and other nosplit funcs.
    // functions.
    //
    //go:nosplit
    unsigned int rec::l2(golang::runtime::arenaIdx i)
    {
        if(arenaL1Bits == 0)
        {
            return (unsigned int)(i);
        }
        else
        {
            return (unsigned int)(i) & ((1 << arenaL2Bits) - 1);
        }
    }

    // inheap reports whether b is a pointer into a (potentially dead) heap object.
    // It returns false for pointers into mSpanManual spans.
    // Non-preemptible because it is used by write barriers.
    //
    //go:nowritebarrier
    //go:nosplit
    bool inheap(uintptr_t b)
    {
        return spanOfHeap(b) != nullptr;
    }

    // inHeapOrStack is a variant of inheap that returns true for pointers
    // into any allocated heap span.
    //
    //go:nowritebarrier
    //go:nosplit
    bool inHeapOrStack(uintptr_t b)
    {
        auto s = spanOf(b);
        if(s == nullptr || b < rec::base(gocpp::recv(s)))
        {
            return false;
        }
        //Go switch emulation
        {
            auto condition = rec::get(gocpp::recv(s->state));
            int conditionId = -1;
            if(condition == mSpanInUse) { conditionId = 0; }
            else if(condition == mSpanManual) { conditionId = 1; }
            switch(conditionId)
            {
                case 0:
                case 1:
                    return b < s->limit;
                    break;
                default:
                    return false;
                    break;
            }
        }
    }

    // spanOf returns the span of p. If p does not point into the heap
    // arena or no span has ever contained p, spanOf returns nil.
    //
    // If p does not point to allocated memory, this may return a non-nil
    // span that does *not* contain p. If this is a possibility, the
    // caller should either call spanOfHeap or check the span bounds
    // explicitly.
    //
    // Must be nosplit because it has callers that are nosplit.
    //
    //go:nosplit
    struct mspan* spanOf(uintptr_t p)
    {
        auto ri = arenaIndex(p);
        if(arenaL1Bits == 0)
        {
            if(rec::l2(gocpp::recv(ri)) >= (unsigned int)(len(mheap_.arenas[0])))
            {
                return nullptr;
            }
        }
        else
        {
            if(rec::l1(gocpp::recv(ri)) >= (unsigned int)(len(mheap_.arenas)))
            {
                return nullptr;
            }
        }
        auto l2 = mheap_.arenas[rec::l1(gocpp::recv(ri))];
        if(arenaL1Bits != 0 && l2 == nullptr)
        {
            return nullptr;
        }
        auto ha = l2[rec::l2(gocpp::recv(ri))];
        if(ha == nullptr)
        {
            return nullptr;
        }
        return ha->spans[(p / pageSize) % pagesPerArena];
    }

    // spanOfUnchecked is equivalent to spanOf, but the caller must ensure
    // that p points into an allocated heap arena.
    //
    // Must be nosplit because it has callers that are nosplit.
    //
    //go:nosplit
    struct mspan* spanOfUnchecked(uintptr_t p)
    {
        auto ai = arenaIndex(p);
        return mheap_.arenas[rec::l1(gocpp::recv(ai))][rec::l2(gocpp::recv(ai))]->spans[(p / pageSize) % pagesPerArena];
    }

    // spanOfHeap is like spanOf, but returns nil if p does not point to a
    // heap object.
    //
    // Must be nosplit because it has callers that are nosplit.
    //
    //go:nosplit
    struct mspan* spanOfHeap(uintptr_t p)
    {
        auto s = spanOf(p);
        if(s == nullptr || rec::get(gocpp::recv(s->state)) != mSpanInUse || p < rec::base(gocpp::recv(s)) || p >= s->limit)
        {
            return nullptr;
        }
        return s;
    }

    // pageIndexOf returns the arena, page index, and page mask for pointer p.
    // The caller must ensure p is in the heap.
    std::tuple<struct heapArena*, uintptr_t, uint8_t> pageIndexOf(uintptr_t p)
    {
        struct heapArena* arena;
        uintptr_t pageIdx;
        uint8_t pageMask;
        auto ai = arenaIndex(p);
        arena = mheap_.arenas[rec::l1(gocpp::recv(ai))][rec::l2(gocpp::recv(ai))];
        pageIdx = ((p / pageSize) / 8) % uintptr_t(len(arena->pageInUse));
        pageMask = (unsigned char)(1 << ((p / pageSize) % 8));
        return {arena, pageIdx, pageMask};
    }

    // Initialize the heap.
    void rec::init(struct mheap* h)
    {
        lockInit(& h->lock, lockRankMheap);
        lockInit(& h->speciallock, lockRankMheapSpecial);
        rec::init(gocpp::recv(h->spanalloc), gocpp::Sizeof<mspan>(), recordspan, unsafe::Pointer(h), & memstats.mspan_sys);
        rec::init(gocpp::recv(h->cachealloc), gocpp::Sizeof<mcache>(), nullptr, nullptr, & memstats.mcache_sys);
        rec::init(gocpp::recv(h->specialfinalizeralloc), gocpp::Sizeof<specialfinalizer>(), nullptr, nullptr, & memstats.other_sys);
        rec::init(gocpp::recv(h->specialprofilealloc), gocpp::Sizeof<specialprofile>(), nullptr, nullptr, & memstats.other_sys);
        rec::init(gocpp::recv(h->specialReachableAlloc), gocpp::Sizeof<specialReachable>(), nullptr, nullptr, & memstats.other_sys);
        rec::init(gocpp::recv(h->specialPinCounterAlloc), gocpp::Sizeof<specialPinCounter>(), nullptr, nullptr, & memstats.other_sys);
        rec::init(gocpp::recv(h->arenaHintAlloc), gocpp::Sizeof<arenaHint>(), nullptr, nullptr, & memstats.other_sys);
        h->spanalloc.zero = false;
        for(auto [i, gocpp_ignored] : h->central)
        {
            rec::init(gocpp::recv(h->central[i].mcentral), spanClass(i));
        }
        rec::init(gocpp::recv(h->pages), & h->lock, & memstats.gcMiscSys, false);
    }

    // reclaim sweeps and reclaims at least npage pages into the heap.
    // It is called before allocating npage pages to keep growth in check.
    //
    // reclaim implements the page-reclaimer half of the sweeper.
    //
    // h.lock must NOT be held.
    void rec::reclaim(struct mheap* h, uintptr_t npage)
    {
        if(rec::Load(gocpp::recv(h->reclaimIndex)) >= (1 << 63))
        {
            return;
        }
        auto mp = acquirem();
        auto trace = traceAcquire();
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::GCSweepStart(gocpp::recv(trace));
            traceRelease(trace);
        }
        auto arenas = h->sweepArenas;
        auto locked = false;
        for(; npage > 0; )
        {
            if(auto credit = rec::Load(gocpp::recv(h->reclaimCredit)); credit > 0)
            {
                auto take = credit;
                if(take > npage)
                {
                    take = npage;
                }
                if(rec::CompareAndSwap(gocpp::recv(h->reclaimCredit), credit, credit - take))
                {
                    npage -= take;
                }
                continue;
            }
            auto idx = uintptr_t(rec::Add(gocpp::recv(h->reclaimIndex), pagesPerReclaimerChunk) - pagesPerReclaimerChunk);
            if(idx / pagesPerArena >= uintptr_t(len(arenas)))
            {
                rec::Store(gocpp::recv(h->reclaimIndex), 1 << 63);
                break;
            }
            if(! locked)
            {
                lock(& h->lock);
                locked = true;
            }
            auto nfound = rec::reclaimChunk(gocpp::recv(h), arenas, idx, pagesPerReclaimerChunk);
            if(nfound <= npage)
            {
                npage -= nfound;
            }
            else
            {
                rec::Add(gocpp::recv(h->reclaimCredit), nfound - npage);
                npage = 0;
            }
        }
        if(locked)
        {
            unlock(& h->lock);
        }
        trace = traceAcquire();
        if(rec::ok(gocpp::recv(trace)))
        {
            rec::GCSweepDone(gocpp::recv(trace));
            traceRelease(trace);
        }
        releasem(mp);
    }

    // reclaimChunk sweeps unmarked spans that start at page indexes [pageIdx, pageIdx+n).
    // It returns the number of pages returned to the heap.
    //
    // h.lock must be held and the caller must be non-preemptible. Note: h.lock may be
    // temporarily unlocked and re-locked in order to do sweeping or if tracing is
    // enabled.
    uintptr_t rec::reclaimChunk(struct mheap* h, gocpp::slice<golang::runtime::arenaIdx> arenas, uintptr_t pageIdx, uintptr_t n)
    {
        assertLockHeld(& h->lock);
        auto n0 = n;
        uintptr_t nFreed = {};
        auto sl = rec::begin(gocpp::recv(sweep.active));
        if(! sl.valid)
        {
            return 0;
        }
        for(; n > 0; )
        {
            auto ai = arenas[pageIdx / pagesPerArena];
            auto ha = h->arenas[rec::l1(gocpp::recv(ai))][rec::l2(gocpp::recv(ai))];
            auto arenaPage = (unsigned int)(pageIdx % pagesPerArena);
            auto inUse = ha->pageInUse.make_slice(arenaPage / 8);
            auto marked = ha->pageMarks.make_slice(arenaPage / 8);
            if(uintptr_t(len(inUse)) > n / 8)
            {
                inUse = inUse.make_slice(0, n / 8);
                marked = marked.make_slice(0, n / 8);
            }
            for(auto [i, gocpp_ignored] : inUse)
            {
                auto inUseUnmarked = atomic::Load8(& inUse[i]) &^ marked[i];
                if(inUseUnmarked == 0)
                {
                    continue;
                }
                for(auto j = (unsigned int)(0); j < 8; j++)
                {
                    if(inUseUnmarked & (1 << j) != 0)
                    {
                        auto s = ha->spans[arenaPage + (unsigned int)(i) * 8 + j];
                        {
                            auto [s_tmp, ok] = rec::tryAcquire(gocpp::recv(sl), s);
                            if(auto& s = s_tmp; ok)
                            {
                                auto npages = s->npages;
                                unlock(& h->lock);
                                if(rec::sweep(gocpp::recv(s), false))
                                {
                                    nFreed += npages;
                                }
                                lock(& h->lock);
                                inUseUnmarked = atomic::Load8(& inUse[i]) &^ marked[i];
                            }
                        }
                    }
                }
            }
            pageIdx += uintptr_t(len(inUse) * 8);
            n -= uintptr_t(len(inUse) * 8);
        }
        rec::end(gocpp::recv(sweep.active), sl);
        auto trace = traceAcquire();
        if(rec::ok(gocpp::recv(trace)))
        {
            unlock(& h->lock);
            rec::GCSweepSpan(gocpp::recv(trace), (n0 - nFreed) * pageSize);
            traceRelease(trace);
            lock(& h->lock);
        }
        assertLockHeld(& h->lock);
        return nFreed;
    }

    // spanAllocType represents the type of allocation to make, or
    // the type of allocation to be freed.
    // manual returns true if the span allocation is manually managed.
    bool rec::manual(golang::runtime::spanAllocType s)
    {
        return s != spanAllocHeap;
    }

    // alloc allocates a new span of npage pages from the GC'd heap.
    //
    // spanclass indicates the span's size class and scannability.
    //
    // Returns a span that has been fully initialized. span.needzero indicates
    // whether the span has been zeroed. Note that it may not be.
    struct mspan* rec::alloc(struct mheap* h, uintptr_t npages, golang::runtime::spanClass spanclass)
    {
        // Don't do any operations that lock the heap on the G stack.
        // It might trigger stack growth, and the stack growth code needs
        // to be able to allocate heap.
        mspan* s = {};
        systemstack([=]() mutable -> void
        {
            if(! isSweepDone())
            {
                rec::reclaim(gocpp::recv(h), npages);
            }
            s = rec::allocSpan(gocpp::recv(h), npages, spanAllocHeap, spanclass);
        });
        return s;
    }

    // allocManual allocates a manually-managed span of npage pages.
    // allocManual returns nil if allocation fails.
    //
    // allocManual adds the bytes used to *stat, which should be a
    // memstats in-use field. Unlike allocations in the GC'd heap, the
    // allocation does *not* count toward heapInUse.
    //
    // The memory backing the returned span may not be zeroed if
    // span.needzero is set.
    //
    // allocManual must be called on the system stack because it may
    // acquire the heap lock via allocSpan. See mheap for details.
    //
    // If new code is written to call allocManual, do NOT use an
    // existing spanAllocType value and instead declare a new one.
    //
    //go:systemstack
    struct mspan* rec::allocManual(struct mheap* h, uintptr_t npages, golang::runtime::spanAllocType typ)
    {
        if(! rec::manual(gocpp::recv(typ)))
        {
            go_throw("manual span allocation called with non-manually-managed type"s);
        }
        return rec::allocSpan(gocpp::recv(h), npages, typ, 0);
    }

    // setSpans modifies the span map so [spanOf(base), spanOf(base+npage*pageSize))
    // is s.
    void rec::setSpans(struct mheap* h, uintptr_t base, uintptr_t npage, struct mspan* s)
    {
        auto p = base / pageSize;
        auto ai = arenaIndex(base);
        auto ha = h->arenas[rec::l1(gocpp::recv(ai))][rec::l2(gocpp::recv(ai))];
        for(auto n = uintptr_t(0); n < npage; n++)
        {
            auto i = (p + n) % pagesPerArena;
            if(i == 0)
            {
                ai = arenaIndex(base + n * pageSize);
                ha = h->arenas[rec::l1(gocpp::recv(ai))][rec::l2(gocpp::recv(ai))];
            }
            ha->spans[i] = s;
        }
    }

    // allocNeedsZero checks if the region of address space [base, base+npage*pageSize),
    // assumed to be allocated, needs to be zeroed, updating heap arena metadata for
    // future allocations.
    //
    // This must be called each time pages are allocated from the heap, even if the page
    // allocator can otherwise prove the memory it's allocating is already zero because
    // they're fresh from the operating system. It updates heapArena metadata that is
    // critical for future page allocations.
    //
    // There are no locking constraints on this method.
    bool rec::allocNeedsZero(struct mheap* h, uintptr_t base, uintptr_t npage)
    {
        bool needZero;
        for(; npage > 0; )
        {
            auto ai = arenaIndex(base);
            auto ha = h->arenas[rec::l1(gocpp::recv(ai))][rec::l2(gocpp::recv(ai))];
            auto zeroedBase = atomic::Loaduintptr(& ha->zeroedBase);
            auto arenaBase = base % heapArenaBytes;
            if(arenaBase < zeroedBase)
            {
                needZero = true;
            }
            auto arenaLimit = arenaBase + npage * pageSize;
            if(arenaLimit > heapArenaBytes)
            {
                arenaLimit = heapArenaBytes;
            }
            for(; arenaLimit > zeroedBase; )
            {
                if(atomic::Casuintptr(& ha->zeroedBase, zeroedBase, arenaLimit))
                {
                    break;
                }
                zeroedBase = atomic::Loaduintptr(& ha->zeroedBase);
                if(zeroedBase <= arenaLimit && zeroedBase > arenaBase)
                {
                    go_throw("potentially overlapping in-use allocations detected"s);
                }
            }
            base += arenaLimit - arenaBase;
            npage -= (arenaLimit - arenaBase) / pageSize;
        }
        return needZero;
    }

    // tryAllocMSpan attempts to allocate an mspan object from
    // the P-local cache, but may fail.
    //
    // h.lock need not be held.
    //
    // This caller must ensure that its P won't change underneath
    // it during this function. Currently to ensure that we enforce
    // that the function is run on the system stack, because that's
    // the only place it is used now. In the future, this requirement
    // may be relaxed if its use is necessary elsewhere.
    //
    //go:systemstack
    struct mspan* rec::tryAllocMSpan(struct mheap* h)
    {
        auto pp = rec::ptr(gocpp::recv(getg()->m->p));
        if(pp == nullptr || pp->mspancache.len == 0)
        {
            return nullptr;
        }
        auto s = pp->mspancache.buf[pp->mspancache.len - 1];
        pp->mspancache.len--;
        return s;
    }

    // allocMSpanLocked allocates an mspan object.
    //
    // h.lock must be held.
    //
    // allocMSpanLocked must be called on the system stack because
    // its caller holds the heap lock. See mheap for details.
    // Running on the system stack also ensures that we won't
    // switch Ps during this function. See tryAllocMSpan for details.
    //
    //go:systemstack
    struct mspan* rec::allocMSpanLocked(struct mheap* h)
    {
        assertLockHeld(& h->lock);
        auto pp = rec::ptr(gocpp::recv(getg()->m->p));
        if(pp == nullptr)
        {
            return (mspan*)(rec::alloc(gocpp::recv(h->spanalloc)));
        }
        if(pp->mspancache.len == 0)
        {
            auto refillCount = len(pp->mspancache.buf) / 2;
            for(auto i = 0; i < refillCount; i++)
            {
                pp->mspancache.buf[i] = (mspan*)(rec::alloc(gocpp::recv(h->spanalloc)));
            }
            pp->mspancache.len = refillCount;
        }
        auto s = pp->mspancache.buf[pp->mspancache.len - 1];
        pp->mspancache.len--;
        return s;
    }

    // freeMSpanLocked free an mspan object.
    //
    // h.lock must be held.
    //
    // freeMSpanLocked must be called on the system stack because
    // its caller holds the heap lock. See mheap for details.
    // Running on the system stack also ensures that we won't
    // switch Ps during this function. See tryAllocMSpan for details.
    //
    //go:systemstack
    void rec::freeMSpanLocked(struct mheap* h, struct mspan* s)
    {
        assertLockHeld(& h->lock);
        auto pp = rec::ptr(gocpp::recv(getg()->m->p));
        if(pp != nullptr && pp->mspancache.len < len(pp->mspancache.buf))
        {
            pp->mspancache.buf[pp->mspancache.len] = s;
            pp->mspancache.len++;
            return;
        }
        rec::free(gocpp::recv(h->spanalloc), unsafe::Pointer(s));
    }

    // allocSpan allocates an mspan which owns npages worth of memory.
    //
    // If typ.manual() == false, allocSpan allocates a heap span of class spanclass
    // and updates heap accounting. If manual == true, allocSpan allocates a
    // manually-managed span (spanclass is ignored), and the caller is
    // responsible for any accounting related to its use of the span. Either
    // way, allocSpan will atomically add the bytes in the newly allocated
    // span to *sysStat.
    //
    // The returned span is fully initialized.
    //
    // h.lock must not be held.
    //
    // allocSpan must be called on the system stack both because it acquires
    // the heap lock and because it must block GC transitions.
    //
    //go:systemstack
    struct mspan* rec::allocSpan(struct mheap* h, uintptr_t npages, golang::runtime::spanAllocType typ, golang::runtime::spanClass spanclass)
    {
        struct mspan* s;
        auto gp = getg();
        auto [base, scav] = std::tuple{uintptr_t(0), uintptr_t(0)};
        auto growth = uintptr_t(0);
        auto needPhysPageAlign = physPageAlignedStacks && typ == spanAllocStack && pageSize < physPageSize;
        auto pp = rec::ptr(gocpp::recv(gp->m->p));
        if(! needPhysPageAlign && pp != nullptr && npages < pageCachePages / 4)
        {
            auto c = & pp->pcache;
            if(rec::empty(gocpp::recv(c)))
            {
                lock(& h->lock);
                *c = rec::allocToCache(gocpp::recv(h->pages));
                unlock(& h->lock);
            }
            std::tie(base, scav) = rec::alloc(gocpp::recv(c), npages);
            if(base != 0)
            {
                s = rec::tryAllocMSpan(gocpp::recv(h));
                if(s != nullptr)
                {
                    goto HaveSpan;
                }
            }
        }
        lock(& h->lock);
        if(needPhysPageAlign)
        {
            auto extraPages = physPageSize / pageSize;
            std::tie(base, gocpp_id_3) = rec::find(gocpp::recv(h->pages), npages + extraPages);
            if(base == 0)
            {
                bool ok = {};
                std::tie(growth, ok) = rec::grow(gocpp::recv(h), npages + extraPages);
                if(! ok)
                {
                    unlock(& h->lock);
                    return nullptr;
                }
                std::tie(base, gocpp_id_4) = rec::find(gocpp::recv(h->pages), npages + extraPages);
                if(base == 0)
                {
                    go_throw("grew heap, but no adequate free space found"s);
                }
            }
            base = alignUp(base, physPageSize);
            scav = rec::allocRange(gocpp::recv(h->pages), base, npages);
        }
        if(base == 0)
        {
            std::tie(base, scav) = rec::alloc(gocpp::recv(h->pages), npages);
            if(base == 0)
            {
                bool ok = {};
                std::tie(growth, ok) = rec::grow(gocpp::recv(h), npages);
                if(! ok)
                {
                    unlock(& h->lock);
                    return nullptr;
                }
                std::tie(base, scav) = rec::alloc(gocpp::recv(h->pages), npages);
                if(base == 0)
                {
                    go_throw("grew heap, but no adequate free space found"s);
                }
            }
        }
        if(s == nullptr)
        {
            s = rec::allocMSpanLocked(gocpp::recv(h));
        }
        unlock(& h->lock);
        HaveSpan:
        auto bytesToScavenge = uintptr_t(0);
        auto forceScavenge = false;
        if(auto limit = rec::Load(gocpp::recv(gcController.memoryLimit)); ! rec::limiting(gocpp::recv(gcCPULimiter)))
        {
            auto inuse = rec::Load(gocpp::recv(gcController.mappedReady));
            if(uint64_t(scav) + inuse > uint64_t(limit))
            {
                bytesToScavenge = uintptr_t(uint64_t(scav) + inuse - uint64_t(limit));
                forceScavenge = true;
            }
        }
        if(auto goal = rec::Load(gocpp::recv(scavenge.gcPercentGoal)); goal != ~ uint64_t(0) && growth > 0)
        {
            if(auto retained = heapRetained(); retained + uint64_t(growth) > goal)
            {
                auto todo = growth;
                if(auto overage = uintptr_t(retained + uint64_t(growth) - goal); todo > overage)
                {
                    todo = overage;
                }
                if(todo > bytesToScavenge)
                {
                    bytesToScavenge = todo;
                }
            }
        }
        // There are a few very limited circumstances where we won't have a P here.
        // It's OK to simply skip scavenging in these cases. Something else will notice
        // and pick up the tab.
        int64_t now = {};
        if(pp != nullptr && bytesToScavenge > 0)
        {
            auto start = nanotime();
            auto track = rec::start(gocpp::recv(pp->limiterEvent), limiterEventScavengeAssist, start);
            auto released = rec::scavenge(gocpp::recv(h->pages), bytesToScavenge, [=]() mutable -> bool
            {
                return rec::limiting(gocpp::recv(gcCPULimiter));
            }, forceScavenge);
            rec::Add(gocpp::recv(mheap_.pages.scav.releasedEager), released);
            now = nanotime();
            if(track)
            {
                rec::stop(gocpp::recv(pp->limiterEvent), limiterEventScavengeAssist, now);
            }
            rec::Add(gocpp::recv(scavenge.assistTime), now - start);
        }
        rec::initSpan(gocpp::recv(h), s, typ, spanclass, base, npages);
        auto nbytes = npages * pageSize;
        if(scav != 0)
        {
            sysUsed(unsafe::Pointer(base), nbytes, scav);
            rec::add(gocpp::recv(gcController.heapReleased), - int64_t(scav));
        }
        rec::add(gocpp::recv(gcController.heapFree), - int64_t(nbytes - scav));
        if(typ == spanAllocHeap)
        {
            rec::add(gocpp::recv(gcController.heapInUse), int64_t(nbytes));
        }
        auto stats = rec::acquire(gocpp::recv(memstats.heapStats));
        atomic::Xaddint64(& stats->committed, int64_t(scav));
        atomic::Xaddint64(& stats->released, - int64_t(scav));
        //Go switch emulation
        {
            auto condition = typ;
            int conditionId = -1;
            if(condition == spanAllocHeap) { conditionId = 0; }
            else if(condition == spanAllocStack) { conditionId = 1; }
            else if(condition == spanAllocPtrScalarBits) { conditionId = 2; }
            else if(condition == spanAllocWorkBuf) { conditionId = 3; }
            switch(conditionId)
            {
                case 0:
                    atomic::Xaddint64(& stats->inHeap, int64_t(nbytes));
                    break;
                case 1:
                    atomic::Xaddint64(& stats->inStacks, int64_t(nbytes));
                    break;
                case 2:
                    atomic::Xaddint64(& stats->inPtrScalarBits, int64_t(nbytes));
                    break;
                case 3:
                    atomic::Xaddint64(& stats->inWorkBufs, int64_t(nbytes));
                    break;
            }
        }
        rec::release(gocpp::recv(memstats.heapStats));
        pageTraceAlloc(pp, now, base, npages);
        return s;
    }

    // initSpan initializes a blank span s which will represent the range
    // [base, base+npages*pageSize). typ is the type of span being allocated.
    void rec::initSpan(struct mheap* h, struct mspan* s, golang::runtime::spanAllocType typ, golang::runtime::spanClass spanclass, uintptr_t base, uintptr_t npages)
    {
        rec::init(gocpp::recv(s), base, npages);
        if(rec::allocNeedsZero(gocpp::recv(h), base, npages))
        {
            s->needzero = 1;
        }
        auto nbytes = npages * pageSize;
        if(rec::manual(gocpp::recv(typ)))
        {
            s->manualFreeList = 0;
            s->nelems = 0;
            s->limit = rec::base(gocpp::recv(s)) + s->npages * pageSize;
            rec::set(gocpp::recv(s->state), mSpanManual);
        }
        else
        {
            s->spanclass = spanclass;
            if(auto sizeclass = rec::sizeclass(gocpp::recv(spanclass)); sizeclass == 0)
            {
                s->elemsize = nbytes;
                s->nelems = 1;
                s->divMul = 0;
            }
            else
            {
                s->elemsize = uintptr_t(class_to_size[sizeclass]);
                if(goexperiment::AllocHeaders && ! rec::noscan(gocpp::recv(s->spanclass)) && heapBitsInSpan(s->elemsize))
                {
                    s->nelems = uint16_t((nbytes - (nbytes / goarch::PtrSize / 8)) / s->elemsize);
                }
                else
                {
                    s->nelems = uint16_t(nbytes / s->elemsize);
                }
                s->divMul = class_to_divmagic[sizeclass];
            }
            s->freeindex = 0;
            s->freeIndexForScan = 0;
            s->allocCache = ~ uint64_t(0);
            s->gcmarkBits = newMarkBits(uintptr_t(s->nelems));
            s->allocBits = newAllocBits(uintptr_t(s->nelems));
            atomic::Store(& s->sweepgen, h->sweepgen);
            rec::set(gocpp::recv(s->state), mSpanInUse);
        }
        rec::setSpans(gocpp::recv(h), rec::base(gocpp::recv(s)), npages, s);
        if(! rec::manual(gocpp::recv(typ)))
        {
            auto [arena, pageIdx, pageMask] = pageIndexOf(rec::base(gocpp::recv(s)));
            atomic::Or8(& arena->pageInUse[pageIdx], pageMask);
            rec::Add(gocpp::recv(h->pagesInUse), npages);
        }
        publicationBarrier();
    }

    // Try to add at least npage pages of memory to the heap,
    // returning how much the heap grew by and whether it worked.
    //
    // h.lock must be held.
    std::tuple<uintptr_t, bool> rec::grow(struct mheap* h, uintptr_t npage)
    {
        assertLockHeld(& h->lock);
        auto ask = alignUp(npage, pallocChunkPages) * pageSize;
        auto totalGrowth = uintptr_t(0);
        auto end = h->curArena.base + ask;
        auto nBase = alignUp(end, physPageSize);
        if(nBase > h->curArena.end || end < h->curArena.base)
        {
            auto [av, asize] = rec::sysAlloc(gocpp::recv(h), ask, & h->arenaHints, true);
            if(av == nullptr)
            {
                auto inUse = rec::load(gocpp::recv(gcController.heapFree)) + rec::load(gocpp::recv(gcController.heapReleased)) + rec::load(gocpp::recv(gcController.heapInUse));
                print("runtime: out of memory: cannot allocate "s, ask, "-byte block ("s, inUse, " in use)\n"s);
                return {0, false};
            }
            if(uintptr_t(av) == h->curArena.end)
            {
                h->curArena.end = uintptr_t(av) + asize;
            }
            else
            {
                if(auto size = h->curArena.end - h->curArena.base; size != 0)
                {
                    sysMap(unsafe::Pointer(h->curArena.base), size, & gcController.heapReleased);
                    auto stats = rec::acquire(gocpp::recv(memstats.heapStats));
                    atomic::Xaddint64(& stats->released, int64_t(size));
                    rec::release(gocpp::recv(memstats.heapStats));
                    rec::grow(gocpp::recv(h->pages), h->curArena.base, size);
                    totalGrowth += size;
                }
                h->curArena.base = uintptr_t(av);
                h->curArena.end = uintptr_t(av) + asize;
            }
            nBase = alignUp(h->curArena.base + ask, physPageSize);
        }
        auto v = h->curArena.base;
        h->curArena.base = nBase;
        sysMap(unsafe::Pointer(v), nBase - v, & gcController.heapReleased);
        auto stats = rec::acquire(gocpp::recv(memstats.heapStats));
        atomic::Xaddint64(& stats->released, int64_t(nBase - v));
        rec::release(gocpp::recv(memstats.heapStats));
        rec::grow(gocpp::recv(h->pages), v, nBase - v);
        totalGrowth += nBase - v;
        return {totalGrowth, true};
    }

    // Free the span back into the heap.
    void rec::freeSpan(struct mheap* h, struct mspan* s)
    {
        systemstack([=]() mutable -> void
        {
            pageTraceFree(rec::ptr(gocpp::recv(getg()->m->p)), 0, rec::base(gocpp::recv(s)), s->npages);
            lock(& h->lock);
            if(msanenabled)
            {
                auto base = unsafe::Pointer(rec::base(gocpp::recv(s)));
                auto bytes = s->npages << _PageShift;
                msanfree(base, bytes);
            }
            if(asanenabled)
            {
                auto base = unsafe::Pointer(rec::base(gocpp::recv(s)));
                auto bytes = s->npages << _PageShift;
                asanpoison(base, bytes);
            }
            rec::freeSpanLocked(gocpp::recv(h), s, spanAllocHeap);
            unlock(& h->lock);
        });
    }

    // freeManual frees a manually-managed span returned by allocManual.
    // typ must be the same as the spanAllocType passed to the allocManual that
    // allocated s.
    //
    // This must only be called when gcphase == _GCoff. See mSpanState for
    // an explanation.
    //
    // freeManual must be called on the system stack because it acquires
    // the heap lock. See mheap for details.
    //
    //go:systemstack
    void rec::freeManual(struct mheap* h, struct mspan* s, golang::runtime::spanAllocType typ)
    {
        pageTraceFree(rec::ptr(gocpp::recv(getg()->m->p)), 0, rec::base(gocpp::recv(s)), s->npages);
        s->needzero = 1;
        lock(& h->lock);
        rec::freeSpanLocked(gocpp::recv(h), s, typ);
        unlock(& h->lock);
    }

    void rec::freeSpanLocked(struct mheap* h, struct mspan* s, golang::runtime::spanAllocType typ)
    {
        assertLockHeld(& h->lock);
        //Go switch emulation
        {
            auto condition = rec::get(gocpp::recv(s->state));
            int conditionId = -1;
            if(condition == mSpanManual) { conditionId = 0; }
            else if(condition == mSpanInUse) { conditionId = 1; }
            switch(conditionId)
            {
                case 0:
                    if(s->allocCount != 0)
                    {
                        go_throw("mheap.freeSpanLocked - invalid stack free"s);
                    }
                    break;
                case 1:
                    if(s->isUserArenaChunk)
                    {
                        go_throw("mheap.freeSpanLocked - invalid free of user arena chunk"s);
                    }
                    if(s->allocCount != 0 || s->sweepgen != h->sweepgen)
                    {
                        print("mheap.freeSpanLocked - span "s, s, " ptr "s, hex(rec::base(gocpp::recv(s))), " allocCount "s, s->allocCount, " sweepgen "s, s->sweepgen, "/"s, h->sweepgen, "\n"s);
                        go_throw("mheap.freeSpanLocked - invalid free"s);
                    }
                    rec::Add(gocpp::recv(h->pagesInUse), - s->npages);
                    auto [arena, pageIdx, pageMask] = pageIndexOf(rec::base(gocpp::recv(s)));
                    atomic::And8(& arena->pageInUse[pageIdx], ~ pageMask);
                    break;
                default:
                    go_throw("mheap.freeSpanLocked - invalid span state"s);
                    break;
            }
        }
        auto nbytes = s->npages * pageSize;
        rec::add(gocpp::recv(gcController.heapFree), int64_t(nbytes));
        if(typ == spanAllocHeap)
        {
            rec::add(gocpp::recv(gcController.heapInUse), - int64_t(nbytes));
        }
        auto stats = rec::acquire(gocpp::recv(memstats.heapStats));
        //Go switch emulation
        {
            auto condition = typ;
            int conditionId = -1;
            if(condition == spanAllocHeap) { conditionId = 0; }
            else if(condition == spanAllocStack) { conditionId = 1; }
            else if(condition == spanAllocPtrScalarBits) { conditionId = 2; }
            else if(condition == spanAllocWorkBuf) { conditionId = 3; }
            switch(conditionId)
            {
                case 0:
                    atomic::Xaddint64(& stats->inHeap, - int64_t(nbytes));
                    break;
                case 1:
                    atomic::Xaddint64(& stats->inStacks, - int64_t(nbytes));
                    break;
                case 2:
                    atomic::Xaddint64(& stats->inPtrScalarBits, - int64_t(nbytes));
                    break;
                case 3:
                    atomic::Xaddint64(& stats->inWorkBufs, - int64_t(nbytes));
                    break;
            }
        }
        rec::release(gocpp::recv(memstats.heapStats));
        rec::free(gocpp::recv(h->pages), rec::base(gocpp::recv(s)), s->npages);
        rec::set(gocpp::recv(s->state), mSpanDead);
        rec::freeMSpanLocked(gocpp::recv(h), s);
    }

    // scavengeAll acquires the heap lock (blocking any additional
    // manipulation of the page allocator) and iterates over the whole
    // heap, scavenging every free page available.
    //
    // Must run on the system stack because it acquires the heap lock.
    //
    //go:systemstack
    void rec::scavengeAll(struct mheap* h)
    {
        auto gp = getg();
        gp->m->mallocing++;
        auto released = rec::scavenge(gocpp::recv(h->pages), ~ uintptr_t(0), nullptr, true);
        gp->m->mallocing--;
        if(debug.scavtrace > 0)
        {
            printScavTrace(0, released, true);
        }
    }

    //go:linkname runtime_debug_freeOSMemory runtime/debug.freeOSMemory
    void runtime_debug_freeOSMemory()
    {
        GC();
        systemstack([=]() mutable -> void
        {
            rec::scavengeAll(gocpp::recv(mheap_));
        });
    }

    // Initialize a new span with the given start and npages.
    void rec::init(struct mspan* span, uintptr_t base, uintptr_t npages)
    {
        span->next = nullptr;
        span->prev = nullptr;
        span->list = nullptr;
        span->startAddr = base;
        span->npages = npages;
        span->allocCount = 0;
        span->spanclass = 0;
        span->elemsize = 0;
        span->speciallock.key = 0;
        span->specials = nullptr;
        span->needzero = 0;
        span->freeindex = 0;
        span->freeIndexForScan = 0;
        span->allocBits = nullptr;
        span->gcmarkBits = nullptr;
        span->pinnerBits = nullptr;
        rec::set(gocpp::recv(span->state), mSpanDead);
        lockInit(& span->speciallock, lockRankMspanSpecial);
    }

    bool rec::inList(struct mspan* span)
    {
        return span->list != nullptr;
    }

    // Initialize an empty doubly-linked list.
    void rec::init(struct mSpanList* list)
    {
        list->first = nullptr;
        list->last = nullptr;
    }

    void rec::remove(struct mSpanList* list, struct mspan* span)
    {
        if(span->list != list)
        {
            print("runtime: failed mSpanList.remove span.npages="s, span->npages, " span="s, span, " prev="s, span->prev, " span.list="s, span->list, " list="s, list, "\n"s);
            go_throw("mSpanList.remove"s);
        }
        if(list->first == span)
        {
            list->first = span->next;
        }
        else
        {
            span->prev->next = span->next;
        }
        if(list->last == span)
        {
            list->last = span->prev;
        }
        else
        {
            span->next->prev = span->prev;
        }
        span->next = nullptr;
        span->prev = nullptr;
        span->list = nullptr;
    }

    bool rec::isEmpty(struct mSpanList* list)
    {
        return list->first == nullptr;
    }

    void rec::insert(struct mSpanList* list, struct mspan* span)
    {
        if(span->next != nullptr || span->prev != nullptr || span->list != nullptr)
        {
            println("runtime: failed mSpanList.insert"s, span, span->next, span->prev, span->list);
            go_throw("mSpanList.insert"s);
        }
        span->next = list->first;
        if(list->first != nullptr)
        {
            list->first->prev = span;
        }
        else
        {
            list->last = span;
        }
        list->first = span;
        span->list = list;
    }

    void rec::insertBack(struct mSpanList* list, struct mspan* span)
    {
        if(span->next != nullptr || span->prev != nullptr || span->list != nullptr)
        {
            println("runtime: failed mSpanList.insertBack"s, span, span->next, span->prev, span->list);
            go_throw("mSpanList.insertBack"s);
        }
        span->prev = list->last;
        if(list->last != nullptr)
        {
            list->last->next = span;
        }
        else
        {
            list->first = span;
        }
        list->last = span;
        span->list = list;
    }

    // takeAll removes all spans from other and inserts them at the front
    // of list.
    void rec::takeAll(struct mSpanList* list, struct mSpanList* other)
    {
        if(rec::isEmpty(gocpp::recv(other)))
        {
            return;
        }
        for(auto s = other->first; s != nullptr; s = s->next)
        {
            s->list = list;
        }
        if(rec::isEmpty(gocpp::recv(list)))
        {
            *list = *other;
        }
        else
        {
            other->last->next = list->first;
            list->first->prev = other->last;
            list->first = other->first;
        }
        std::tie(other->first, other->last) = std::tuple{nullptr, nullptr};
    }

    // _KindSpecialReachable is a special used for tracking
    // reachability during testing.
    // _KindSpecialPinCounter is a special used for objects that are pinned
    // multiple times
    
    template<typename T> requires gocpp::GoStruct<T>
    special::operator T()
    {
        T result;
        result._1 = this->_1;
        result.next = this->next;
        result.offset = this->offset;
        result.kind = this->kind;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool special::operator==(const T& ref) const
    {
        if (_1 != ref._1) return false;
        if (next != ref.next) return false;
        if (offset != ref.offset) return false;
        if (kind != ref.kind) return false;
        return true;
    }

    std::ostream& special::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << _1;
        os << " " << next;
        os << " " << offset;
        os << " " << kind;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct special& value)
    {
        return value.PrintTo(os);
    }

    // spanHasSpecials marks a span as having specials in the arena bitmap.
    void spanHasSpecials(struct mspan* s)
    {
        auto arenaPage = (rec::base(gocpp::recv(s)) / pageSize) % pagesPerArena;
        auto ai = arenaIndex(rec::base(gocpp::recv(s)));
        auto ha = mheap_.arenas[rec::l1(gocpp::recv(ai))][rec::l2(gocpp::recv(ai))];
        atomic::Or8(& ha->pageSpecials[arenaPage / 8], uint8_t(1) << (arenaPage % 8));
    }

    // spanHasNoSpecials marks a span as having no specials in the arena bitmap.
    void spanHasNoSpecials(struct mspan* s)
    {
        auto arenaPage = (rec::base(gocpp::recv(s)) / pageSize) % pagesPerArena;
        auto ai = arenaIndex(rec::base(gocpp::recv(s)));
        auto ha = mheap_.arenas[rec::l1(gocpp::recv(ai))][rec::l2(gocpp::recv(ai))];
        atomic::And8(& ha->pageSpecials[arenaPage / 8], ~ (uint8_t(1) << (arenaPage % 8)));
    }

    // Adds the special record s to the list of special records for
    // the object p. All fields of s should be filled in except for
    // offset & next, which this routine will fill in.
    // Returns true if the special was successfully added, false otherwise.
    // (The add will fail only if a record with the same p and s->kind
    // already exists.)
    bool addspecial(unsafe::Pointer p, struct special* s)
    {
        auto span = spanOfHeap(uintptr_t(p));
        if(span == nullptr)
        {
            go_throw("addspecial on invalid pointer"s);
        }
        auto mp = acquirem();
        rec::ensureSwept(gocpp::recv(span));
        auto offset = uintptr_t(p) - rec::base(gocpp::recv(span));
        auto kind = s->kind;
        lock(& span->speciallock);
        auto [iter, exists] = rec::specialFindSplicePoint(gocpp::recv(span), offset, kind);
        if(! exists)
        {
            s->offset = uint16_t(offset);
            s->next = *iter;
            *iter = s;
            spanHasSpecials(span);
        }
        unlock(& span->speciallock);
        releasem(mp);
        return ! exists;
    }

    // Removes the Special record of the given kind for the object p.
    // Returns the record if the record existed, nil otherwise.
    // The caller must FixAlloc_Free the result.
    struct special* removespecial(unsafe::Pointer p, uint8_t kind)
    {
        auto span = spanOfHeap(uintptr_t(p));
        if(span == nullptr)
        {
            go_throw("removespecial on invalid pointer"s);
        }
        auto mp = acquirem();
        rec::ensureSwept(gocpp::recv(span));
        auto offset = uintptr_t(p) - rec::base(gocpp::recv(span));
        special* result = {};
        lock(& span->speciallock);
        auto [iter, exists] = rec::specialFindSplicePoint(gocpp::recv(span), offset, kind);
        if(exists)
        {
            auto s = *iter;
            *iter = s->next;
            result = s;
        }
        if(span->specials == nullptr)
        {
            spanHasNoSpecials(span);
        }
        unlock(& span->speciallock);
        releasem(mp);
        return result;
    }

    // Find a splice point in the sorted list and check for an already existing
    // record. Returns a pointer to the next-reference in the list predecessor.
    // Returns true, if the referenced item is an exact match.
    std::tuple<struct special**, bool> rec::specialFindSplicePoint(struct mspan* span, uintptr_t offset, unsigned char kind)
    {
        auto iter = & span->specials;
        auto found = false;
        for(; ; )
        {
            auto s = *iter;
            if(s == nullptr)
            {
                break;
            }
            if(offset == uintptr_t(s->offset) && kind == s->kind)
            {
                found = true;
                break;
            }
            if(offset < uintptr_t(s->offset) || (offset == uintptr_t(s->offset) && kind < s->kind))
            {
                break;
            }
            iter = & s->next;
        }
        return {iter, found};
    }

    // The described object has a finalizer set for it.
    //
    // specialfinalizer is allocated from non-GC'd memory, so any heap
    // pointers must be specially handled.
    
    template<typename T> requires gocpp::GoStruct<T>
    specialfinalizer::operator T()
    {
        T result;
        result._1 = this->_1;
        result.special = this->special;
        result.fn = this->fn;
        result.nret = this->nret;
        result.fint = this->fint;
        result.ot = this->ot;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool specialfinalizer::operator==(const T& ref) const
    {
        if (_1 != ref._1) return false;
        if (special != ref.special) return false;
        if (fn != ref.fn) return false;
        if (nret != ref.nret) return false;
        if (fint != ref.fint) return false;
        if (ot != ref.ot) return false;
        return true;
    }

    std::ostream& specialfinalizer::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << _1;
        os << " " << special;
        os << " " << fn;
        os << " " << nret;
        os << " " << fint;
        os << " " << ot;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct specialfinalizer& value)
    {
        return value.PrintTo(os);
    }

    // Adds a finalizer to the object p. Returns true if it succeeded.
    bool addfinalizer(unsafe::Pointer p, struct funcval* f, uintptr_t nret, golang::runtime::_type* fint, golang::runtime::ptrtype* ot)
    {
        lock(& mheap_.speciallock);
        auto s = (specialfinalizer*)(rec::alloc(gocpp::recv(mheap_.specialfinalizeralloc)));
        unlock(& mheap_.speciallock);
        s->special.kind = _KindSpecialFinalizer;
        s->fn = f;
        s->nret = nret;
        s->fint = fint;
        s->ot = ot;
        if(addspecial(p, & s->special))
        {
            if(gcphase != _GCoff)
            {
                auto [base, span, gocpp_id_5] = findObject(uintptr_t(p), 0, 0);
                auto mp = acquirem();
                auto gcw = & rec::ptr(gocpp::recv(mp->p))->gcw;
                if(! rec::noscan(gocpp::recv(span->spanclass)))
                {
                    scanobject(base, gcw);
                }
                scanblock(uintptr_t(unsafe::Pointer(& s->fn)), goarch::PtrSize, & oneptrmask[0], gcw, nullptr);
                releasem(mp);
            }
            return true;
        }
        lock(& mheap_.speciallock);
        rec::free(gocpp::recv(mheap_.specialfinalizeralloc), unsafe::Pointer(s));
        unlock(& mheap_.speciallock);
        return false;
    }

    // Removes the finalizer (if any) from the object p.
    void removefinalizer(unsafe::Pointer p)
    {
        auto s = (specialfinalizer*)(unsafe::Pointer(removespecial(p, _KindSpecialFinalizer)));
        if(s == nullptr)
        {
            return;
        }
        lock(& mheap_.speciallock);
        rec::free(gocpp::recv(mheap_.specialfinalizeralloc), unsafe::Pointer(s));
        unlock(& mheap_.speciallock);
    }

    // The described object is being heap profiled.
    
    template<typename T> requires gocpp::GoStruct<T>
    specialprofile::operator T()
    {
        T result;
        result._1 = this->_1;
        result.special = this->special;
        result.b = this->b;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool specialprofile::operator==(const T& ref) const
    {
        if (_1 != ref._1) return false;
        if (special != ref.special) return false;
        if (b != ref.b) return false;
        return true;
    }

    std::ostream& specialprofile::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << _1;
        os << " " << special;
        os << " " << b;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct specialprofile& value)
    {
        return value.PrintTo(os);
    }

    // Set the heap profile bucket associated with addr to b.
    void setprofilebucket(unsafe::Pointer p, struct bucket* b)
    {
        lock(& mheap_.speciallock);
        auto s = (specialprofile*)(rec::alloc(gocpp::recv(mheap_.specialprofilealloc)));
        unlock(& mheap_.speciallock);
        s->special.kind = _KindSpecialProfile;
        s->b = b;
        if(! addspecial(p, & s->special))
        {
            go_throw("setprofilebucket: profile already set"s);
        }
    }

    // specialReachable tracks whether an object is reachable on the next
    // GC cycle. This is used by testing.
    
    template<typename T> requires gocpp::GoStruct<T>
    specialReachable::operator T()
    {
        T result;
        result.special = this->special;
        result.done = this->done;
        result.reachable = this->reachable;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool specialReachable::operator==(const T& ref) const
    {
        if (special != ref.special) return false;
        if (done != ref.done) return false;
        if (reachable != ref.reachable) return false;
        return true;
    }

    std::ostream& specialReachable::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << special;
        os << " " << done;
        os << " " << reachable;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct specialReachable& value)
    {
        return value.PrintTo(os);
    }

    // specialPinCounter tracks whether an object is pinned multiple times.
    
    template<typename T> requires gocpp::GoStruct<T>
    specialPinCounter::operator T()
    {
        T result;
        result.special = this->special;
        result.counter = this->counter;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool specialPinCounter::operator==(const T& ref) const
    {
        if (special != ref.special) return false;
        if (counter != ref.counter) return false;
        return true;
    }

    std::ostream& specialPinCounter::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << special;
        os << " " << counter;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct specialPinCounter& value)
    {
        return value.PrintTo(os);
    }

    // specialsIter helps iterate over specials lists.
    
    template<typename T> requires gocpp::GoStruct<T>
    specialsIter::operator T()
    {
        T result;
        result.pprev = this->pprev;
        result.s = this->s;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool specialsIter::operator==(const T& ref) const
    {
        if (pprev != ref.pprev) return false;
        if (s != ref.s) return false;
        return true;
    }

    std::ostream& specialsIter::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << pprev;
        os << " " << s;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct specialsIter& value)
    {
        return value.PrintTo(os);
    }

    struct specialsIter newSpecialsIter(struct mspan* span)
    {
        return specialsIter {& span->specials, span->specials};
    }

    bool rec::valid(struct specialsIter* i)
    {
        return i->s != nullptr;
    }

    void rec::next(struct specialsIter* i)
    {
        i->pprev = & i->s->next;
        i->s = *i->pprev;
    }

    // unlinkAndNext removes the current special from the list and moves
    // the iterator to the next special. It returns the unlinked special.
    struct special* rec::unlinkAndNext(struct specialsIter* i)
    {
        auto cur = i->s;
        i->s = cur->next;
        *i->pprev = i->s;
        return cur;
    }

    // freeSpecial performs any cleanup on special s and deallocates it.
    // s must already be unlinked from the specials list.
    void freeSpecial(struct special* s, unsafe::Pointer p, uintptr_t size)
    {
        //Go switch emulation
        {
            auto condition = s->kind;
            int conditionId = -1;
            if(condition == _KindSpecialFinalizer) { conditionId = 0; }
            else if(condition == _KindSpecialProfile) { conditionId = 1; }
            else if(condition == _KindSpecialReachable) { conditionId = 2; }
            else if(condition == _KindSpecialPinCounter) { conditionId = 3; }
            switch(conditionId)
            {
                case 0:
                    auto sf = (specialfinalizer*)(unsafe::Pointer(s));
                    queuefinalizer(p, sf->fn, sf->nret, sf->fint, sf->ot);
                    lock(& mheap_.speciallock);
                    rec::free(gocpp::recv(mheap_.specialfinalizeralloc), unsafe::Pointer(sf));
                    unlock(& mheap_.speciallock);
                    break;
                case 1:
                    auto sp = (specialprofile*)(unsafe::Pointer(s));
                    mProf_Free(sp->b, size);
                    lock(& mheap_.speciallock);
                    rec::free(gocpp::recv(mheap_.specialprofilealloc), unsafe::Pointer(sp));
                    unlock(& mheap_.speciallock);
                    break;
                case 2:
                    auto sp = (specialReachable*)(unsafe::Pointer(s));
                    sp->done = true;
                    break;
                case 3:
                    lock(& mheap_.speciallock);
                    rec::free(gocpp::recv(mheap_.specialPinCounterAlloc), unsafe::Pointer(s));
                    unlock(& mheap_.speciallock);
                    break;
                default:
                    go_throw("bad special kind"s);
                    gocpp::panic("not reached"s);
                    break;
            }
        }
    }

    // gcBits is an alloc/mark bitmap. This is always used as gcBits.x.
    
    template<typename T> requires gocpp::GoStruct<T>
    gcBits::operator T()
    {
        T result;
        result._1 = this->_1;
        result.x = this->x;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool gcBits::operator==(const T& ref) const
    {
        if (_1 != ref._1) return false;
        if (x != ref.x) return false;
        return true;
    }

    std::ostream& gcBits::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << _1;
        os << " " << x;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct gcBits& value)
    {
        return value.PrintTo(os);
    }

    // bytep returns a pointer to the n'th byte of b.
    uint8_t* rec::bytep(struct gcBits* b, uintptr_t n)
    {
        return addb(& b->x, n);
    }

    // bitp returns a pointer to the byte containing bit n and a mask for
    // selecting that bit from *bytep.
    std::tuple<uint8_t*, uint8_t> rec::bitp(struct gcBits* b, uintptr_t n)
    {
        uint8_t* bytep;
        uint8_t mask;
        return {rec::bytep(gocpp::recv(b), n / 8), 1 << (n % 8)};
    }

    
    template<typename T> requires gocpp::GoStruct<T>
    gcBitsHeader::operator T()
    {
        T result;
        result.free = this->free;
        result.next = this->next;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool gcBitsHeader::operator==(const T& ref) const
    {
        if (free != ref.free) return false;
        if (next != ref.next) return false;
        return true;
    }

    std::ostream& gcBitsHeader::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << free;
        os << " " << next;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct gcBitsHeader& value)
    {
        return value.PrintTo(os);
    }

    
    template<typename T> requires gocpp::GoStruct<T>
    gcBitsArena::operator T()
    {
        T result;
        result._1 = this->_1;
        result.free = this->free;
        result.next = this->next;
        result.bits = this->bits;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool gcBitsArena::operator==(const T& ref) const
    {
        if (_1 != ref._1) return false;
        if (free != ref.free) return false;
        if (next != ref.next) return false;
        if (bits != ref.bits) return false;
        return true;
    }

    std::ostream& gcBitsArena::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << _1;
        os << " " << free;
        os << " " << next;
        os << " " << bits;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct gcBitsArena& value)
    {
        return value.PrintTo(os);
    }

    struct gocpp_id_6
    {
        mutex lock;
        gcBitsArena* free;
        gcBitsArena* next;
        gcBitsArena* current;
        gcBitsArena* previous;

        using isGoStruct = void;

        template<typename T> requires gocpp::GoStruct<T>
        operator T()
        {
            T result;
            result.lock = this->lock;
            result.free = this->free;
            result.next = this->next;
            result.current = this->current;
            result.previous = this->previous;
            return result;
        }

        template<typename T> requires gocpp::GoStruct<T>
        bool operator==(const T& ref) const
        {
            if (lock != ref.lock) return false;
            if (free != ref.free) return false;
            if (next != ref.next) return false;
            if (current != ref.current) return false;
            if (previous != ref.previous) return false;
            return true;
        }

        std::ostream& PrintTo(std::ostream& os) const
        {
            os << '{';
            os << "" << lock;
            os << " " << free;
            os << " " << next;
            os << " " << current;
            os << " " << previous;
            os << '}';
            return os;
        }
    };

    std::ostream& operator<<(std::ostream& os, const struct gocpp_id_6& value)
    {
        return value.PrintTo(os);
    }


    gocpp_id_6 gcBitsArenas;
    // tryAlloc allocates from b or returns nil if b does not have enough room.
    // This is safe to call concurrently.
    struct gcBits* rec::tryAlloc(struct gcBitsArena* b, uintptr_t bytes)
    {
        if(b == nullptr || atomic::Loaduintptr(& b->free) + bytes > uintptr_t(len(b->bits)))
        {
            return nullptr;
        }
        auto end = atomic::Xadduintptr(& b->free, bytes);
        if(end > uintptr_t(len(b->bits)))
        {
            return nullptr;
        }
        auto start = end - bytes;
        return & b->bits[start];
    }

    // newMarkBits returns a pointer to 8 byte aligned bytes
    // to be used for a span's mark bits.
    struct gcBits* newMarkBits(uintptr_t nelems)
    {
        auto blocksNeeded = (nelems + 63) / 64;
        auto bytesNeeded = blocksNeeded * 8;
        auto head = (gcBitsArena*)(atomic::Loadp(unsafe::Pointer(& gcBitsArenas.next)));
        if(auto p = rec::tryAlloc(gocpp::recv(head), bytesNeeded); p != nullptr)
        {
            return p;
        }
        lock(& gcBitsArenas.lock);
        if(auto p = rec::tryAlloc(gocpp::recv(gcBitsArenas.next), bytesNeeded); p != nullptr)
        {
            unlock(& gcBitsArenas.lock);
            return p;
        }
        auto fresh = newArenaMayUnlock();
        if(auto p = rec::tryAlloc(gocpp::recv(gcBitsArenas.next), bytesNeeded); p != nullptr)
        {
            fresh->next = gcBitsArenas.free;
            gcBitsArenas.free = fresh;
            unlock(& gcBitsArenas.lock);
            return p;
        }
        auto p = rec::tryAlloc(gocpp::recv(fresh), bytesNeeded);
        if(p == nullptr)
        {
            go_throw("markBits overflow"s);
        }
        fresh->next = gcBitsArenas.next;
        atomic::StorepNoWB(unsafe::Pointer(& gcBitsArenas.next), unsafe::Pointer(fresh));
        unlock(& gcBitsArenas.lock);
        return p;
    }

    // newAllocBits returns a pointer to 8 byte aligned bytes
    // to be used for this span's alloc bits.
    // newAllocBits is used to provide newly initialized spans
    // allocation bits. For spans not being initialized the
    // mark bits are repurposed as allocation bits when
    // the span is swept.
    struct gcBits* newAllocBits(uintptr_t nelems)
    {
        return newMarkBits(nelems);
    }

    // nextMarkBitArenaEpoch establishes a new epoch for the arenas
    // holding the mark bits. The arenas are named relative to the
    // current GC cycle which is demarcated by the call to finishweep_m.
    //
    // All current spans have been swept.
    // During that sweep each span allocated room for its gcmarkBits in
    // gcBitsArenas.next block. gcBitsArenas.next becomes the gcBitsArenas.current
    // where the GC will mark objects and after each span is swept these bits
    // will be used to allocate objects.
    // gcBitsArenas.current becomes gcBitsArenas.previous where the span's
    // gcAllocBits live until all the spans have been swept during this GC cycle.
    // The span's sweep extinguishes all the references to gcBitsArenas.previous
    // by pointing gcAllocBits into the gcBitsArenas.current.
    // The gcBitsArenas.previous is released to the gcBitsArenas.free list.
    void nextMarkBitArenaEpoch()
    {
        lock(& gcBitsArenas.lock);
        if(gcBitsArenas.previous != nullptr)
        {
            if(gcBitsArenas.free == nullptr)
            {
                gcBitsArenas.free = gcBitsArenas.previous;
            }
            else
            {
                auto last = gcBitsArenas.previous;
                for(last = gcBitsArenas.previous; last->next != nullptr; last = last->next)
                {
                }
                last->next = gcBitsArenas.free;
                gcBitsArenas.free = gcBitsArenas.previous;
            }
        }
        gcBitsArenas.previous = gcBitsArenas.current;
        gcBitsArenas.current = gcBitsArenas.next;
        atomic::StorepNoWB(unsafe::Pointer(& gcBitsArenas.next), nullptr);
        unlock(& gcBitsArenas.lock);
    }

    // newArenaMayUnlock allocates and zeroes a gcBits arena.
    // The caller must hold gcBitsArena.lock. This may temporarily release it.
    struct gcBitsArena* newArenaMayUnlock()
    {
        gcBitsArena* result = {};
        if(gcBitsArenas.free == nullptr)
        {
            unlock(& gcBitsArenas.lock);
            result = (gcBitsArena*)(sysAlloc(gcBitsChunkBytes, & memstats.gcMiscSys));
            if(result == nullptr)
            {
                go_throw("runtime: cannot allocate memory"s);
            }
            lock(& gcBitsArenas.lock);
        }
        else
        {
            result = gcBitsArenas.free;
            gcBitsArenas.free = gcBitsArenas.free->next;
            memclrNoHeapPointers(unsafe::Pointer(result), gcBitsChunkBytes);
        }
        result->next = nullptr;
        if(unsafe::Offsetof(gcBitsArena {}.bits) & 7 == 0)
        {
            result->free = 0;
        }
        else
        {
            result->free = 8 - (uintptr_t(unsafe::Pointer(& result->bits[0])) & 7);
        }
        return result;
    }

}

