// generated by GoCpp from file '$(ImportDir)/runtime/mgcscavenge.go'
#include <complex>
#include <functional>
#include <iostream>
#include <iomanip>
#include <map>
#include <string>
#include <tuple>
#include <vector>

#include "golang/runtime/mgcscavenge.h"
#include "gocpp/support.h"

#include "golang/internal/abi/type.h"
#include "golang/internal/chacha8rand/chacha8.h"
#include "golang/internal/cpu/cpu.h"
#include "golang/internal/goos/zgoos_windows.h"
#include "golang/runtime/cgocall.h"
#include "golang/runtime/chan.h"
#include "golang/runtime/coro.h"
#include "golang/runtime/debuglog_off.h"
#include "golang/runtime/float.h"
#include "golang/runtime/internal/atomic/stubs.h"
#include "golang/runtime/internal/atomic/types.h"
#include "golang/runtime/internal/sys/intrinsics.h"
#include "golang/runtime/internal/sys/nih.h"
#include "golang/runtime/lock_sema.h"
#include "golang/runtime/lockrank.h"
#include "golang/runtime/lockrank_off.h"
#include "golang/runtime/malloc.h"
#include "golang/runtime/mbitmap_allocheaders.h"
#include "golang/runtime/mcache.h"
#include "golang/runtime/mcentral.h"
#include "golang/runtime/mcheckmark.h"
#include "golang/runtime/mem.h"
#include "golang/runtime/mfixalloc.h"
#include "golang/runtime/mgc.h"
#include "golang/runtime/mgclimit.h"
#include "golang/runtime/mgcpacer.h"
#include "golang/runtime/mgcwork.h"
#include "golang/runtime/mheap.h"
#include "golang/runtime/mpagealloc.h"
#include "golang/runtime/mpagealloc_64bit.h"
#include "golang/runtime/mpagecache.h"
#include "golang/runtime/mpallocbits.h"
#include "golang/runtime/mprof.h"
#include "golang/runtime/mranges.h"
#include "golang/runtime/mspanset.h"
#include "golang/runtime/mstats.h"
#include "golang/runtime/mwbbuf.h"
#include "golang/runtime/os_windows.h"
#include "golang/runtime/pagetrace_off.h"
#include "golang/runtime/panic.h"
#include "golang/runtime/pinner.h"
#include "golang/runtime/print.h"
#include "golang/runtime/proc.h"
#include "golang/runtime/runtime2.h"
#include "golang/runtime/signal_windows.h"
#include "golang/runtime/stubs.h"
#include "golang/runtime/symtab.h"
#include "golang/runtime/time.h"
#include "golang/runtime/time_nofake.h"
#include "golang/runtime/trace2buf.h"
#include "golang/runtime/trace2runtime.h"
#include "golang/runtime/trace2status.h"
#include "golang/runtime/trace2time.h"
#include "golang/unsafe/unsafe.h"

namespace golang::runtime
{
    namespace rec
    {
        using namespace mocklib::rec;
        using atomic::rec::Add;
        using atomic::rec::Load;
        using atomic::rec::Store;
    }

    // The background scavenger is paced according to these parameters.
    //
    // scavengePercent represents the portion of mutator time we're willing
    // to spend on scavenging in percent.
    // retainExtraPercent represents the amount of memory over the heap goal
    // that the scavenger should keep as a buffer space for the allocator.
    // This constant is used when we do not have a memory limit set.
    //
    // The purpose of maintaining this overhead is to have a greater pool of
    // unscavenged memory available for allocation (since using scavenged memory
    // incurs an additional cost), to account for heap fragmentation and
    // the ever-changing layout of the heap.
    // reduceExtraPercent represents the amount of memory under the limit
    // that the scavenger should target. For example, 5 means we target 95%
    // of the limit.
    //
    // The purpose of shooting lower than the limit is to ensure that, once
    // close to the limit, the scavenger is working hard to maintain it. If
    // we have a memory limit set but are far away from it, there's no harm
    // in leaving up to 100-retainExtraPercent live, and it's more efficient
    // anyway, for the same reasons that retainExtraPercent exists.
    // maxPagesPerPhysPage is the maximum number of supported runtime pages per
    // physical page, based on maxPhysPageSize.
    // scavengeCostRatio is the approximate ratio between the costs of using previously
    // scavenged memory and scavenging memory.
    //
    // For most systems the cost of scavenging greatly outweighs the costs
    // associated with using scavenged memory, making this constant 0. On other systems
    // (especially ones where "sysUsed" is not just a no-op) this cost is non-trivial.
    //
    // This ratio is used as part of multiplicative factor to help the scavenger account
    // for the additional costs of using scavenged memory in its pacing.
    // scavChunkHiOcFrac indicates the fraction of pages that need to be allocated
    // in the chunk in a single GC cycle for it to be considered high density.
    // heapRetained returns an estimate of the current heap RSS.
    uint64_t heapRetained()
    {
        return rec::load(gocpp::recv(gcController.heapInUse)) + rec::load(gocpp::recv(gcController.heapFree));
    }

    // gcPaceScavenger updates the scavenger's pacing, particularly
    // its rate and RSS goal. For this, it requires the current heapGoal,
    // and the heapGoal for the previous GC cycle.
    //
    // The RSS goal is based on the current heap goal with a small overhead
    // to accommodate non-determinism in the allocator.
    //
    // The pacing is based on scavengePageRate, which applies to both regular and
    // huge pages. See that constant for more information.
    //
    // Must be called whenever GC pacing is updated.
    //
    // mheap_.lock must be held or the world must be stopped.
    void gcPaceScavenger(int64_t memoryLimit, uint64_t heapGoal, uint64_t lastHeapGoal)
    {
        assertWorldStoppedOrLockHeld(& mheap_.lock);
        auto memoryLimitGoal = uint64_t(double(memoryLimit) * (1 - reduceExtraPercent / 100.0));
        auto mappedReady = rec::Load(gocpp::recv(gcController.mappedReady));
        if(mappedReady <= memoryLimitGoal)
        {
            rec::Store(gocpp::recv(scavenge.memoryLimitGoal), ~ uint64_t(0));
        }
        else
        {
            rec::Store(gocpp::recv(scavenge.memoryLimitGoal), memoryLimitGoal);
        }
        if(lastHeapGoal == 0)
        {
            rec::Store(gocpp::recv(scavenge.gcPercentGoal), ~ uint64_t(0));
            return;
        }
        auto goalRatio = double(heapGoal) / double(lastHeapGoal);
        auto gcPercentGoal = uint64_t(double(memstats.lastHeapInUse) * goalRatio);
        gcPercentGoal += gcPercentGoal / (1.0 / (retainExtraPercent / 100.0));
        gcPercentGoal = (gcPercentGoal + uint64_t(physPageSize) - 1) &^ (uint64_t(physPageSize) - 1);
        auto heapRetainedNow = heapRetained();
        if(heapRetainedNow <= gcPercentGoal || heapRetainedNow - gcPercentGoal < uint64_t(physPageSize))
        {
            rec::Store(gocpp::recv(scavenge.gcPercentGoal), ~ uint64_t(0));
        }
        else
        {
            rec::Store(gocpp::recv(scavenge.gcPercentGoal), gcPercentGoal);
        }
    }

    struct gocpp_id_0
    {
        atomic::Uint64 gcPercentGoal;
        atomic::Uint64 memoryLimitGoal;
        atomic::Int64 assistTime;
        atomic::Int64 backgroundTime;

        using isGoStruct = void;

        template<typename T> requires gocpp::GoStruct<T>
        operator T()
        {
            T result;
            result.gcPercentGoal = this->gcPercentGoal;
            result.memoryLimitGoal = this->memoryLimitGoal;
            result.assistTime = this->assistTime;
            result.backgroundTime = this->backgroundTime;
            return result;
        }

        template<typename T> requires gocpp::GoStruct<T>
        bool operator==(const T& ref) const
        {
            if (gcPercentGoal != ref.gcPercentGoal) return false;
            if (memoryLimitGoal != ref.memoryLimitGoal) return false;
            if (assistTime != ref.assistTime) return false;
            if (backgroundTime != ref.backgroundTime) return false;
            return true;
        }

        std::ostream& PrintTo(std::ostream& os) const
        {
            os << '{';
            os << "" << gcPercentGoal;
            os << " " << memoryLimitGoal;
            os << " " << assistTime;
            os << " " << backgroundTime;
            os << '}';
            return os;
        }
    };

    std::ostream& operator<<(std::ostream& os, const struct gocpp_id_0& value)
    {
        return value.PrintTo(os);
    }


    gocpp_id_0 scavenge;
    // It doesn't really matter what value we start at, but we can't be zero, because
    // that'll cause divide-by-zero issues. Pick something conservative which we'll
    // also use as a fallback.
    // Spend at least 1 ms scavenging, otherwise the corresponding
    // sleep time to maintain our desired utilization is too low to
    // be reliable.
    // Sleep/wait state of the background scavenger.
    scavengerState scavenger;
    
    template<typename T> requires gocpp::GoStruct<T>
    scavengerState::operator T()
    {
        T result;
        result.lock = this->lock;
        result.g = this->g;
        result.parked = this->parked;
        result.timer = this->timer;
        result.sysmonWake = this->sysmonWake;
        result.targetCPUFraction = this->targetCPUFraction;
        result.sleepRatio = this->sleepRatio;
        result.sleepController = this->sleepController;
        result.controllerCooldown = this->controllerCooldown;
        result.printControllerReset = this->printControllerReset;
        result.sleepStub = this->sleepStub;
        result.scavenge = this->scavenge;
        result.shouldStop = this->shouldStop;
        result.gomaxprocs = this->gomaxprocs;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool scavengerState::operator==(const T& ref) const
    {
        if (lock != ref.lock) return false;
        if (g != ref.g) return false;
        if (parked != ref.parked) return false;
        if (timer != ref.timer) return false;
        if (sysmonWake != ref.sysmonWake) return false;
        if (targetCPUFraction != ref.targetCPUFraction) return false;
        if (sleepRatio != ref.sleepRatio) return false;
        if (sleepController != ref.sleepController) return false;
        if (controllerCooldown != ref.controllerCooldown) return false;
        if (printControllerReset != ref.printControllerReset) return false;
        if (sleepStub != ref.sleepStub) return false;
        if (scavenge != ref.scavenge) return false;
        if (shouldStop != ref.shouldStop) return false;
        if (gomaxprocs != ref.gomaxprocs) return false;
        return true;
    }

    std::ostream& scavengerState::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << lock;
        os << " " << g;
        os << " " << parked;
        os << " " << timer;
        os << " " << sysmonWake;
        os << " " << targetCPUFraction;
        os << " " << sleepRatio;
        os << " " << sleepController;
        os << " " << controllerCooldown;
        os << " " << printControllerReset;
        os << " " << sleepStub;
        os << " " << scavenge;
        os << " " << shouldStop;
        os << " " << gomaxprocs;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct scavengerState& value)
    {
        return value.PrintTo(os);
    }

    // init initializes a scavenger state and wires to the current G.
    //
    // Must be called from a regular goroutine that can allocate.
    void rec::init(golang::runtime::scavengerState* s)
    {
        if(s->g != nullptr)
        {
            go_throw("scavenger state is already wired"_s);
        }
        lockInit(& s->lock, lockRankScavenge);
        s->g = getg();
        s->timer = new(timer);
        s->timer->arg = s;
        s->timer->f = [=](go_any s, uintptr_t _1) mutable -> void
        {
            rec::wake(gocpp::recv(gocpp::getValue<scavengerState*>(s)));
        };
        s->sleepController = gocpp::Init<piController>([=](auto& x) {
            x.kp = 0.3375;
            x.ti = 3.2e6;
            x.tt = 1e9;
            x.min = 0.001;
            x.max = 1000.0;
        });
        s->sleepRatio = startingScavSleepRatio;
        if(s->scavenge == nullptr)
        {
            s->scavenge = [=](uintptr_t n) mutable -> std::tuple<uintptr_t, int64_t>
            {
                auto start = nanotime();
                auto r = rec::scavenge(gocpp::recv(mheap_.pages), n, nullptr, false);
                auto end = nanotime();
                if(start >= end)
                {
                    return {r, 0};
                }
                rec::Add(gocpp::recv(scavenge.backgroundTime), end - start);
                return {r, end - start};
            };
        }
        if(s->shouldStop == nullptr)
        {
            s->shouldStop = [=]() mutable -> bool
            {
                return heapRetained() <= rec::Load(gocpp::recv(scavenge.gcPercentGoal)) && rec::Load(gocpp::recv(gcController.mappedReady)) <= rec::Load(gocpp::recv(scavenge.memoryLimitGoal));
            };
        }
        if(s->gomaxprocs == nullptr)
        {
            s->gomaxprocs = [=]() mutable -> int32_t
            {
                return gomaxprocs;
            };
        }
    }

    // park parks the scavenger goroutine.
    void rec::park(golang::runtime::scavengerState* s)
    {
        lock(& s->lock);
        if(getg() != s->g)
        {
            go_throw("tried to park scavenger from another goroutine"_s);
        }
        s->parked = true;
        goparkunlock(& s->lock, waitReasonGCScavengeWait, traceBlockSystemGoroutine, 2);
    }

    // ready signals to sysmon that the scavenger should be awoken.
    void rec::ready(golang::runtime::scavengerState* s)
    {
        rec::Store(gocpp::recv(s->sysmonWake), 1);
    }

    // wake immediately unparks the scavenger if necessary.
    //
    // Safe to run without a P.
    void rec::wake(golang::runtime::scavengerState* s)
    {
        lock(& s->lock);
        if(s->parked)
        {
            rec::Store(gocpp::recv(s->sysmonWake), 0);
            s->parked = false;
            // Ready the goroutine by injecting it. We use injectglist instead
            // of ready or goready in order to allow us to run this function
            // without a P. injectglist also avoids placing the goroutine in
            // the current P's runnext slot, which is desirable to prevent
            // the scavenger from interfering with user goroutine scheduling
            // too much.
            gList list = {};
            rec::push(gocpp::recv(list), s->g);
            injectglist(& list);
        }
        unlock(& s->lock);
    }

    // sleep puts the scavenger to sleep based on the amount of time that it worked
    // in nanoseconds.
    //
    // Note that this function should only be called by the scavenger.
    //
    // The scavenger may be woken up earlier by a pacing change, and it may not go
    // to sleep at all if there's a pending pacing change.
    void rec::sleep(golang::runtime::scavengerState* s, double worked)
    {
        lock(& s->lock);
        if(getg() != s->g)
        {
            go_throw("tried to sleep scavenger from another goroutine"_s);
        }
        if(worked < minScavWorkTime)
        {
            worked = minScavWorkTime;
        }
        worked *= 1 + scavengeCostRatio;
        auto sleepTime = int64_t(worked / s->sleepRatio);
        int64_t slept = {};
        if(s->sleepStub == nullptr)
        {
            auto start = nanotime();
            resetTimer(s->timer, start + sleepTime);
            s->parked = true;
            goparkunlock(& s->lock, waitReasonSleep, traceBlockSleep, 2);
            slept = nanotime() - start;
            lock(& s->lock);
            stopTimer(s->timer);
            unlock(& s->lock);
        }
        else
        {
            unlock(& s->lock);
            slept = s->sleepStub(sleepTime);
        }
        if(s->controllerCooldown > 0)
        {
            auto t = slept + int64_t(worked);
            if(t > s->controllerCooldown)
            {
                s->controllerCooldown = 0;
            }
            else
            {
                s->controllerCooldown -= t;
            }
            return;
        }
        auto idealFraction = double(scavengePercent) / 100.0;
        auto cpuFraction = worked / ((double(slept) + worked) * double(s->gomaxprocs()));
        // Update the critSleepRatio, adjusting until we reach our ideal fraction.
        bool ok = {};
        std::tie(s->sleepRatio, ok) = rec::next(gocpp::recv(s->sleepController), cpuFraction, idealFraction, double(slept) + worked);
        if(! ok)
        {
            s->sleepRatio = startingScavSleepRatio;
            s->controllerCooldown = 5e9;
            rec::controllerFailed(gocpp::recv(s));
        }
    }

    // controllerFailed indicates that the scavenger's scheduling
    // controller failed.
    void rec::controllerFailed(golang::runtime::scavengerState* s)
    {
        lock(& s->lock);
        s->printControllerReset = true;
        unlock(& s->lock);
    }

    // run is the body of the main scavenging loop.
    //
    // Returns the number of bytes released and the estimated time spent
    // releasing those bytes.
    //
    // Must be run on the scavenger goroutine.
    std::tuple<uintptr_t, double> rec::run(golang::runtime::scavengerState* s)
    {
        uintptr_t released;
        double worked;
        lock(& s->lock);
        if(getg() != s->g)
        {
            go_throw("tried to run scavenger from another goroutine"_s);
        }
        unlock(& s->lock);
        for(; worked < minScavWorkTime; )
        {
            if(s->shouldStop())
            {
                break;
            }
            // scavengeQuantum is the amount of memory we try to scavenge
            // in one go. A smaller value means the scavenger is more responsive
            // to the scheduler in case of e.g. preemption. A larger value means
            // that the overheads of scavenging are better amortized, so better
            // scavenging throughput.
            //
            // The current value is chosen assuming a cost of ~10µs/physical page
            // (this is somewhat pessimistic), which implies a worst-case latency of
            // about 160µs for 4 KiB physical pages. The current value is biased
            // toward latency over throughput.
            auto scavengeQuantum = 64 << 10;
            auto [r, duration] = s->scavenge(scavengeQuantum);
            // On some platforms we may see end >= start if the time it takes to scavenge
            // memory is less than the minimum granularity of its clock (e.g. Windows) or
            // due to clock bugs.
            //
            // In this case, just assume scavenging takes 10 µs per regular physical page
            // (determined empirically), and conservatively ignore the impact of huge pages
            // on timing.
            auto approxWorkedNSPerPhysicalPage = 10e3;
            if(duration == 0)
            {
                worked += approxWorkedNSPerPhysicalPage * double(r / physPageSize);
            }
            else
            {
                worked += double(duration);
            }
            released += r;
            if(r < scavengeQuantum)
            {
                break;
            }
            if(faketime != 0)
            {
                break;
            }
        }
        if(released > 0 && released < physPageSize)
        {
            go_throw("released less than one physical page of memory"_s);
        }
        return {released, worked};
    }

    // Background scavenger.
    //
    // The background scavenger maintains the RSS of the application below
    // the line described by the proportional scavenging statistics in
    // the mheap struct.
    void bgscavenge(gocpp::channel<int> c)
    {
        rec::init(gocpp::recv(scavenger));
        c.send(1);
        rec::park(gocpp::recv(scavenger));
        for(; ; )
        {
            auto [released, workTime] = rec::run(gocpp::recv(scavenger));
            if(released == 0)
            {
                rec::park(gocpp::recv(scavenger));
                continue;
            }
            rec::Add(gocpp::recv(mheap_.pages.scav.releasedBg), released);
            rec::sleep(gocpp::recv(scavenger), workTime);
        }
    }

    // scavenge scavenges nbytes worth of free pages, starting with the
    // highest address first. Successive calls continue from where it left
    // off until the heap is exhausted. force makes all memory available to
    // scavenge, ignoring huge page heuristics.
    //
    // Returns the amount of memory scavenged in bytes.
    //
    // scavenge always tries to scavenge nbytes worth of memory, and will
    // only fail to do so if the heap is exhausted for now.
    uintptr_t rec::scavenge(golang::runtime::pageAlloc* p, uintptr_t nbytes, std::function<bool ()> shouldStop, bool force)
    {
        auto released = uintptr_t(0);
        for(; released < nbytes; )
        {
            auto [ci, pageIdx] = rec::find(gocpp::recv(p->scav.index), force);
            if(ci == 0)
            {
                break;
            }
            systemstack([=]() mutable -> void
            {
                released += rec::scavengeOne(gocpp::recv(p), ci, pageIdx, nbytes - released);
            });
            if(shouldStop != nullptr && shouldStop())
            {
                break;
            }
        }
        return released;
    }

    // printScavTrace prints a scavenge trace line to standard error.
    //
    // released should be the amount of memory released since the last time this
    // was called, and forced indicates whether the scavenge was forced by the
    // application.
    //
    // scavenger.lock must be held.
    void printScavTrace(uintptr_t releasedBg, uintptr_t releasedEager, bool forced)
    {
        assertLockHeld(& scavenger.lock);
        printlock();
        print("scav "_s, releasedBg >> 10, " KiB work (bg), "_s, releasedEager >> 10, " KiB work (eager), "_s, rec::load(gocpp::recv(gcController.heapReleased)) >> 10, " KiB now, "_s, (rec::load(gocpp::recv(gcController.heapInUse)) * 100) / heapRetained(), "% util"_s);
        if(forced)
        {
            print(" (forced)"_s);
        }
        else
        if(scavenger.printControllerReset)
        {
            print(" [controller reset]"_s);
            scavenger.printControllerReset = false;
        }
        println();
        printunlock();
    }

    // scavengeOne walks over the chunk at chunk index ci and searches for
    // a contiguous run of pages to scavenge. It will try to scavenge
    // at most max bytes at once, but may scavenge more to avoid
    // breaking huge pages. Once it scavenges some memory it returns
    // how much it scavenged in bytes.
    //
    // searchIdx is the page index to start searching from in ci.
    //
    // Returns the number of bytes scavenged.
    //
    // Must run on the systemstack because it acquires p.mheapLock.
    //
    //go:systemstack
    uintptr_t rec::scavengeOne(golang::runtime::pageAlloc* p, golang::runtime::chunkIdx ci, unsigned int searchIdx, uintptr_t max)
    {
        auto maxPages = max / pageSize;
        if(max % pageSize != 0)
        {
            maxPages++;
        }
        auto minPages = physPageSize / pageSize;
        if(minPages < 1)
        {
            minPages = 1;
        }
        lock(p->mheapLock);
        if(rec::max(gocpp::recv(p->summary[len(p->summary) - 1][ci])) >= (unsigned int)(minPages))
        {
            auto [base, npages] = rec::findScavengeCandidate(gocpp::recv(rec::chunkOf(gocpp::recv(p), ci)), searchIdx, minPages, maxPages);
            if(npages != 0)
            {
                auto addr = chunkBase(ci) + uintptr_t(base) * pageSize;
                rec::allocRange(gocpp::recv(rec::chunkOf(gocpp::recv(p), ci)), base, npages);
                rec::update(gocpp::recv(p), addr, uintptr_t(npages), true, true);
                unlock(p->mheapLock);
                if(! p->test)
                {
                    pageTraceScav(rec::ptr(gocpp::recv(getg()->m->p)), 0, addr, uintptr_t(npages));
                    sysUnused(gocpp::unsafe_pointer(addr), uintptr_t(npages) * pageSize);
                    auto nbytes = int64_t(npages * pageSize);
                    rec::add(gocpp::recv(gcController.heapReleased), nbytes);
                    rec::add(gocpp::recv(gcController.heapFree), - nbytes);
                    auto stats = rec::acquire(gocpp::recv(memstats.heapStats));
                    atomic::Xaddint64(& stats->committed, - nbytes);
                    atomic::Xaddint64(& stats->released, nbytes);
                    rec::release(gocpp::recv(memstats.heapStats));
                }
                lock(p->mheapLock);
                if(auto b = (offAddr {addr}); rec::lessThan(gocpp::recv(b), p->searchAddr))
                {
                    p->searchAddr = b;
                }
                rec::free(gocpp::recv(rec::chunkOf(gocpp::recv(p), ci)), base, npages);
                rec::update(gocpp::recv(p), addr, uintptr_t(npages), true, false);
                rec::setRange(gocpp::recv(rec::chunkOf(gocpp::recv(p), ci)->scavenged), base, npages);
                unlock(p->mheapLock);
                return uintptr_t(npages) * pageSize;
            }
        }
        rec::setEmpty(gocpp::recv(p->scav.index), ci);
        unlock(p->mheapLock);
        return 0;
    }

    // fillAligned returns x but with all zeroes in m-aligned
    // groups of m bits set to 1 if any bit in the group is non-zero.
    //
    // For example, fillAligned(0x0100a3, 8) == 0xff00ff.
    //
    // Note that if m == 1, this is a no-op.
    //
    // m must be a power of 2 <= maxPagesPerPhysPage.
    uint64_t fillAligned(uint64_t x, unsigned int m)
    {
        auto apply = [=](uint64_t x, uint64_t c) mutable -> uint64_t
        {
            return ~ ((((x & c) + c) | x) | c);
        };
        //Go switch emulation
        {
            auto condition = m;
            int conditionId = -1;
            if(condition == 1) { conditionId = 0; }
            else if(condition == 2) { conditionId = 1; }
            else if(condition == 4) { conditionId = 2; }
            else if(condition == 8) { conditionId = 3; }
            else if(condition == 16) { conditionId = 4; }
            else if(condition == 32) { conditionId = 5; }
            else if(condition == 64) { conditionId = 6; }
            switch(conditionId)
            {
                case 0:
                    return x;
                    break;
                case 1:
                    x = apply(x, 0x5555555555555555);
                    break;
                case 2:
                    x = apply(x, 0x7777777777777777);
                    break;
                case 3:
                    x = apply(x, 0x7f7f7f7f7f7f7f7f);
                    break;
                case 4:
                    x = apply(x, 0x7fff7fff7fff7fff);
                    break;
                case 5:
                    x = apply(x, 0x7fffffff7fffffff);
                    break;
                case 6:
                    x = apply(x, 0x7fffffffffffffff);
                    break;
                default:
                    go_throw("bad m value"_s);
                    break;
            }
        }
        return ~ ((x - (x >> (m - 1))) | x);
    }

    // findScavengeCandidate returns a start index and a size for this pallocData
    // segment which represents a contiguous region of free and unscavenged memory.
    //
    // searchIdx indicates the page index within this chunk to start the search, but
    // note that findScavengeCandidate searches backwards through the pallocData. As
    // a result, it will return the highest scavenge candidate in address order.
    //
    // min indicates a hard minimum size and alignment for runs of pages. That is,
    // findScavengeCandidate will not return a region smaller than min pages in size,
    // or that is min pages or greater in size but not aligned to min. min must be
    // a non-zero power of 2 <= maxPagesPerPhysPage.
    //
    // max is a hint for how big of a region is desired. If max >= pallocChunkPages, then
    // findScavengeCandidate effectively returns entire free and unscavenged regions.
    // If max < pallocChunkPages, it may truncate the returned region such that size is
    // max. However, findScavengeCandidate may still return a larger region if, for
    // example, it chooses to preserve huge pages, or if max is not aligned to min (it
    // will round up). That is, even if max is small, the returned size is not guaranteed
    // to be equal to max. max is allowed to be less than min, in which case it is as if
    // max == min.
    std::tuple<unsigned int, unsigned int> rec::findScavengeCandidate(golang::runtime::pallocData* m, unsigned int searchIdx, uintptr_t minimum, uintptr_t max)
    {
        if(minimum & (minimum - 1) != 0 || minimum == 0)
        {
            print("runtime: min = "_s, minimum, "\n"_s);
            go_throw("min must be a non-zero power of 2"_s);
        }
        else
        if(minimum > maxPagesPerPhysPage)
        {
            print("runtime: min = "_s, minimum, "\n"_s);
            go_throw("min too large"_s);
        }
        if(max == 0)
        {
            max = minimum;
        }
        else
        {
            max = alignUp(max, minimum);
        }
        auto i = int(searchIdx / 64);
        for(; i >= 0; i--)
        {
            auto x = fillAligned(m->scavenged[i] | m->pallocBits[i], (unsigned int)(minimum));
            if(x != ~ uint64_t(0))
            {
                break;
            }
        }
        if(i < 0)
        {
            return {0, 0};
        }
        auto x = fillAligned(m->scavenged[i] | m->pallocBits[i], (unsigned int)(minimum));
        auto z1 = (unsigned int)(sys::LeadingZeros64(~ x));
        auto [run, end] = std::tuple{(unsigned int)(0), (unsigned int)(i) * 64 + (64 - z1)};
        if((x << z1) != 0)
        {
            run = (unsigned int)(sys::LeadingZeros64(x << z1));
        }
        else
        {
            run = 64 - z1;
            for(auto j = i - 1; j >= 0; j--)
            {
                auto x = fillAligned(m->scavenged[j] | m->pallocBits[j], (unsigned int)(minimum));
                run += (unsigned int)(sys::LeadingZeros64(x));
                if(x != 0)
                {
                    break;
                }
            }
        }
        auto size = gocpp::min(run, (unsigned int)(max));
        auto start = end - size;
        if(physHugePageSize > pageSize && physHugePageSize > physPageSize)
        {
            auto pagesPerHugePage = physHugePageSize / pageSize;
            auto hugePageAbove = (unsigned int)(alignUp(uintptr_t(start), pagesPerHugePage));
            if(hugePageAbove <= end)
            {
                auto hugePageBelow = (unsigned int)(alignDown(uintptr_t(start), pagesPerHugePage));
                if(hugePageBelow >= end - run)
                {
                    size = size + (start - hugePageBelow);
                    start = hugePageBelow;
                }
            }
        }
        return {start, size};
    }

    // scavengeIndex is a structure for efficiently managing which pageAlloc chunks have
    // memory available to scavenge.
    
    template<typename T> requires gocpp::GoStruct<T>
    scavengeIndex::operator T()
    {
        T result;
        result.chunks = this->chunks;
        result.min = this->min;
        result.max = this->max;
        result.minHeapIdx = this->minHeapIdx;
        result.searchAddrBg = this->searchAddrBg;
        result.searchAddrForce = this->searchAddrForce;
        result.freeHWM = this->freeHWM;
        result.gen = this->gen;
        result.test = this->test;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool scavengeIndex::operator==(const T& ref) const
    {
        if (chunks != ref.chunks) return false;
        if (min != ref.min) return false;
        if (max != ref.max) return false;
        if (minHeapIdx != ref.minHeapIdx) return false;
        if (searchAddrBg != ref.searchAddrBg) return false;
        if (searchAddrForce != ref.searchAddrForce) return false;
        if (freeHWM != ref.freeHWM) return false;
        if (gen != ref.gen) return false;
        if (test != ref.test) return false;
        return true;
    }

    std::ostream& scavengeIndex::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << chunks;
        os << " " << min;
        os << " " << max;
        os << " " << minHeapIdx;
        os << " " << searchAddrBg;
        os << " " << searchAddrForce;
        os << " " << freeHWM;
        os << " " << gen;
        os << " " << test;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct scavengeIndex& value)
    {
        return value.PrintTo(os);
    }

    // init initializes the scavengeIndex.
    //
    // Returns the amount added to sysStat.
    uintptr_t rec::init(golang::runtime::scavengeIndex* s, bool test, golang::runtime::sysMemStat* sysStat)
    {
        rec::Clear(gocpp::recv(s->searchAddrBg));
        rec::Clear(gocpp::recv(s->searchAddrForce));
        s->freeHWM = minOffAddr;
        s->test = test;
        return rec::sysInit(gocpp::recv(s), test, sysStat);
    }

    // sysGrow updates the index's backing store in response to a heap growth.
    //
    // Returns the amount of memory added to sysStat.
    uintptr_t rec::grow(golang::runtime::scavengeIndex* s, uintptr_t base, uintptr_t limit, golang::runtime::sysMemStat* sysStat)
    {
        auto minHeapIdx = rec::Load(gocpp::recv(s->minHeapIdx));
        if(auto baseIdx = uintptr_t(chunkIndex(base)); minHeapIdx == 0 || baseIdx < minHeapIdx)
        {
            rec::Store(gocpp::recv(s->minHeapIdx), baseIdx);
        }
        return rec::sysGrow(gocpp::recv(s), base, limit, sysStat);
    }

    // find returns the highest chunk index that may contain pages available to scavenge.
    // It also returns an offset to start searching in the highest chunk.
    std::tuple<runtime::chunkIdx, unsigned int> rec::find(golang::runtime::scavengeIndex* s, bool force)
    {
        auto cursor = & s->searchAddrBg;
        if(force)
        {
            cursor = & s->searchAddrForce;
        }
        auto [searchAddr, marked] = rec::Load(gocpp::recv(cursor));
        if(searchAddr == rec::addr(gocpp::recv(minOffAddr)))
        {
            return {0, 0};
        }
        auto gen = s->gen;
        auto min = chunkIdx(rec::Load(gocpp::recv(s->minHeapIdx)));
        auto start = chunkIndex(searchAddr);
        for(auto i = start; i >= min; i--)
        {
            if(! rec::shouldScavenge(gocpp::recv(rec::load(gocpp::recv(s->chunks[i]))), gen, force))
            {
                continue;
            }
            if(i == start)
            {
                return {i, chunkPageIndex(searchAddr)};
            }
            auto newSearchAddr = chunkBase(i) + pallocChunkBytes - pageSize;
            if(marked)
            {
                rec::StoreUnmark(gocpp::recv(cursor), searchAddr, newSearchAddr);
            }
            else
            {
                rec::StoreMin(gocpp::recv(cursor), newSearchAddr);
            }
            return {i, pallocChunkPages - 1};
        }
        rec::Clear(gocpp::recv(cursor));
        return {0, 0};
    }

    // alloc updates metadata for chunk at index ci with the fact that
    // an allocation of npages occurred. It also eagerly attempts to collapse
    // the chunk's memory into hugepage if the chunk has become sufficiently
    // dense and we're not allocating the whole chunk at once (which suggests
    // the allocation is part of a bigger one and it's probably not worth
    // eagerly collapsing).
    //
    // alloc may only run concurrently with find.
    void rec::alloc(golang::runtime::scavengeIndex* s, golang::runtime::chunkIdx ci, unsigned int npages)
    {
        auto sc = rec::load(gocpp::recv(s->chunks[ci]));
        rec::alloc(gocpp::recv(sc), npages, s->gen);
        rec::store(gocpp::recv(s->chunks[ci]), sc);
    }

    // free updates metadata for chunk at index ci with the fact that
    // a free of npages occurred.
    //
    // free may only run concurrently with find.
    void rec::free(golang::runtime::scavengeIndex* s, golang::runtime::chunkIdx ci, unsigned int page, unsigned int npages)
    {
        auto sc = rec::load(gocpp::recv(s->chunks[ci]));
        rec::free(gocpp::recv(sc), npages, s->gen);
        rec::store(gocpp::recv(s->chunks[ci]), sc);
        auto addr = chunkBase(ci) + uintptr_t(page + npages - 1) * pageSize;
        if(rec::lessThan(gocpp::recv(s->freeHWM), offAddr {addr}))
        {
            s->freeHWM = offAddr {addr};
        }
        auto [searchAddr, gocpp_id_1] = rec::Load(gocpp::recv(s->searchAddrForce));
        if(rec::lessThan(gocpp::recv((offAddr {searchAddr})), offAddr {addr}))
        {
            rec::StoreMarked(gocpp::recv(s->searchAddrForce), addr);
        }
    }

    // nextGen moves the scavenger forward one generation. Must be called
    // once per GC cycle, but may be called more often to force more memory
    // to be released.
    //
    // nextGen may only run concurrently with find.
    void rec::nextGen(golang::runtime::scavengeIndex* s)
    {
        s->gen++;
        auto [searchAddr, gocpp_id_2] = rec::Load(gocpp::recv(s->searchAddrBg));
        if(rec::lessThan(gocpp::recv((offAddr {searchAddr})), s->freeHWM))
        {
            rec::StoreMarked(gocpp::recv(s->searchAddrBg), rec::addr(gocpp::recv(s->freeHWM)));
        }
        s->freeHWM = minOffAddr;
    }

    // setEmpty marks that the scavenger has finished looking at ci
    // for now to prevent the scavenger from getting stuck looking
    // at the same chunk.
    //
    // setEmpty may only run concurrently with find.
    void rec::setEmpty(golang::runtime::scavengeIndex* s, golang::runtime::chunkIdx ci)
    {
        auto val = rec::load(gocpp::recv(s->chunks[ci]));
        rec::setEmpty(gocpp::recv(val));
        rec::store(gocpp::recv(s->chunks[ci]), val);
    }

    // atomicScavChunkData is an atomic wrapper around a scavChunkData
    // that stores it in its packed form.
    
    template<typename T> requires gocpp::GoStruct<T>
    atomicScavChunkData::operator T()
    {
        T result;
        result.value = this->value;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool atomicScavChunkData::operator==(const T& ref) const
    {
        if (value != ref.value) return false;
        return true;
    }

    std::ostream& atomicScavChunkData::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << value;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct atomicScavChunkData& value)
    {
        return value.PrintTo(os);
    }

    // load loads and unpacks a scavChunkData.
    struct scavChunkData rec::load(golang::runtime::atomicScavChunkData* sc)
    {
        return unpackScavChunkData(rec::Load(gocpp::recv(sc->value)));
    }

    // store packs and writes a new scavChunkData. store must be serialized
    // with other calls to store.
    void rec::store(golang::runtime::atomicScavChunkData* sc, struct scavChunkData ssc)
    {
        rec::Store(gocpp::recv(sc->value), rec::pack(gocpp::recv(ssc)));
    }

    // scavChunkData tracks information about a palloc chunk for
    // scavenging. It packs well into 64 bits.
    //
    // The zero value always represents a valid newly-grown chunk.
    
    template<typename T> requires gocpp::GoStruct<T>
    scavChunkData::operator T()
    {
        T result;
        result.inUse = this->inUse;
        result.lastInUse = this->lastInUse;
        result.gen = this->gen;
        result.scavChunkFlags = this->scavChunkFlags;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool scavChunkData::operator==(const T& ref) const
    {
        if (inUse != ref.inUse) return false;
        if (lastInUse != ref.lastInUse) return false;
        if (gen != ref.gen) return false;
        if (scavChunkFlags != ref.scavChunkFlags) return false;
        return true;
    }

    std::ostream& scavChunkData::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << inUse;
        os << " " << lastInUse;
        os << " " << gen;
        os << " " << scavChunkFlags;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct scavChunkData& value)
    {
        return value.PrintTo(os);
    }

    // unpackScavChunkData unpacks a scavChunkData from a uint64.
    struct scavChunkData unpackScavChunkData(uint64_t sc)
    {
        return gocpp::Init<scavChunkData>([=](auto& x) {
            x.inUse = uint16_t(sc);
            x.lastInUse = uint16_t(sc >> 16) & scavChunkInUseMask;
            x.gen = uint32_t(sc >> 32);
            x.scavChunkFlags = scavChunkFlags(uint8_t(sc >> (16 + logScavChunkInUseMax)) & scavChunkFlagsMask);
        });
    }

    // pack returns sc packed into a uint64.
    uint64_t rec::pack(golang::runtime::scavChunkData sc)
    {
        return uint64_t(sc.inUse) | (uint64_t(sc.lastInUse) << 16) | (uint64_t(sc.scavChunkFlags) << (16 + logScavChunkInUseMax)) | (uint64_t(sc.gen) << 32);
    }

    // scavChunkHasFree indicates whether the chunk has anything left to
    // scavenge. This is the opposite of "empty," used elsewhere in this
    // file. The reason we say "HasFree" here is so the zero value is
    // correct for a newly-grown chunk. (New memory is scavenged.)
    // scavChunkMaxFlags is the maximum number of flags we can have, given how
    // a scavChunkData is packed into 8 bytes.
    // logScavChunkInUseMax is the number of bits needed to represent the number
    // of pages allocated in a single chunk. This is 1 more than log2 of the
    // number of pages in the chunk because we need to represent a fully-allocated
    // chunk.
    // scavChunkFlags is a set of bit-flags for the scavenger for each palloc chunk.
    // isEmpty returns true if the hasFree flag is unset.
    bool rec::isEmpty(golang::runtime::scavChunkFlags* sc)
    {
        return (*sc) & scavChunkHasFree == 0;
    }

    // setEmpty clears the hasFree flag.
    void rec::setEmpty(golang::runtime::scavChunkFlags* sc)
    {
        *sc &^= scavChunkHasFree;
    }

    // setNonEmpty sets the hasFree flag.
    void rec::setNonEmpty(golang::runtime::scavChunkFlags* sc)
    {
        *sc |= scavChunkHasFree;
    }

    // shouldScavenge returns true if the corresponding chunk should be interrogated
    // by the scavenger.
    bool rec::shouldScavenge(golang::runtime::scavChunkData sc, uint32_t currGen, bool force)
    {
        if(rec::isEmpty(gocpp::recv(sc)))
        {
            return false;
        }
        if(force)
        {
            return true;
        }
        if(sc.gen == currGen)
        {
            return sc.inUse < scavChunkHiOccPages && sc.lastInUse < scavChunkHiOccPages;
        }
        return sc.inUse < scavChunkHiOccPages;
    }

    // alloc updates sc given that npages were allocated in the corresponding chunk.
    void rec::alloc(golang::runtime::scavChunkData* sc, unsigned int npages, uint32_t newGen)
    {
        if((unsigned int)(sc->inUse) + npages > pallocChunkPages)
        {
            print("runtime: inUse="_s, sc->inUse, " npages="_s, npages, "\n"_s);
            go_throw("too many pages allocated in chunk?"_s);
        }
        if(sc->gen != newGen)
        {
            sc->lastInUse = sc->inUse;
            sc->gen = newGen;
        }
        sc->inUse += uint16_t(npages);
        if(sc->inUse == pallocChunkPages)
        {
            rec::setEmpty(gocpp::recv(sc));
        }
    }

    // free updates sc given that npages was freed in the corresponding chunk.
    void rec::free(golang::runtime::scavChunkData* sc, unsigned int npages, uint32_t newGen)
    {
        if((unsigned int)(sc->inUse) < npages)
        {
            print("runtime: inUse="_s, sc->inUse, " npages="_s, npages, "\n"_s);
            go_throw("allocated pages below zero?"_s);
        }
        if(sc->gen != newGen)
        {
            sc->lastInUse = sc->inUse;
            sc->gen = newGen;
        }
        sc->inUse -= uint16_t(npages);
        rec::setNonEmpty(gocpp::recv(sc));
    }

    
    template<typename T> requires gocpp::GoStruct<T>
    piController::operator T()
    {
        T result;
        result.kp = this->kp;
        result.ti = this->ti;
        result.tt = this->tt;
        result.min = this->min;
        result.max = this->max;
        result.errIntegral = this->errIntegral;
        result.errOverflow = this->errOverflow;
        result.inputOverflow = this->inputOverflow;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool piController::operator==(const T& ref) const
    {
        if (kp != ref.kp) return false;
        if (ti != ref.ti) return false;
        if (tt != ref.tt) return false;
        if (min != ref.min) return false;
        if (max != ref.max) return false;
        if (errIntegral != ref.errIntegral) return false;
        if (errOverflow != ref.errOverflow) return false;
        if (inputOverflow != ref.inputOverflow) return false;
        return true;
    }

    std::ostream& piController::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << kp;
        os << " " << ti;
        os << " " << tt;
        os << " " << min;
        os << " " << max;
        os << " " << errIntegral;
        os << " " << errOverflow;
        os << " " << inputOverflow;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct piController& value)
    {
        return value.PrintTo(os);
    }

    // next provides a new sample to the controller.
    //
    // input is the sample, setpoint is the desired point, and period is how much
    // time (in whatever unit makes the most sense) has passed since the last sample.
    //
    // Returns a new value for the variable it's controlling, and whether the operation
    // completed successfully. One reason this might fail is if error has been growing
    // in an unbounded manner, to the point of overflow.
    //
    // In the specific case of an error overflow occurs, the errOverflow field will be
    // set and the rest of the controller's internal state will be fully reset.
    std::tuple<double, bool> rec::next(golang::runtime::piController* c, double input, double setpoint, double period)
    {
        auto prop = c->kp * (setpoint - input);
        auto rawOutput = prop + c->errIntegral;
        auto output = rawOutput;
        if(isInf(output) || isNaN(output))
        {
            rec::reset(gocpp::recv(c));
            c->inputOverflow = true;
            return {c->min, false};
        }
        if(output < c->min)
        {
            output = c->min;
        }
        else
        if(output > c->max)
        {
            output = c->max;
        }
        if(c->ti != 0 && c->tt != 0)
        {
            c->errIntegral += (c->kp * period / c->ti) * (setpoint - input) + (period / c->tt) * (output - rawOutput);
            if(isInf(c->errIntegral) || isNaN(c->errIntegral))
            {
                rec::reset(gocpp::recv(c));
                c->errOverflow = true;
                return {c->min, false};
            }
        }
        return {output, true};
    }

    // reset resets the controller state, except for controller error flags.
    void rec::reset(golang::runtime::piController* c)
    {
        c->errIntegral = 0;
    }

}

