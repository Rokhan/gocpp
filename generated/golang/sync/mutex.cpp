// generated by GoCpp from file '$(ImportDir)/sync/mutex.go'
#include <complex>
#include <functional>
#include <iostream>
#include <iomanip>
#include <map>
#include <string>
#include <tuple>
#include <vector>

#include "golang/sync/mutex.h"
#include "gocpp/support.h"

#include "golang/internal/race/norace.h"
#include "golang/sync/atomic/doc.h"
#include "golang/sync/runtime.h"
#include "golang/unsafe/unsafe.h"

// Package sync provides basic synchronization primitives such as mutual
// exclusion locks. Other than the Once and WaitGroup types, most are intended
// for use by low-level library routines. Higher-level synchronization is
// better done via channels and communication.
//
// Values containing the types defined in this package should not be copied.
namespace golang::sync
{
    namespace rec
    {
        using namespace mocklib::rec;
    }

    // Provided by runtime via linkname.
    void go_throw(std::string)
    /* convertBlockStmt, nil block */;

    void fatal(std::string)
    /* convertBlockStmt, nil block */;

    // A Mutex is a mutual exclusion lock.
    // The zero value for a Mutex is an unlocked mutex.
    //
    // A Mutex must not be copied after first use.
    //
    // In the terminology of the Go memory model,
    // the n'th call to Unlock “synchronizes before” the m'th call to Lock
    // for any n < m.
    // A successful call to TryLock is equivalent to a call to Lock.
    // A failed call to TryLock does not establish any “synchronizes before”
    // relation at all.
    
    template<typename T> requires gocpp::GoStruct<T>
    Mutex::operator T()
    {
        T result;
        result.state = this->state;
        result.sema = this->sema;
        return result;
    }

    template<typename T> requires gocpp::GoStruct<T>
    bool Mutex::operator==(const T& ref) const
    {
        if (state != ref.state) return false;
        if (sema != ref.sema) return false;
        return true;
    }

    std::ostream& Mutex::PrintTo(std::ostream& os) const
    {
        os << '{';
        os << "" << state;
        os << " " << sema;
        os << '}';
        return os;
    }

    std::ostream& operator<<(std::ostream& os, const struct Mutex& value)
    {
        return value.PrintTo(os);
    }

    // A Locker represents an object that can be locked and unlocked.
    
    template<typename T>
    Locker::Locker(T& ref)
    {
        value.reset(new LockerImpl<T, std::unique_ptr<T>>(new T(ref)));
    }

    template<typename T>
    Locker::Locker(const T& ref)
    {
        value.reset(new LockerImpl<T, std::unique_ptr<T>>(new T(ref)));
    }

    template<typename T>
    Locker::Locker(T* ptr)
    {
        value.reset(new LockerImpl<T, gocpp::ptr<T>>(ptr));
    }

    std::ostream& Locker::PrintTo(std::ostream& os) const
    {
        return os;
    }

    template<typename T, typename StoreT>
    void Locker::LockerImpl<T, StoreT>::vLock()
    {
        return rec::Lock(gocpp::PtrRecv<T, false>(value.get()));
    }
    template<typename T, typename StoreT>
    void Locker::LockerImpl<T, StoreT>::vUnlock()
    {
        return rec::Unlock(gocpp::PtrRecv<T, false>(value.get()));
    }

    namespace rec
    {
        void Lock(const gocpp::PtrRecv<struct Locker, false>& self)
        {
            return self.ptr->value->vLock();
        }

        void Lock(const gocpp::ObjRecv<struct Locker>& self)
        {
            return self.obj.value->vLock();
        }

        void Unlock(const gocpp::PtrRecv<struct Locker, false>& self)
        {
            return self.ptr->value->vUnlock();
        }

        void Unlock(const gocpp::ObjRecv<struct Locker>& self)
        {
            return self.obj.value->vUnlock();
        }
    }

    std::ostream& operator<<(std::ostream& os, const struct Locker& value)
    {
        return value.PrintTo(os);
    }

    // Mutex fairness.
    //
    // Mutex can be in 2 modes of operations: normal and starvation.
    // In normal mode waiters are queued in FIFO order, but a woken up waiter
    // does not own the mutex and competes with new arriving goroutines over
    // the ownership. New arriving goroutines have an advantage -- they are
    // already running on CPU and there can be lots of them, so a woken up
    // waiter has good chances of losing. In such case it is queued at front
    // of the wait queue. If a waiter fails to acquire the mutex for more than 1ms,
    // it switches mutex to the starvation mode.
    //
    // In starvation mode ownership of the mutex is directly handed off from
    // the unlocking goroutine to the waiter at the front of the queue.
    // New arriving goroutines don't try to acquire the mutex even if it appears
    // to be unlocked, and don't try to spin. Instead they queue themselves at
    // the tail of the wait queue.
    //
    // If a waiter receives ownership of the mutex and sees that either
    // (1) it is the last waiter in the queue, or (2) it waited for less than 1 ms,
    // it switches mutex back to normal operation mode.
    //
    // Normal mode has considerably better performance as a goroutine can acquire
    // a mutex several times in a row even if there are blocked waiters.
    // Starvation mode is important to prevent pathological cases of tail latency.
    // Lock locks m.
    // If the lock is already in use, the calling goroutine
    // blocks until the mutex is available.
    void rec::Lock(struct Mutex* m)
    {
        if(atomic::CompareAndSwapInt32(& m->state, 0, mutexLocked))
        {
            if(race::Enabled)
            {
                race::Acquire(unsafe::Pointer(m));
            }
            return;
        }
        rec::lockSlow(gocpp::recv(m));
    }

    // TryLock tries to lock m and reports whether it succeeded.
    //
    // Note that while correct uses of TryLock do exist, they are rare,
    // and use of TryLock is often a sign of a deeper problem
    // in a particular use of mutexes.
    bool rec::TryLock(struct Mutex* m)
    {
        auto old = m->state;
        if(old & (mutexLocked | mutexStarving) != 0)
        {
            return false;
        }
        if(! atomic::CompareAndSwapInt32(& m->state, old, old | mutexLocked))
        {
            return false;
        }
        if(race::Enabled)
        {
            race::Acquire(unsafe::Pointer(m));
        }
        return true;
    }

    void rec::lockSlow(struct Mutex* m)
    {
        int64_t waitStartTime = {};
        auto starving = false;
        auto awoke = false;
        auto iter = 0;
        auto old = m->state;
        for(; ; )
        {
            if(old & (mutexLocked | mutexStarving) == mutexLocked && runtime_canSpin(iter))
            {
                if(! awoke && old & mutexWoken == 0 && (old >> mutexWaiterShift) != 0 && atomic::CompareAndSwapInt32(& m->state, old, old | mutexWoken))
                {
                    awoke = true;
                }
                runtime_doSpin();
                iter++;
                old = m->state;
                continue;
            }
            auto go_new = old;
            if(old & mutexStarving == 0)
            {
                go_new |= mutexLocked;
            }
            if(old & (mutexLocked | mutexStarving) != 0)
            {
                go_new += 1 << mutexWaiterShift;
            }
            if(starving && old & mutexLocked != 0)
            {
                go_new |= mutexStarving;
            }
            if(awoke)
            {
                if(go_new & mutexWoken == 0)
                {
                    go_throw("sync: inconsistent mutex state"s);
                }
                go_new &^= mutexWoken;
            }
            if(atomic::CompareAndSwapInt32(& m->state, old, go_new))
            {
                if(old & (mutexLocked | mutexStarving) == 0)
                {
                    break;
                }
                auto queueLifo = waitStartTime != 0;
                if(waitStartTime == 0)
                {
                    waitStartTime = runtime_nanotime();
                }
                runtime_SemacquireMutex(& m->sema, queueLifo, 1);
                starving = starving || runtime_nanotime() - waitStartTime > starvationThresholdNs;
                old = m->state;
                if(old & mutexStarving != 0)
                {
                    if(old & (mutexLocked | mutexWoken) != 0 || (old >> mutexWaiterShift) == 0)
                    {
                        go_throw("sync: inconsistent mutex state"s);
                    }
                    auto delta = int32_t(mutexLocked - (1 << mutexWaiterShift));
                    if(! starving || (old >> mutexWaiterShift) == 1)
                    {
                        delta -= mutexStarving;
                    }
                    atomic::AddInt32(& m->state, delta);
                    break;
                }
                awoke = true;
                iter = 0;
            }
            else
            {
                old = m->state;
            }
        }
        if(race::Enabled)
        {
            race::Acquire(unsafe::Pointer(m));
        }
    }

    // Unlock unlocks m.
    // It is a run-time error if m is not locked on entry to Unlock.
    //
    // A locked Mutex is not associated with a particular goroutine.
    // It is allowed for one goroutine to lock a Mutex and then
    // arrange for another goroutine to unlock it.
    void rec::Unlock(struct Mutex* m)
    {
        if(race::Enabled)
        {
            _ = m->state;
            race::Release(unsafe::Pointer(m));
        }
        auto go_new = atomic::AddInt32(& m->state, - mutexLocked);
        if(go_new != 0)
        {
            rec::unlockSlow(gocpp::recv(m), go_new);
        }
    }

    void rec::unlockSlow(struct Mutex* m, int32_t go_new)
    {
        if((go_new + mutexLocked) & mutexLocked == 0)
        {
            fatal("sync: unlock of unlocked mutex"s);
        }
        if(go_new & mutexStarving == 0)
        {
            auto old = go_new;
            for(; ; )
            {
                if((old >> mutexWaiterShift) == 0 || old & (mutexLocked | mutexWoken | mutexStarving) != 0)
                {
                    return;
                }
                go_new = (old - (1 << mutexWaiterShift)) | mutexWoken;
                if(atomic::CompareAndSwapInt32(& m->state, old, go_new))
                {
                    runtime_Semrelease(& m->sema, false, 1);
                    return;
                }
                old = m->state;
            }
        }
        else
        {
            runtime_Semrelease(& m->sema, true, 1);
        }
    }

}

